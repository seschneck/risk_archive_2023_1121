---
title: "Machine learning models for temporally precise lapse prediction in alcohol use disorder"
format: pdf
csl: https://raw.githubusercontent.com/jjcurtin/lab_support/main/rmd_templates/csl/elsevier-vancouver.csl
geometry: margin=.5in
fontsize: 11pt
bibliography: paper_ema.bib
editor_options: 
  chunk_output_type: console
execute:
  echo: false
  warning: false
  message: false
  output: false
---




<!--Target Journal
Considering American Journal of Psychiatry
https://ajp.psychiatryonline.org/ajp_ifora

ARTICLES
Articles are reports of original work that embodies scientific excellence in psychiatric medicine and advances in clinical research. Typically, articles will contain new data derived from a sizable series of patients or subjects. The text is usually within 3,500 words, which does not include an abstract of no more than 250 words, a maximum of 5 tables and figures (total), and up to 40 references. Word count includes only the main body of text (i.e., not tables, figures, abstracts or references). Additional tables can be submitted in a separate file as supplemental data for posting online. (See Supplemental Data for what types of data and formats are acceptable for posting online.)
-->

<!--terminology
Prediction window, window width, week, day, or hour model (always in that order)
relapse (a goal-inconsistent return to harmful substance use) and lapse (a single instance of goal-inconsistent substance use) 
-->

<!--Notes from lab meeting

Intro:
- language around AUD/SUD/psychiatric disorders - DONE by JJC
- reduced focus on digital therapeutics/increased focus on personal sensing & prediction algorithm/changed headings - PARTIALLY DONE BY JJC
- add information about specific features in “current study” and how they map onto relapse prevention literature -TBD; JC MARKED LOCATION FOR A FEW SENTENCES.

Methods:
- improve terminology and definitions of things like test set, validation set, etc - so that we can just use simpler language throughout

Results:
- shorten?/simplify?
- remove “methods” sentences from results
- determine how to “answer question” (how do we know if our AUC is good, did we answer the question about “can we predict lapses”/“how accurately can we predict lapses”)

Discussion:
-add to limitations/future directions: no existing method to say whether an AUC is clinically useful or whether a SHAP value (global value or local variability) is meaningful
-->



```{r setup}
library(knitr)
# library(yardstick) # for roc_curve
library(kableExtra)
library(janitor)
# library(corx)
library(patchwork)
library(ggtext)
library(consort)
library(tidyverse)
library(tidymodels)
library(tidyposterior)
library(cowplot)

theme_set(theme_classic()) 
```

```{r knitr_settings}
options(knitr.kable.NA = '')
knitr::opts_chunk$set(fig.pos = "ht", out.extra = "")
```

```{r paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- "P:/studydata/risk/chtc/ema"
          path_processed <- "P:/studydata/risk/data_processed/ema"
          path_models <- "P:/studydata/risk/models/ema"
          path_shared <- "P:/studydata/risk/data_processed/shared"
          path_manuscript <- "P:/studydata/risk/manuscripts/EMA"},

        # IOS paths
        Darwin = {
          path_input <- "/Volumes/private/studydata/risk/chtc/ema"
          path_processed <- "/Volumes/private/studydata/risk/data_processed/ema"
          path_models <- "/Volumes/private/studydata/risk/models/ema"
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"
          path_manuscript <- "/Volumes/private/studydata/risk/manuscripts/EMA"},
        
        # Linux paths
        Linux = {
          path_input <- "~/mnt/private/studydata/risk/chtc/ema"
          path_processed <- "~/mnt/private/studydata/risk/data_processed/ema"
          path_models <- "~/mnt/private/studydata/risk/models/ema"
          path_shared <- "~/mnt/private/studydata/risk/data_processed/shared"
          path_manuscript <- "~/mnt/private/studydata/risk/manuscripts/EMA"}
        )
```


```{r load_data}
# For table 1
disposition <- read_csv(file.path(path_processed, "disposition.csv"), 
                        col_types = "ccDDcccccccccc")
# For table 1
screen <- read_csv(file.path(path_shared, "screen.csv"), 
                   col_types = cols()) |>
  filter(subid %in% subset(disposition, analysis == "yes")$subid)

lapses <- read_csv(file.path(path_shared, "lapses.csv"), col_types = cols()) |>
  filter(exclude == FALSE)

# lapse labels
labels_week <- read_csv(file.path(path_processed, "labels_1week.csv"), col_types = cols())
labels_day <- read_csv(file.path(path_processed, "labels_1day.csv"), col_types = cols())
labels_hour <- read_csv(file.path(path_processed, "labels_1hour.csv"), col_types = cols())


# Predictions data
preds_week<- read_rds(file.path(path_models, "outer_preds_1week_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)
preds_day<- read_rds(file.path(path_models, "outer_preds_1day_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)
preds_hour<- read_rds(file.path(path_models, "outer_preds_1hour_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)

auc_week <- read_rds(file.path(path_models, "outer_metrics_1week_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))
auc_day <- read_rds(file.path(path_models, "outer_metrics_1day_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))
auc_hour <- read_rds(file.path(path_models, "outer_metrics_1hour_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))

# ROC curves
roc_week <- preds_week |>  
  roc_curve(prob, truth = label) |>  
  mutate(model = "1week")

roc_day <- preds_day |>  
  roc_curve(prob, truth = label) |>  
  mutate(model = "1day")

roc_hour <- preds_hour|>  
  roc_curve(prob, truth = label) |>  
  mutate(model = "1hour")

roc_all <- roc_week |>  
  bind_rows(roc_day) |>  
  bind_rows(roc_hour)

# PR curves
pr_week <- preds_week |>  
  pr_curve(prob, truth = label) |>  
  mutate(model = "1week")

pr_day <- preds_day |>  
  pr_curve(prob, truth = label) |>  
  mutate(model = "1day")

pr_hour <- preds_hour|>  
  pr_curve(prob, truth = label) |>  
  mutate(model = "1hour")

pr_all <- pr_week |>  
  bind_rows(pr_day) |>  
  bind_rows(pr_hour)


# posterior probabilities
pp <- read_rds(file.path(path_models, "posteriors_all_0_v5_nested.rds"))

pp_tidy <- pp |>  
  tidy(seed = 123)

q = c(.025, .5, .975)
ci <- pp_tidy |>  
  group_by(model) |>  
  summarize(median = quantile(posterior, probs = q[2]),
            lower = quantile(posterior, probs = q[1]), 
            upper = quantile(posterior, probs = q[3])) |>  
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour")),
         y = 1000) |> 
  arrange(model)

pp_diffs <- pp |>  
  contrast_models(list("hour","hour", "day"), 
                list("week", "day", "week"))

ci_diffs <- pp_diffs |> 
  group_by(contrast) |>  
  summarize(median = quantile(difference, probs = q[2]),
            lower = quantile(difference, probs = q[1]), 
            upper = quantile(difference, probs = q[3]))


# SHAPS
shap_local_week <- read_rds(file.path(path_models, "outer_shapsgrp_1week_0_v5_nested.rds")) 
shap_local_day <- read_rds(file.path(path_models, "outer_shapsgrp_1day_0_v5_nested.rds"))
shap_local_hour <- read_rds(file.path(path_models, "outer_shapsgrp_1hour_0_v5_nested.rds")) 

shap_global_week <- shap_local_week |>  
  group_by(variable_grp) |>  
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>  
  arrange(mean_value) |> 
  mutate(model = "Week")
shap_global_day <- shap_local_day |>  
  group_by(variable_grp) |>  
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>  
  arrange(mean_value) |> 
  mutate(model = "Day")
shap_global_hour <- shap_local_hour |>  
  group_by(variable_grp) |>  
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>  
  arrange(mean_value) |> 
  mutate(model = "Hour")
shap_global_all <- shap_global_week |> 
  bind_rows(shap_global_day) |> 
  bind_rows(shap_global_hour) |> 
  mutate(model = factor(model, levels = c("Week", "Day", "Hour"))) |>  
  mutate(variable_grp = factor(variable_grp, levels = c("past use (EMA item)", 
                                          "craving (EMA item)", 
                                          "past risky situation (EMA item)", 
                                          "past stressful event (EMA item)", 
                                          "past pleasant event (EMA item)", 
                                          "valence (EMA item)", 
                                          "arousal (EMA item)", 
                                          "future risky situation (EMA item)", 
                                          "future stressful event (EMA item)", 
                                          "future efficacy (EMA item)",
                                          "lapse day (other)",
                                          "lapse hour (other)",
                                          "missing surveys (other)",
                                          "age (demographic)",
                                          "sex (demographic)",
                                          "race (demographic)",
                                          "marital (demographic)",
                                          "education (demographic)")))
```

```{r table_1_calcs}

# Calcs to make df for table 1 (demographics and clinical characteristics)
n_total <- 151

dem <- screen |>  
  summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
            SD = as.character(round(sd(dem_1, na.rm = TRUE), 1)),
            min = as.character(min(dem_1, na.rm = TRUE)),
            max = as.character(max(dem_1, na.rm = TRUE))) |>  
  mutate(var = "Age",
         n = as.numeric(""),
         perc = as.numeric("")) |>  
  select(var, n, perc, everything()) |>  
  full_join(screen |>  
  select(var = dem_2) |>  
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_3) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("American Indian/Alaska Native", "Asian", "Black/African American",
                           "White/Caucasian", "Other/Multiracial")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_4) |>  
  mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
                         TRUE ~ "Yes"),
         var = fct_relevel(factor(var, c("Yes", "No")))) |>  
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_5) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("Less than high school or GED degree", "High school or GED", 
                           "Some college", "2-Year degree", "College degree", "Advanced degree")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_6, dem_6_1) |>  
  mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
                         dem_6_1 == "Part-time" ~ "Employed part-time",
                         TRUE ~ var)) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("Employed full-time", "Employed part-time", "Full-time student",
                           "Homemaker", "Disabled", "Retired", "Unemployed", 
                           "Temporarily laid off, sick leave, or maternity leave",
                           "Other, not otherwise specified")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  summarise(mean = format(round(mean(dem_7, na.rm = TRUE), 0), big.mark = ","),
            SD = format(round(sd(dem_7, na.rm = TRUE), 0), big.mark = ","),
            min =format(round(min(dem_7, na.rm = TRUE), 0), big.mark = ","),
            max = format(round(max(dem_7, na.rm = TRUE), 0), scientific = FALSE, big.mark = ",")) |>  
  mutate(var = "Personal Income",
        n = as.numeric(""),
        perc = as.numeric(""),
        mean = str_c("$", as.character(mean)),
        SD = str_c("$", as.character(SD)),
        min = str_c("$", as.character(min)),
        max = as.character(max)) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", "min", "max")) |>  
  full_join(screen |>  
  select(var = dem_8) |>  
  mutate(var = case_when(var == "Never Married" ~ "Never married",
                         TRUE ~ var)) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("Never married", "Married", "Divorced", "Separated",
                           "Widowed")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))

auh <- screen |>  
  summarise(mean = mean(auh_1, na.rm = TRUE),
            SD = sd(auh_1, na.rm = TRUE),
            min = min(auh_1, na.rm = TRUE),
            max = max(auh_1, na.rm = TRUE)) |>  
  mutate(var = "Age of first drink",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()) |>  
  full_join(screen |>  
  summarise(mean = mean(auh_2, na.rm = TRUE),
            SD = sd(auh_2, na.rm = TRUE),
            min = min(auh_2, na.rm = TRUE),
            max = max(auh_2, na.rm = TRUE)) |>  
  mutate(var = "Age of regular drinking",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", 
                                             "min", "max")) |>  
  full_join(screen |>  
  summarise(mean = mean(auh_3, na.rm = TRUE),
            SD = sd(auh_3, na.rm = TRUE),
            min = min(auh_3, na.rm = TRUE),
            max = max(auh_3, na.rm = TRUE)) |>  
  mutate(var = "Age at which drinking became problematic",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  summarise(mean = mean(auh_4, na.rm = TRUE),
            SD = sd(auh_4, na.rm = TRUE),
            min = min(auh_4, na.rm = TRUE),
            max = max(auh_4, na.rm = TRUE)) |>  
  mutate(var = "Age of first quit attempt",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  # filter out 2 people with 100 and 365 reported quit attempts - will make footnote in table
  filter(auh_5 < 100) |>  
  summarise(mean = mean(auh_5, na.rm = TRUE),
            SD = sd(auh_5, na.rm = TRUE),
            min = min(auh_5, na.rm = TRUE),
            max = max(auh_5, na.rm = TRUE)) |>  
  mutate(var = "Number of Quit Attempts*",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  select(var = auh_6_1) |> 
  mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ months)",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_2) |> 
  mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 months)",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_3) |> 
  mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_4) |> 
  mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_5) |> 
  mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_6) |> 
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_7) |> 
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_7) |>  
  mutate(var = fct_relevel(factor(var, c("Yes", "No")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) |>  
  rowwise() |>  
  # calculate dsm5 score by adding up dsm5_1 through dsm5_11
  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7, 
                            dsm5_8, dsm5_9, dsm5_10, dsm5_11))) |>  
  ungroup() |>  
  summarise(mean = mean(dsm5_total),
            SD = sd(dsm5_total),
            min = min(dsm5_total, na.rm = TRUE),
            max = max(dsm5_total, na.rm = TRUE)) |>  
  mutate(var = "DSM-5 Alcohol Use Disorder Symptom Count",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  select(var = assist_2_1) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_2) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Cannabis (marijuana, pot, grass, hash, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_3) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Cocaine (coke, crack, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_4) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_5) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Inhalants (nitrous, glue, petrol, paint thinner, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_6) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_7) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_8) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Opioids (heroin, morphine, methadone, codeine, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) 

lapses_per_subid <- screen |>  
  select(subid) |>  
  left_join(lapses |>  
  tabyl(subid) |>  
  select(-percent), by = "subid") |>  
  mutate(n = if_else(is.na(n), 0, n),
         lapse = if_else(n > 0, "yes", "no")) 

lapse_info <- lapses_per_subid |>  
  group_by(lapse) |>  
  rename(var = lapse) |>  
  mutate(var = factor(var, levels = c("yes", "no"), labels = c("Yes", "No"))) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100,
         mean = NA_real_,
         SD = NA_real_,
         min = NA_real_,
         max = NA_real_) |>  
  full_join(lapses_per_subid |>  
  summarise(mean = mean(n),
            SD = sd(n),
            min = min(n),
            max = max(n)) |>  
  mutate(var = "Number of reported lapses"), 
  by = c("var", "mean", "SD", "min", "max"))
```

```{r table_2_calcs}

# Calcs to create df for table 2 (performance metrics at Youden's Index)
j_thres_week <- roc_week |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)

j_thres_day <- roc_day |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)

j_thres_hour <- roc_hour |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)


metrics_week <- preds_week |> 
  mutate(estimate = if_else(prob > j_thres_week, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(week = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics_day <- preds_day |> 
  mutate(estimate = if_else(prob > j_thres_day, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(day = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics_hour <- preds_hour |> 
  mutate(estimate = if_else(prob > j_thres_hour, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(hour = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics <- metrics_week |>  
  full_join(metrics_day, by = "metric") |>  
  full_join(metrics_hour, by = "metric") |>  
  filter(metric %in% c("bal_accuracy", "sens", "spec", "ppv", "npv"))

auc <- tibble(metric = "auROC", 
              week = preds_week |>  roc_auc(prob, truth = label) |>   
                pull(.estimate) |>  round(3), 
              day = preds_day |>  roc_auc(prob, truth = label) |>   
                pull(.estimate) |>  round(3),
              hour = preds_hour |>  roc_auc(prob, truth = label) |>   
                pull(.estimate) |>  round(3))

metrics <- metrics |>  
  bind_rows(auc)

metrics <- metrics[c(6,1,2,5,3,4),]
```



```{r ppv_70_values}
# PPV and sens values at ppv = .70
ppv_70 <- pr_all |>  
  mutate(recall = round(recall, 3),
         precision = round(precision, 3),
         .threshold = round(.threshold, 3),
         model = factor(model,
                        levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) |>  
  filter(precision == .70) |>  
  group_by(model, precision) |>  
  summarise(recall = mean(recall),
            threshold = mean(.threshold),
            .groups = "drop")
```
# Introduction
<!-- SS: currently 1,345 words. Wondering about further combining the first three paragraphs of the improving digital therapeudics via personal sensing section? -->
<!-- GEF: now down to 1288 words-->

Alcohol use disorder (AUD) is highly prevalent and costly. Among United States (U.S.) adults, over 30 million had an active AUD in 2021, and 23.3% reported engaging in past-month binge drinking [@samhsa2021NationalSurvey; @samhsaTable1ASubstance; @samhsaTable9BAlcohol]. Alcohol ranks as the third leading preventable cause of death, accounting for approximately 140,000 fatalities per year [@centersfordiseasecontrolandpreventionAlcoholPublicHealth; @esserEstimatedDeathsAttributable2022]. Economically, alcohol misuse cost the United States $249 billion in 2016 alone [@administrationusFacingAddictionAmerica2016]. 

Existing clinician-delivered treatments for AUD such as cognitive-behavioral therapy [@mchughCognitiveBehavioralTherapySubstance2010; @lieseCognitiveBehavioralTherapyAddictive2022], mindfulness-based relapse prevention [@bowenMindfulnessBasedRelapsePrevention2021; @goldbergMindfulnessbasedInterventionsPsychiatric2018], motivational interviewing [@millerMotivationalInterviewingHelping2012], and contingency management [@bigelowTheoreticalEmpiricalFoundations1999; @dutraMetaAnalyticReviewPsychosocial2008<!--can cut some refs eventually-->] are effective when provided to patients.  Unfortunately, fewer than 1 in 20 adults with an active AUD receive any treatment[@samhsaTable22BSubstances].  Even more concerning, the failure to access treatment is associated with demographic factors including race, ethnicity, geographic region, and socioeconomic status, which further increase mental health disparities [@wangFailureDelayInitial2005; @MentalHealthReport1999; @generalusMentalHealthCulture2001; @mauraMentalHealthDisparities2017; @novakChangesHealthInsurance2018]. This treatment gap and associated disparities stem from well-known barriers to receiving clinician-delivered mental healthcare related to affordability, accessibility, availability, and acceptability[@jacobsonUsingDigitalTherapeutics2023].

Digital therapeutics may help to combat these disparities. Digital therapeutics deliver evidence-based treatments through a smartphone "app" and are used to prevent, treat, or manage a medical disorder, either independently or in conjunction with traditional treatments [@jacobsonDigitalTherapeuticsMental2022 <!--Can re-use this reference later rather than chapters if needed to get cites <= 40-->]. They offer highly scalable, on-demand therapeutic support that is accessible whenever and wherever it is needed most. Several large, randomized controlled trials have confirmed that digital therapeutics for AUD improve clinical outcomes (e.g., abstinence, heavy drinking days[@gustafsonSmartphoneApplicationSupport2014; @campbellInternetdeliveredTreatmentSubstance2014]; see Campbell et al. [@campbellFirstWaveScalable2023] for review). Additionally, U.S. adults display high rates of smartphone ownership (approximately 85% in April 2021), with minimal variation across race, ethnicity, socioeconomic status, geographic settings (e.g., urban, suburban, rural), or AUD diagnosis [@pewresearchcenterMobileFactSheet2021; @collinsFactorsAssociatedPatterns2016]. Therefore, digital therapeutics may mitigate many of the barriers associated with in-person, clinician-delivered treatments.

## Improving Digital Therapeutics via Personal Sensing

Despite the documented benefits of digital therapeutics, their full potential has not yet been realized.  Patients often don't engage with them as developers intended, and long-term engagement may not be sustained or matched to patients' needs [@hatchExpertConsensusSurvey2018; @lattieDigitalMentalHealth2019; @ngUserEngagementMental2019; @yeagerIfWeBuild2018].  The substantial benefits of digital therapeutics come from easy, 24/7 access to their treatment tools and other support services.  However, identifying the most appropriate tool remains a burden primarily placed on the patient. 

This difficulty is magnified by the dynamic, chronic, and relapsing nature of AUD [@brandonRelapseRelapsePrevention2007; @mclellanDrugDependenceChronic2000]. Numerous risk and protective factors interact in complex, non-linear ways to influence the probability, timing, and severity of relapse (i.e., a goal-inconsistent return to frequent, harmful substance use) [@hendershotRelapsePreventionAddictive2011; @witkiewitzRelapsePreventionAlcohol2004; @huffordRelapseNonlinearDynamic2003; @witkiewitzNonnormalityDivergencePosttreatment2007; @witkiewitzModelingComplexityPosttreatment2007]. Factors such as urges, mood, lifestyle imbalances, self-efficacy, and motivation can all vary over time. Social networks may evolve to be more protective or risky, and high-risk situations can arise unexpectedly. Consequently, relapse risk fluctuates. 

<!-- GEF: note that I changed where the paragraph break is between the next two paragraphs. previously, the sentence about personal sensing started the next paragraph, but that felt like an inaccurate topic sentence for the EMA-focused paragraph and instead felt like a good closing sentence for the continuous monitoring paragraph-->
Successful continuous monitoring of risk for lapse (i.e., a single instance of goal-inconsistent substance use), relapse, and their contributing factors would enable patients to adapt their lifestyle, behaviors, and supports to their changing needs. Successful monitoring could also direct patients to engage with the most appropriate treatment tools within a digital therapeutic, addressing the unique risks present at any given moment throughout their recovery. Until recently, the continuous monitoring of intra- and interpersonal risk factors required to track and forecast psychiatric disorders has proved challenging.<!-- cite?--> However, this continuous monitoring is now feasible via moment-by-moment personal sensing (i.e., in-situ data collection via sensors embedded in individuals' day to day lives)[@epsteinPredictionStressDrug2020; @suchtingUsingElasticNet2019; @hebertPredictingFirstSmoking2021a; @engelhardPredictingSmokingEvents2018; @businelleUsingIntensiveLongitudinal2016; @soysterPooledPersonspecificMachine2022; @hebertEcologicalMomentaryIntervention2018; @moshontzProspectivePredictionLapses2021; @wyantAcceptabilityPersonalSensing2022; @chihPredictiveModelingAddiction2014; @baeMobilePhoneSensors2018]. 

The current project focuses explicitly on the use of ecological momentary assessment (EMA) to monitor AUD lapse risk. EMA can be easily implemented with only a smartphone. Moreover, comparable raw data (items and responses) can be used consistently across different hardware and operating systems. Thus, EMA can be incorporated essentially identically into any existing or future smartphone-based digital therapeutic. EMA, like other personal sensing methods, can support the frequent, in situ longitudinal measurement necessary for monitoring episodic or otherwise fluctuating relapse risk.  Long-term monitoring with EMA has been well-tolerated by individuals with AUD [@wyantAcceptabilityPersonalSensing2022].  Previous research has validated the use of EMA to measure theoretically implicated risk and protective factors for relapse including craving [@dulinSmartphonebasedMomentaryIntervention2017], mood [@russellAffectRelativeDayLevel2020], stressors[@wemmDaybydayProspectiveAnalysis2019], positive life events[@dvorakTensionReductionAffect2018], and motivation/efficacy [@dvorakEcologicalMomentaryAssessment2014].  Furthermore, EMA provides privileged access to these subjective constructs that may be difficult to quantify reliably through other sensing methods.  

## Promising Preliminary Research
Preliminary research is now emerging that uses EMA as features in machine learning models to predict the probability of future alcohol use [@baeMobilePhoneSensors2018; @soysterPooledPersonspecificMachine2022; @waltersUsingMachineLearning2021].  This research is important because it rigorously establishes temporal ordering between the predictors (features engineered from EMAs) and the outcomes. Additionally, the use of resampling methods prioritizes model generalizability (i.e., models are evaluated on data not used for training).

Despite this initial promise, several important gaps exist.  Prediction models developed with convenience samples (e.g., college students) [@soysterPooledPersonspecificMachine2022; @baeMobilePhoneSensors2018] may not generalize to clinical samples (i.e., people with AUD).  Similarly, models that have been developed to predict alcohol use in non-treatment-seeking hazardous drinking populations may be less clinically useful [@waltersUsingMachineLearning2021]. These individuals are unlikely to use digital therapeutics to support their recovery until they begin to contemplate and/or commit to behavior change regarding their alcohol use [@prochaskaSearchHowPeople1992].  Moreover, features that predict planned or otherwise intentional alcohol use among individuals that are not motivated to change their behavior may not generalize to patients in recovery.  

A handful of studies have trained models to predict putative precursors of substance use, such as craving [@burgess-hullTrajectoriesCravingMedicationassisted2022; @dumortierClassifyingSmokingUrges2016] and stress [@epsteinPredictionStressDrug2020]. However, even though craving and stress can be associated with substance use, their relationships with lapse and relapse are complex, inconsistent, and not always very strong [@fronkStressAllostasisSubstance2020, @sayetteRoleCravingSubstance2016]. 

Models that predict lapses may be preferred. Lapses are clearly defined, observable, and finite (with crisp onset and offset) instances of goal-inconsistent behavior. Definitions of relapse vary [@sliedrechtVarietyAlcoholUse2022], and it is difficult to clearly delineate when a lapse or series of lapses progresses to full relapse.  Furthermore, in many instances, a single or series of lapses may precede full relapse and therefore serve as early warnings signs that provide opportunity for intervention. Finally, maladaptive responses to individual lapses (e.g., abstinence violation effects; [@marlattRelapsePreventionMaintenance1985]) can undermine recovery by themselves, making them clinically meaningful events to detect and address.

<!--KW: confirm Chih et al is only alcohol lapse prediction paper?-->
An early lapse prediction model developed by Gustafson et al. [@chihPredictiveModelingAddiction2014] provided the foundation on which our current project builds. Participants with AUD completed weekly EMAs for 8 months while using a digital therapeutic (A-CHESS). This study used a clinical sample committed to AUD recovery-related behavior change to predict a clinically meaningful outcome. However, the temporal precision for both the machine learning features and outcome was coarse. Model predictions were updated only once per week at best, and lapse onsets could occur anytime within the next two weeks. This coarseness restricts the model from being used to implement "just-in-time" interventions (e.g., guided mindfulness or other stress reduction techniques, urge surfing), well-suited to digital therapeutics.

## The Current Study

The purpose of this project was to build a model using theoretically-implicated features to predict alcohol lapses. We designed the current study to address the gaps and limitations of previous prediction models.  We developed our models using participants in early recovery from moderate to severe AUD who reported a goal of alcohol abstinence.  We developed three separate models that provide hour-by-hour probabilities of a future lapse back to alcohol use with increasing temporal precision (i.e., lapses in the next week, next day, and next hour).  Model features were based on raw scores and longitudinal change in theoretically implicated risk factors[@marlattRelapsePreventionMaintenance1985; @witkiewitzTherapistGuideEvidenceBased2007]. Relapse prevention literature suggests lapses are preceded by a high-risk context that increases one's vulnerability to lapsing (e.g., emotional or cognitive states, environmental contingencies, and physiological states) [@marlattRelapsePreventionMaintenance1985; @hendershotRelapsePreventionAddictive2011]. We measured these constructs including past use, urges, risky situations (past and future), hassles or stressful events (past and future), pleasant or positive events, affect, and self-efficacy via 4x daily EMAs. This research represents an important step toward the development of a "smart" (machine learning guided) sensing system that can both identify periods of peak lapse risk and recommend specific supports to address factors contributing to this risk.  <!--KW: I added info on RP constructs-->  <!--JC to edit-->


# Method
<!-- SS: current length 3,044 words...-->

## Research Transparency
We value the principles of research transparency that are fundamental to the robustness and replicability of science and took several steps to follow open science guidelines. We reported how we determined our sample size, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]. We completed a transparency checklist (see Supplement; [@aczelConsensusbasedTransparencyChecklist2019]). Finally, we made the data, analysis scripts, annotated results, questionnaires, and other study materials associated with this report publicly available [https://osf.io/w5h9y/](https://osf.io/w5h9y/). 

<!--JC to add statement about power?-->
<!--KW will add transparency checklist to supplement-->

Throughout this project, we iteratively improved machine learning methods that are rapidly evolving in the social sciences and used in this study. However, we restricted many researcher degrees of freedom via cross-validation procedures that can robustly guide decision-making. Replication is built into cross-validation; models are fit using held-in training sets, decisions are made using held-out validation sets, and final model performance is evaluated in a confirmatory manner using held-out test sets.

## Participants
We recruited 151 participants in early recovery (1-8 weeks of abstinence) from AUD in Madison, Wisconsin, USA, to participate in a 3-month longitudinal study. Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required that participants:

1.  were 18 years of age or older,
2.  were able to write and read in English,
3.  had at least moderate AUD (\>= 4 DSM-5 symptoms^[We measured DSM-5 symptoms with a self-report survey administered to participants during the in-person screening visit.]),
4.  were abstinent from alcohol for at least 1 week but no longer than 2 months,
5.  were willing to use a single smartphone (their personal phone or one provided by us) while enrolled in the study.

We also excluded participants if they exhibited severe symptoms of psychosis or paranoia^[Psychosis and paranoia were defined as scores greater than 2.2 or 2.8, respectively, on the psychosis or paranoia scales of the on the Symptom Checklist – 90 (SCL-90) [@derogatisSCL90OutpatientPsychiatric1973].]. 

## Procedure
Participants completed five study visits over approximately three months. After an initial phone screen, participants attended an in-person screening visit where we determined eligibility, obtained informed consent, and collected self-report measures of individual differences (e.g., demographics, mental health, and alcohol use history; see Measures below). Eligible and consented participants returned approximately one week later to enroll in the study at an intake visit. Three additional follow-up visits occurred about every 30 days that participants remained on study. At each follow-up visit, we collected additional self-report and interview measures. 

Participants were expected to complete EMAs four times each day while on study. Other personal sensing data streams (geolocation, cellular communications, sleep quality, and audio check-ins) were collected as part of the parent grant's aims (R01 AA024391). A full description of the procedure and data collected at each visit can be found at the study's Open Science Framework (OSF) page [https://osf.io/w5h9y/](https://osf.io/w5h9y/). We compensated participants for all time spent in the laboratory (\$20/hour) and paid them \$66/month to offset costs associated with their cellular plan. We paid participants a \$99 bonus if they completed the study for the full 3-month duration. We also paid participants bonuses (ranging from \$10-\$25) if they had 10% or less missing data for that method each month (e.g., participants were paid a bonus of \$25 for each month that they completed at least 90% of their EMAs). All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

## Measures
### EMA
<!--citation for validity of self-reported alcohol use: https://pubmed.ncbi.nlm.nih.gov/26160523/-->
Participants completed a brief (7-10 questions) EMA four times each day following pushed text message reminders. These text messages included a link to a Qualtrics survey, optimized for completion on their smartphone. 

All four EMAs included seven items that asked about any alcohol use not yet reported, current affective state (pleasantness and arousal), greatest urge to drink alcohol since the last EMA, any pleasant or positive events, any hassles or stressful events, and any exposure to risky situations (i.e., people, places, or things) that occurred since the last EMA. The first EMA each day asked three additional questions about how likely participants were to encounter a risky situation, encounter a stressful event, and drink alcohol in the upcoming week. 

The first and last EMAs of the day were scheduled within one hour of participants' typical wake and sleep times. The other two EMAs were each scheduled randomly within the first and second halves of the participants' typical day. All EMAs were separated from each other by at least one hour.

### Individual Differences
At the screening visit we collected self-report information about demographics (age, sex, race, ethnicity, education, employment, personal income, and marital status) and clinical characteristics (AUD milestones, number of quit attempts, lifetime history of treatment for AUD, lifetime receipt of medication for AUD, DSM-5 AUD symptom count, and current drug use [@whoassistworkinggroupAlcoholSmokingSubstance2002]) to describe our sample. Only age, sex, race, education, and marital status are used as features in our machine learning models. A full description of all measures collected as part of the parent grant is available on the study's OSF page.

## Data Analytic Strategy
Data preprocessing and modeling were done in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020]. Bayesian analyses were accomplished using the tidyposterior [@kuhnTidyposteriorBayesianAnalysis2022] and rstanarm [@goodrichBayesianAppliedRegression2023] packages in R. All models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computer [@chtc].

### Lapse Labels
We created future prediction windows that varied in their temporal precision by using three distinct window widths (i.e., duration between window start and end time): 1 week, 1 day, and 1 hour.  For each participant, the start of the first prediction window for all three widths began at midnight on their second day of participation and ended 1 week, 1 day, or 1 hour later. By beginning at the end of the second day, we were assured that there would be at least 24 hours of past EMAs to use for future lapse prediction in these first windows.  Subsequent windows for each participant were created for all three widths by repeatedly rolling the window start/end forward 1 hour until the end their study participation was reached (i.e., each participant's last prediction window for each width ended at the date and hour of their last recorded EMA). By using windows that rolled by 1 hour, we were able to develop classification models that provided hour-by-hour predictions of future lapse probability for all three window widths.

We labeled each prediction window as "lapse" or "no lapse" using participants' reports from the EMA item "Have you drank any alcohol that you have not yet reported?". If participants answered yes to this question, they were prompted to enter the hour and date of the start and end of the drinking episode.  These reports were also validated by study staff during the monthly visits, and staff probed for any unreported lapses at that time using a timeline follow-back format. 

A prediction window was labeled "lapse" if the start date/hour of any drinking episode fell within that window.  Conversely, a window was labeled as "no lapse" if no alcohol use occurred within 24 hours of the window start/end.  If no alcohol use occurred within the window but did occur within the 24 hours of the start or end of the window, the window was excluded. We used this conservative 24-hour fence for labeling windows as "no lapse" (vs. excluded) to increase the fidelity of these labels.  Given that most windows were labeled "no lapse" (i.e., no lapse was the majority class, and the outcome was highly unbalanced), it was not problematic to exclude some "no lapse" events to further increase confidence in those labels.

### Feature Engineering
Features were calculated using only data that were collected prior to the start of each prediction window to ensure that our models were true "prediction" models (i.e., making future predictions rather than identifying concurrent associations).  

Features were derived from three sources.   The first source of features included demographic characteristics (i.e., age, sex, race, marital status, education) measured at baseline.  The second source of features used previous EMA responses that were collected prior to the start of the associated prediction window.  We used these EMA responses to create features using both raw (e.g., min., max., median, most recent response, and total counts) and change (e.g., within-subject baseline comparisons) scores. We scored raw min, max, median, and count features within a small set of periods prior to the start of the prediction window (6, 12, 24, 48, 72, and 168 hours prior to start of window).  We scored change features by subtracting the mean response for each feature over all data prior to the start of the prediction window from the associated raw feature.  The third source of features was based on the day of the week and the time of day (daytime vs. evening/night) of the start of the prediction window; these features were available for hour and week window widths only. 

Other generic feature engineering steps included: 1) imputation for missing data for features (median imputation for numeric features, mode imputation for nominal features); 2) dummy coding for nominal features; and 3) removal of any zero variance features. A sample feature engineering script (i.e., tidymodels recipe) containing all feature engineering steps is available on our OSF study page<!--KW will post one training controls file)-->.  Medians/mode for missing data imputation and identification of zero variance features were derived from training (held-in) data and applied to held out (validation and test) data to prevent issues associated with data leakage (see Model Training procedures below). 

### Model Training and Evaluation

#### Statistical Algorithm and Hyperparameters
We trained and evaluated three separate machine learning classification models (i.e., one for each of the three prediction window widths - week, day, and hour).  We initially considered four candidate statistical algorithms (XGBoost, Random Forest, K-Nearest Neighbors, and Elastic Net). These algorithms are well-established with documented good "out of box" performance, and they vary with respect to various characteristics expected to affect model performance (e.g., flexibility, ability to handle higher-order interactions natively, complexity, linear vs. non-linear) as well as the degree of feature selection performed automatically during model fitting [@kuhnAppliedPredictiveModeling2018]. However, preliminary exploratory analyses suggested that XGBoost consistently outperformed the other three algorithms.  Furthermore, the calculation of Shapley Additive Explanations (SHAP) values, which we planned to use for explanatory analyses of feature importance, are optimized for XGBoost.  For these reasons, we focused our primary model training and evaluation on the XGBoost algorithm only.  
  
We trained candidate XGBoost model configurations that differed across sensible values for the hyperparameters mtry, tree depth, and learning rate using grid search.  All configurations used 500 trees combined with early stopping to prevent over-fitting.  All other hyperparameters were set to defaults established by the tidymodels packages in R.  Candidate model configurations also differed with respect to the outcome resampling method.  Specifically, we used up-sampling and down-sampling of the outcome using majority (no lapse) to minority (lapse) ratios that ranged from 1:1 to 5:1 to address class imbalance (i.e., "lapse" windows were much less frequent than "no lapse" windows for all three window widths).  In addition, we calibrated the predicted probabilities from these XGBoost models using the beta distribution to support optimal decision-making under variable outcome class distributions [@kullSigmoidsHowObtain2017]

#### Performance Metric
Our primary performance metric for selecting and evaluating the best model configurations was the area under the Receiver Operating Characteristic Curve (auROC)[@kuhnAppliedPredictiveModeling2018]. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (i.e., lapse) relative to a randomly selected negative case (i.e., no lapse).  An auROC of 0.5 indicates chance performance; an AUC of 1.0 perfectly discriminates between positive and negative outcome classes.  This metric was selected because it 1) combines sensitivity and specificity, which are both important characteristics to consider for clinical implementation; 2) is an aggregate metric across all possible decision thresholds, which is important because the optimal decision threshold may differ depending on setting and goals; and 3) is not affected by class imbalance, which is important when comparing models across the three different prediction window widths that have different levels of class imbalance.  

#### Cross-validation 
We used grouped (by participant), nested cross-validation to train models, select best models, and evaluate those best models on auROC.  With grouped cross-validation, all a participant's data is either held-in or held-out to avoid bias that is introduced when predicting a participant's data from their own data[@saebVoodooMachineLearning2016].  

In nested cross-validation, there are two nested loops for dividing and holding out folds: an outer loop with k~outer~ folds, where held-out folds serve as test sets for model evaluation; and inner loops with k~inner~ folds, where held-out folds serve as validation sets for model selection. The full dataset is divided into k~outer~ approximately equal-sized, non-overlapping folds. Each k~outer~ fold serves once as an independent *test set* for model evaluation. The remaining data (i.e., the k~outer~ - 1 outer folds) are divided into k~inner~ folds. Each k~inner~ fold is used once as a *validation set* for models trained on k~inner~ - 1 inner folds (i.e., *training set*). Nested cross-validation maintains separation between *training sets* (k~inner~ - 1 folds), *validation sets* (k~inner~th held-out fold), and *test sets* (k~outer~th held-out fold). This separation removes optimization bias from the evaluation of model performance in the test sets and can also yield lower variance performance estimates than when a single independent test set is used [@jonathanUseCrossvalidationAssess2000].  <!--JC: we may need to shorten the description of nested further to save space but keeping full (but hopefully clearer) description for now--> <!-- GEF: John - I shortened just slightly to remove some redundancy and a bit of jargon ("simple k-fold") and to try to separate test/validation/training set descriptions-->

In this study, we used 1 repeat of 10-fold cross-validation for the inner loop and 3 repeats of 10-fold cross-validation for the outer loop.  Therefore, best model configurations were selected based on the median auROC across the 10 *validation sets*.  Final evaluation of the performance of those best model configurations was based on the median <!--JC will add footnote for why median-->auROC across the 30 *test sets*.  For completeness, we report median auROC for our best model configurations for each of the models (week, day, and hour) separately from both the validation and test sets. In addition, we report key additional performance metrics for the best model configurations including sensitivity, specificity, balanced accuracy, positive predictive value (PPV), and negative predictive value (NPV) from the test sets. We also display the Receiver Operating Characteristic (ROC) curves and Precision-Recall Curves for each model from the test sets[@jamesIntroductionStatisticalLearning2013; @kuhnAppliedPredictiveModeling2018].

### Bayesian Estimation of auROC and Model Comparisons 

We used a Bayesian hierarchical generalized linear model [@mcelreathStatisticalRethinkingBayesian2020] to estimate the posterior probability distributions for auROC for the three best models.  These analyses allowed us to calculate 95% Bayesian confidence intervals for the auROC for these models and to determine the probability that these models' performance (i.e., auROCs) differed systematically from each other.  We regressed the auROCs (logit transformed to address bounded, skewed distribution) from the 30 test sets for each model as a function of window width.  Following recommendations from the tidymodels team[@kuhnTidyposteriorBayesianAnalysis2022; @kuhnBayesianAnalysisResampling], we set two random intercepts: one for the repeat, and another for the fold within repeat (folds are nested with repeats for auROCs collected with 3x10-fold cross-validation).  Following recommendations from the rstanarm team and others [@rstudioteamRStudioIntegratedDevelopment2020; @gabryPriorDistributionsRstanarm2023], we used the rstanarm default autoscaled, weakly informative, data-dependent priors that take into account the order of magnitude of the variables to provide some regularization to stabilize computation and avoid over-fitting.  Specifically, the priors were set as follows: residual standard deviation ~ normal(location=0, scale=exp(2)), intercept (after centering predictors) ~ normal(location=2.3, scale=1.3), the two coefficients for window width contrasts ~ normal (location=0, scale=2.69), and covariance ~ decov(regularization=1, concentration=1, shape=1, scale=1).  <!-- GEF: John - I think that the previous 3 sentences (starting with "Following recs from the tidymodels team") could be moved to a footnote or the Supplement. The detail is important, but I think it distracts from the primary steps and purpose of the Bayesian analyses-->We report the 95% (equal-tailed) Bayesian confidence intervals (i.e., intervals that have 0.95 posterior probability of containing the true population value for the parameter) from the posterior probability distributions for auROC for the the models.  We also report 95% (equal-tailed) Bayesian confidence intervals for the differences in performance among the three models.  



### Shapley Additive Explanations for Feature Importance

We used the Shapley Additive Explanations (SHAP) method [@lundbergUnifiedApproachInterpreting2017] to provide a consistent and objective explanation of the importance for model predictions associated with categories of features from the three models.  SHAP computes Shapley values, which have a solid theoretical foundation in game theory <!-- GEF: John - the clause about game theory seems unimportant to me. if you do maintain it, it may need a citation-->.  Shapley values are model-agnostic (i.e., can be computed for any statistical algorithm) and possess several useful properties including: Additivity (Shapley values for each feature can be computed independently and then summed); Efficiency (the sum of Shapley values across features must add up to the difference between the predicted and observed outcome for each observation); Symmetry (Shapley values for two features should be equal if the two features contribute equally to all possible coalitions); and Dummy (a feature that does not change the predicted value in any coalition will have a Shapley value of 0).  Finally, SHAP has a fast implementation for tree-based models that makes its use computationally feasible for the XGBoost algorithm, even with large sample sizes.  We calculated Shapley values from the 30 test sets using the SHAPforxgboost package that provides Shapley values in log-odds units for binary classification models.  We averaged the three Shapley values for each observation for each feature across the three repeats to increase their stability. To calculate the local (i.e., for each observation) impact of categories of features (e.g., all features associated with the EMA urge item), we added Shapley values across all features in a category, separately for each observation.  We display these local Shapley values to document the range of effects of feature categories described earlier on lapse predictions for each model <!--GEF: John - not sure this sentence or the final one about our plan to display them is necessary - we'll just have the figures in the results!-->. To calculate global importance for categories of features, we averaged the absolute value of the Shapley values of all features in the category across all observations.  We display these global importance scores for feature categories for each model.  



\newpage


# Results

## Demographic and Clinical Characteristics

One hundred ninety-two participants were eligible for enrollment. Of these participants, 191 consented to participate in the study at the screening session, and 169 subsequently enrolled in the study at the intake visit which occurred approximately one week later. Fifteen participants discontinued prior to the first monthly follow-up visit. 

We excluded data from one participant who did not maintain a goal of abstinence during their participation (i.e., they reported they were uncertain if their goal was abstinence on the daily EMA and monthly follow-up surveys). We also excluded data from two participants who showed evidence of careless responding and unusually low compliance, rendering their lapse labels unusable. Our final sample consisted of 151 participants. These participants provided study measures for one (N = 14), two (N = 6) or three (N = 131) months. Figure S1 presents a CONSORT diagram that displays more detail on enrollment and disposition for all eligible participants. 

The final sample of 151 participants included approximately equal numbers of men (N=77; 51%) and women (N=74; 49%) who ranged in age from 21 - 72 years old.  The sample was majority White (N=131; 87%) and non-Hispanic (N=147; 97%).  Participants self-reported a mean of 8.9 DSM-5 symptoms of alcohol use disorder (SD=5.8; range=4-11), a mean of 5.5 previous quit attempts (SD=5.8, range=0-30), and many reported using medications for AUD (N=59; 39%).  Most participants (N=84; 56%) reported one or more alcohol lapses during the study period.  The mean number of lapses per participant during the study period was 6.8 (SD=12.0; range=0-75).  Table 1 provides more detail on demographic and clinical characteristics of the sample.
<!--KW - update supplemental figure numbers because we deleted S2-->

\newpage
<!--************************************************************************-->
<!-- Table 1: Demographics and clinical characteristics-->

<!--UPDATE -JJC handle description of outliers for quit attempts-->
```{r dem_table}
#| output: true

footnote_table_dem_a <- "N = 151"

footnote_table_dem_b <- "Two participants reported 100 or more quit attempts. We removed these outliers prior to calculating the mean (M), standard deviation (SD), and range."

dem  |>  
  bind_rows(auh |>  
              mutate(across(mean:max, ~round(.x, 1))) |>  
              mutate(across(mean:max, ~as.character(.x)))) |>  
  bind_rows(lapse_info |>  
              mutate(across(mean:max, ~round(.x, 1))) |>  
              mutate(across(mean:max, ~as.character(.x)))) |>  
  mutate(range = str_c(min, "-", max)) |> 
  select(-c(min, max)) |>  
  kbl(longtable = TRUE,
      booktabs = TRUE,
      col.names = c("", "N", "%", "M", "SD", "Range"),
      align = c("l", "c", "c", "c", "c", "c"),
      digits = 1,
      caption = "Demographics and Clinical Characteristics for the Sample") |> 
  kable_styling() |>  
  row_spec(row = 0, align = "c", italic = TRUE) |>  
  pack_rows("Sex", 2, 3, bold = FALSE) |>  
  pack_rows("Race", 4, 8, bold = FALSE) |>  
  pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) |>  
  pack_rows("Education", 11, 16, bold = FALSE) |>  
  pack_rows("Employment", 17, 25, bold = FALSE) |>  
  pack_rows("Marital Status", 27, 31, bold = FALSE) |>  
  pack_rows("Alcohol Use Disorder Milestones", 32, 35, bold = FALSE) |>  
  pack_rows("Lifetime History of Treatment (Can choose more than 1)", 37, 43, bold = FALSE) |>  
  pack_rows("Received Medication for Alcohol Use Disorder", 44, 45, bold = FALSE) |>  
  pack_rows("Current (Past 3 Month) Drug Use", 47, 54, bold = FALSE) |>  
  pack_rows("Reported 1 or More Lapse During Study Period", 55, 56, bold = FALSE) |>  
  footnote(general=footnote_table_dem_a, symbol = c(footnote_table_dem_b), 
           threeparttable = TRUE)
```


\newpage


## EMA compliance, features (predictors), and prediction window labels (outcome)

EMA compliance over the study was generally good, with participants completing an average of 3.1 (SD=0.6) of the 4 daily EMAs per day or 78% compliance overall.  Participants completed at least 1 EMA on 95% of days.  Across individual weeks in the study, EMA compliance percentages ranged from 75% - 87% completion for all of the 4x daily EMAs and from 92% - 99% for at least 1 daily EMA completed.  Figure S3 displays completion percentages over time (by week) across the study period for both 4x daily and at least one daily report.  

Using these EMA reports, we created datasets with 270,081, 274,179, and 267,287 future prediction windows for the outcome that were labeled as "lapse" or "no lapse" for the week, day, and hour window widths, respectively.  Each of these datasets also contained 286 features that were derived from 1) EMAs that were completed prior to the start of the associated prediction window, 2) baseline demographic characteristics, and 3) prediction window start dates/times as described earlier in the method. Each of these datasets was unbalanced with respect to the outcome such that lapses were observed in 68,467 (25.3%) week windows, 21,107 (7.7%) day windows, and 1,017 (0.3%) hour windows. 

 
## Model Performance

### auROC

Best model configurations were selected via validation set performance. The median auROCs from the validation sets for the best configurations were high for the week (median=`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> median())`, IQR=`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> IQR())`, range=`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> min())`-`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> max())`), day (median=`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> median())`, IQR=`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> IQR())`, range=`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> min())`-`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> max())`), and hour (median=`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> median())`, IQR=`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> IQR())`, range=`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> min())`-`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> max())`) prediction windows. 

Best model configurations were evaluated via test set performance. The median auROC across the 30 test sets remained high for the week (median=`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> median())`, IQR=`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> IQR())`, range=`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> min())`-`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> max())`), day (median=`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> median())`, IQR=`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> IQR())`, range=`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> min())`-`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> max())`), and hour (median=`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> median())`, IQR=`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> IQR())`, range=`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> min())`-`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> max())`) prediction windows.  The left panel of Figure 1 displays the Receiver Operating Characteristic curves by model (i.e., window width) derived by aggregating predicted lapse probabilities across these same 30 test sets.  Figure S4 presents the individual ROC curves from each of the 30 test sets.  

Posterior probability distributions for the week, day, and hour models are displayed separately for each model in the right panels of Figure 1.  The median auROCs from these posterior distributions were `r sprintf("%1.2f", ci |> filter(model=="Week") |> pull(median) |> as.numeric())`,  `r sprintf("%1.2f", ci |> filter(model=="Day") |> pull(median) |> as.numeric())`, and `r sprintf("%1.2f", ci |> filter(model=="Hour") |> pull(median) |> as.numeric())` for the week, day, and hour models, respectively.  These median values from the Bayesian posterior distributions represent our best estimates for magnitude of the auROC parameter for the models for these three prediction windows.  The 95% Bayesian confidence intervals (95% CI) for the auROCs for these models were relatively narrow and did not contain 0.5 for any of the three window widths: week [`r sprintf("%1.2f", ci |> filter(model=="Week") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci |> filter(model=="Week") |> pull(upper) |> as.numeric())`], day [`r sprintf("%1.2f", ci |> filter(model=="Day") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci |> filter(model=="Day") |> pull(upper) |> as.numeric())`], hour [`r sprintf("%1.2f", ci |> filter(model=="Hour") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci |> filter(model=="Hour") |> pull(upper) |> as.numeric())`].  The right panel of Figure 1 displays histograms (and 95% CIs) of the posterior probability distribution for the auROC by model. <!-- GEF: John - I think this is what is already stated in the opening sentence? Just confirming in case you meant to reference something else here-->  

We also used these posterior probability distributions for the auROCs of the three models to formally compare the differences in performance of these models.  The median increase in auROC for the hour vs. the day model was `r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. day") |> pull(median) |> as.numeric())` (95% CI=[`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. day") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. day") |> pull(upper) |> as.numeric())`], yielding a probability of `r sprintf("%1.3f", pp_diffs |> filter(contrast=="hour vs. day") |> pull(difference) |> (\(x) mean(x>0))())` that the hour model had superior performance relative to the day model.  The median increase in auROC for the hour vs. the week model was `r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. week") |> pull(median) |> as.numeric())` (95% CI=[`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. week") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. week") |> pull(upper) |> as.numeric())`], yielding a probability of `r sprintf("%1.3f", pp_diffs |> filter(contrast=="hour vs. week") |> pull(difference) |> (\(x) mean(x>0))())` that the hour model had superior performance relative to the week model.  The median increase in auROC for the day vs. the week model was `r sprintf("%1.2f", ci_diffs |> filter(contrast == "day vs. week") |> pull(median) |> as.numeric())` (95% CI=[`r sprintf("%1.2f", ci_diffs |> filter(contrast == "day vs. week") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci_diffs |> filter(contrast == "day vs. week") |> pull(upper) |> as.numeric())`], yielding a probability of `r sprintf("%1.3f", pp_diffs |> filter(contrast=="day vs. week") |> pull(difference) |> (\(x) mean(x>0))())` that the day model had superior performance relative to the week model.  We present histograms (and 95% CIs) of the posterior probability distributions for these model contrasts on auROC in Figure S5.

\newpage
<!--************************************************************************-->
<!-- Figure 1: ROC and Posterior probability histograms for auROC by model-->  
<!--ADD FIGURE CAPTION-->

```{r fig_1_roc-pp}
#| output: true
#| fig-cap: 
#|   - "Receiver Operating Characteristic Curves and Area Under Curves by Model. ADD CAPTION INFO"

roc_plot <- roc_all |>  
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"), 
                        labels = c("Week", "Day", "Hour"))) |>  
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path(linewidth = 1.25, show.legend = FALSE) +
  geom_abline(lty = 3) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Specificity",
       y = "Sensitivity") +
  scale_x_continuous(breaks = seq(0,1,.25),
                     labels = sprintf("%.2f", seq(1,0,-.25))) 

pp_plot <- pp_tidy |>  
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour"))) |> 
  ggplot() + 
  geom_histogram(aes(x = posterior, fill = model), color = "black", alpha = .4, 
                 bins = 30) +
  geom_segment(mapping = aes(y = y+100, yend = y-100, x = median, xend = median,
                           color = model), show.legend = FALSE, data = ci) +
  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, color = model),
              show.legend = FALSE, data = ci) +
  # geom_text(data = ci, x = c(.93, .907, .92), y = 1000, 
  #           label = str_c(round(ci$median, 2), " [", round(ci$lower, 2), ", ", round(ci$upper, 2), "]")) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous("Posterior Probability", breaks = c(0, 500, 1000)) +
  xlab("Area Under ROC Curve") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank())

roc_plot + pp_plot +
  plot_layout (guides = "collect") &
  theme(legend.position = "bottom")
```

\newpage

### Other Performance Metrics

To rigorously evaluate the performance of these three prediction models, we calculated additional performance metrics beyond the auROC in the 30 test sets.  Figure S6 displays histograms for the predicted probabilities of lapse for all observations in these test sets separately for each model.  We evaluated the sensitivity, specificity, balanced accuracy (i.e., mean of sensitivity and specificity), PPV, and NPV when these predicted lapse probabilities were used for binary classification (lapse vs. no lapse) with decision thresholds (i.e., the probability cut-point above which an observation is classified as a lapse) identified by Youden's Index <!--ref--> to balance the trade-off between sensitivity and specificity for each of the three models.  Table 2 presents these additional performance metrics by model.    

We also created Precision-Recall curves by aggregating predicted lapse probabilities across the 30 test sets to evaluate the trade-off between positive predictive value (i.e., precision) and sensitivity (i.e., recall) across decision thresholds. As seen in Figure 2, the PPV of any model can be increased by increasing the decision threshold for classifying an observation as a lapse; however, increasing the decision threshold will also lower the model's sensitivity. Decision thresholds can be selected from anywhere on the curve to maximize specific performance estimates (e.g., Youden's Index as displayed in Table 2) or to fit an intended application context. These Precision-Recall curves (Figure 2) depict the trade-off between positive predictive value and sensitivity across decision thresholds.   For example, the dotted lines in Figure 2 depict the sensitivities (`r sprintf("%1.2f", ppv_70 |> filter(model == "Week") |> pull(recall))`, `r sprintf("%1.2f", ppv_70 |> filter(model == "Day") |> pull(recall))`, and `r sprintf("%1.2f", ppv_70 |> filter(model == "Hour") |> pull(recall))` for week, day, and hour models, respectively) associated with decision thresholds that yield 0.70 positive predictive value for each of those models.  

\newpage
<!--************************************************************************-->
<!-- Table 2: Performance metrics-->

```{r table_2}
#| output: true


footnote_table_metrics <- "Insert footnote"

metrics |> 
 kbl(col.names = c("Metric", "Week", "Day", "Hour"),
      booktabs = TRUE,
      digits = 2,
      align = c("r"),
      caption = "Performance Metrics by Model",
     linesep = "") |>  
  row_spec(row = 0, align ="r") |>  
  kable_styling(position = "left") |>  
  footnote(general=footnote_table_metrics)
```

\newpage
<!--************************************************************************-->
<!-- Figure 2: PR curves by model-->
<!--UPDATE:  NEED TO FIX LINES.  THEY DONT LINE UP!-->
```{r fig_2_pr}
#| output: true
#| fig-cap: 
#| - "Precision-Recall Curves by Model.  ADD CAPTION TEXT"


pr_all |>  
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) |> 
  ggplot(aes(x = recall, y = precision, color = model)) +
  geom_path(linewidth = 1.25) +
  geom_segment(mapping = aes(y = .75, yend = .75, x = -.5, xend = recall,
                           color = model),
               linetype = "dashed",
               alpha = .8,
               show.legend = FALSE,
               data = ppv_70) +
  geom_segment(mapping = aes(y = -.5, yend = .75, x = recall, xend = recall,
                           color = model),
               linetype = "dashed",
               alpha = .8,
               show.legend = FALSE,
               data = ppv_70) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Sensitivity (Recall)",
       y = "Positive Predictive Value (Precision)")
```

\newpage


## Feature Importance

We display the global importance (mean |Shapley value|) for feature categories for each of the three models in Panel A of Figure 3.  These feature categories are ordered by their aggregate global importance (i.e., total bar length) across the three models.  The importance of each feature category for specific models is displayed separately by color.  Past use is a strong global predictor of future use consistently for all three models.  Self-reported future abstinence efficacy also emerges as a strong and consistent global predictor of future use.  The start time of the prediction window affects lapse predictions for next hour prediction but loses its value when the prediction windows encompass a full day or week.  Most of the more punctuate, time varying constructs (e.g., craving, stressful events, arousal) appear to have greater impact within the next hour prediction model. <!--JJC: not sure how much of this is starting to sounds like discussion vs. results--> <!-- GEF: i took out the final clause about what that might mean-->

We display local Shapley values that quantify the influence of feature categories on individual observations (i.e., a single prediction window for a specific participant) for each model in Panels B-D of Figure 3.  Critically, these Sina plots confirm that some feature categories (e.g., past pleasant events, future stressful events) impact lapse probability for specific individuals at specific times even if they are not globally important across all observations.  


<!--************************************************************************-->
<!-- Figure 3: SHAP Importance figure-->
<!--NEED TO UPDATE FIGURE CAPTION - JJC-->
<!--NEED TO ADD HEAT COLORS FOR DIRECTION TO LOCAL PLOTS - KENDRA?-->
<!--CONSIDER PANEL TITLES?-->
<!--CONSIDER WHERE LEGEND GOES-->
```{r make_global_shap}
panel_shap_global <- shap_global_all |>  
  mutate(variable_grp = reorder(variable_grp, mean_value, sum)) |>  
  ggplot() +
  geom_bar(aes(x = variable_grp, y = mean_value, fill = model), stat = "identity", alpha = .4) +
  ylab("Mean(|Shapley Value|)") +
  xlab("") +
  coord_flip()
```

```{r make_shap_local}
# get colors for models to match other figures that use faceting or grouping
colors_hex <- scales::hue_pal()(3) # week = 1, day = 2, hour = 3

# order features to match global plot
shap_levels <- shap_global_all |>
  mutate(variable_grp = reorder(variable_grp, mean_value, sum)) |>
  pull(variable_grp) |>
  levels()

# downsample to 10% of observations for each plot
downsample_ratio <- .10
ids_week <- shap_local_week |>
  pull(id_obs) |>
  unique()
ids_week <- ids_week |> sample(size = round(length(ids_week)/(1/downsample_ratio)))
ids_day <- shap_local_day |>
  pull(id_obs) |>
  unique()
ids_day <- ids_day |> sample(size = round(length(ids_day)/(1/downsample_ratio)))
ids_hour <- shap_local_hour |>
  pull(id_obs) |>
  unique()
ids_hour <- ids_hour |> sample(size = round(length(ids_hour)/(1/downsample_ratio)))

# week panel
panel_shap_local_week <- shap_local_week |>
  filter(id_obs %in% ids_week) |>
  mutate(variable_grp = factor(variable_grp, levels = shap_levels)) |>
  ggplot(mapping = aes(x = variable_grp, y = value)) +
  ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4,
                     color = colors_hex[1]) +
  geom_hline(yintercept = 0) +
  scale_y_continuous(limits = c(-2, 5), breaks = seq(-2, 5)) +
  ylab("Shapley Value") +
  xlab("") +
  coord_flip()

# day panel
panel_shap_local_day <- shap_local_day |>
  filter(id_obs %in% ids_day) |>
  mutate(variable_grp = factor(variable_grp, levels = shap_levels)) |>
  ggplot(mapping = aes(x = variable_grp, y = value)) +
  ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4,
                     color = colors_hex[2]) +
  geom_hline(yintercept = 0) +
  scale_y_continuous(limits = c(-2, 5), breaks = seq(-2, 5)) +
  ylab("Shapley Value") +
  xlab("") +
  coord_flip()

# hour panel
panel_shap_local_hour <- shap_local_hour |>
  filter(id_obs %in% ids_hour) |>
  mutate(variable_grp = factor(variable_grp, levels = shap_levels)) |>
  ggplot(mapping = aes(x = variable_grp, y = value)) +
  ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4,
                     color = colors_hex[3]) +
  geom_hline(yintercept = 0) +
  scale_y_continuous(limits = c(-2, 5), breaks = seq(-2, 5)) +
  ylab("Shapley Value") +
  xlab("") +
  coord_flip()
```


```{r fig_3_shap}
#| output: true
#| fig-cap: 
#| - "Variable Importance (Shapley Values) by Model.  Raw EMA features are grouped by the original item from the EMA. Features from demographics and the day and hour for the start of the  prediction window are also included."

panel_shap_global + 
  panel_shap_local_week + 
  panel_shap_local_day + 
  panel_shap_local_hour +
  plot_layout(ncol = 2)


# panel_shap_global + panel_shap_local_week + panel_shap_local_day + panel_shap_local_hour +
#    plot_layout (ncol = 2, guides = "collect") &
#    theme(legend.position = "bottom")
```



\newpage

# Discussion

<!--Word count: -->

## Model Performance

Models across all three window widths performed exceptionally well, yielding auROCs of .90, .91, and .94 for week, day, and hour models, respectively. auROCs summarize the model's sensitivity and specificity over all possible decision thresholds (i.e., the probability cut-points at which an observation is classified as positive). auROCs above .9 are generally described as having "excellent" performance, meaning that the model will correctly classify a positive case as positive and negative case as negative 90% of the time [@mandrekarReceiverOperatingCharacteristic2010]. All three of our models performed as well as or better than existing alcohol lapse prediction models to date [@chihPredictiveModelingAddiction2014<!--KW: Sarah, is this the only true "alcohol lapse" prediction paper to cite? You had Berenholtz et al., 2020 listed but can't figure out what article you are referring to? Also noting this statement would not be true if we include alcohol use prediction (i.e., Bae et al)-->]. This indicates EMA data can be used to predict alcohol lapse in the next week, next day, and next hour with high levels of sensitivity and specificity. 

An additional, critical component of model performance is PPV, which describes the proportion of a model's predicted positive cases relative to the true amount positive cases in the data at a given decision threshold.  We used Youden's index to determine an optimal decision threshold to maximize sensitivity and specificity. At this threshold, PPV across our models varied considerably with values of .60, .27, and .02 for week, day, and hour models, respectively. PPV, unlike auROC, is highly influenced by unbalanced outcomes (i.e., low numbers of positive cases). Therefore as the prediction window width (and number of positive cases) increased, PPV naturally improved. PPV can also be improved by increasing the decision threshold -- as the threshold increases, the model needs to be "more confident" that a positive prediction represents a true positive, increasing PPV but decreasing sensitivity (see the Precision-Recall curves in Figure 2 for a depiction of this trade off). 

High performance of prediction models is only beneficial if the models are generalizable. Our study was the first to employ grouped, nested, k-fold cross-validation for generating the reported model performances. This design provides an unbiased estimate of model performance by separating hyperparameter tuning and model selection processes from model performance estimation. Furthermore, it groups an individual's data so that it is only used in model selection or evaluation for a given fold. Thus, our methods provide the most rigorous test of any EMA lapse prediction model to date.

We built upon additional gaps in existing EMA lapse<!--KW: these gaps are related to alcohol use prediction not necessarily alcohol lapses. Should we clarify this somehow? Especially since the 2nd gap we point to is we are predicting lapses.--> prediction literature to further increase the generalizability of our models. First, our models were trained on a large, treatment-seeking sample of adults in early recovery from AUD that more closely matches the individuals most likely to benefit from a risk prediction model (vs non-treatment-seeking samples). Second, we specifically predicted episodes of goal-inconsistent alcohol use (i.e., *lapses*) because factors predicting goal-inconsistent use likely differ substantially from factors that predict other types of alcohol use (e.g., episodes of binge drinking among college students, intentional instances of drinking among people pursuing harm reduction goals). This relatively homogeneous prediction outcome likely increased the predictive ability of our model. Finally, we collected data from participants with high frequency (4x per day) over a clinically meaningful period of time (up to three months). This sampling density allowed us not only to capture changes in dynamic risk factors with high precision but also to build models whose prediction windows updated frequently. Altogether, these specifications help to maximize both the predictive ability and eventual clinical utility of our model.

## Understanding & Contextualizing Model Performance

Remaining analyses explored the feature categories driving our model performance. Some feature categories consistently emerged as being globally important (i.e., mean absolute SHAP value across all observations) across our week, day, and hour level models. Unsurprisingly, the largest contribution to prediction of a lapse was frequency of previously reported lapses. An individual who reported lapsing frequently was more likely to lapse at any given observation in the future. This is consistent with decades of research on relapse precipitants and our understanding of human behavior more generally (i.e., past behavior predicts future behavior) [@marlattRelapsePreventionMaintenance1985; @hogstrombrandtPredictionSingleEpisodes1999]. 

Additionally, congruent with relapse prevention literature, we also saw an increased likelihood of lapse when participants reported lower ratings of confidence in their ability to maintain their goal of abstinence (i.e., lower self-efficacy), higher levels of craving, increased magnitude of stressful events experienced in the past 24 hours, and increased exposure to situations described as "risky for [your] recovery." 

Some differences emerged in the global importance of time-varying feature categories across models due to varying temporal precision of our prediction window widths. For example, our hour level model ranked time of day and day of week as important features contributing to lapse. However, in our day-level models only day of week emerged as being important, and week-level models were unable to capture enough variance in either feature. This improved precision was also reflected in our bayesian model comparisons that showed prediction of lapse in the next hour yielded better performance than prediction of lapse in the next day or week.

Importantly, non-aggregated SHAP values for individual observations (i.e., local importance) shed light on the multidimensional and heterogeneous nature of lapse and relapse in our sample. Sina plots in Figure 3 displayed much variance in what feature categories were locally important for a specific observation. For example, some feature categories with low global importance (e.g., past pleasant events, future stressful events) still impacted lapse probability for specific individuals at specific times.

The demographic variables included in our models did not yield high global or local importance. Despite our data having wide representation with respect to socioeconomic status, gender, and age, these features did not significantly contribute to the lapse prediction. While this does not rule out these features' predictive utility, it does suggest that other EMA feature categories (e.g, past use, self-efficacy, craving) may be more relevant for lapse prediction than these characteristics. Race and ethnicity also did not emerge as globally or locally important features. However, the limited representation of Black and Brown participants in our sample warrants caution in drawing conclusions about the predictive utility of race and ethnicity. Data collection is underway for a related project in our laboratory to build a lapse risk prediction model for individuals with opioid use disorder. Participants are being recruited nationally with the explicit goal of improving geographic, racial, and ethnic diversity to match national population data.

## Considerations for Clinical Implementation

The goal of the current project was to build models for lapse risk prediction using EMA data. Although this is a necessary first step, the ultimate goal of this line of work is to use this model clinically. We believe these models may be most effective when embedded in a digital therapeutic context for reasons of access, availability, and affordability described previously. Below we describe potential considerations for clinical incorporation of our model predictions.

The primary output of our model is the likelihood of lapsing for an individual within the next week, day, or hour. This probability could in turn be used to recommend actions that an individual could take to reduce their risk of a lapse. In this situation, a model like ours could communicate an actionable treatment recommendation to the individual based on their lapse risk and the top features contributing locally to that risk (e.g., recommending an urge surfing activity in response to reported high cravings). Importantly, many of our features with high local and/or global importance align well with the risk factors and associated intervention strategies delineated in Marlatt's Relapse Prevention model. Thus, recommendations could be mapped onto existing therapeutic frameworks shown to be effective for AUD (e.g., CBT, mindfulness-based relapse). Thus, prediction models can optimize digital therapeutics such that these personalized treatment goals are realized while capitalizing on the benefits of existing self-guided digital therapeutics (e.g., reaching people not connected with a treatment provider, around-the-clock availability). 

Prediction models can also be tailored to an individual user or their context with respect to decision thresholds. Some contexts support raising the decision threshold to improve PPV while reducing sensitivity (i.e., increasing confidence in lapse predictions while the chance of a "missed" lapse)<!--This means that we may miss some lapses, but the lapses we do predict are more likely to be true lapses.-->. For example, an individual using the app may wish to know that if they receive an alert, they can trust it. Conversely, some contexts support lowering the decision threshold to avoid missing true lapses. Modules recommended in a digital therapeutic platform in response to a lapse risk alert are likely to benefit an individual whether they are truly at risk or not. For example, completing a mindfulness meditation activity in response to a risk alert is likely to benefit the individual whether this was a true prediction or a "false positive." There is a high personal, health, and economic cost when an individual returns to harmful substance use; thus, avoiding misses may be safest in some contexts. Overall, a recommendation-guided digital therapeutic could adjust the decision threshold based on overall costs of a recommendation (e.g., sending a reminder to check in on their recovery goals vs. reaching out to a supportive friend or family member), the availability of resources, and the user's own preference.

## Future directions

A primary future direction for this research is the eventual clinical implementation of a lapse risk prediction model into a digital therapeutic platform. Future research is needed to determine the optimal way to integrate output from these types of algorithms into treatment. Although our project served to increase substantially the generalizability of existing EMA-guided lapse prediction, several limitations continue to hinder generalizability including low representation of Black and Brown individuals and unknown assessment of engagement after three months. To address these limitations, data collection efforts are currently underway in our laboratory for a nationally-recruited sample of individuals with opioid use disorder. These individuals will participate for a full year. We will compare completion and attrition rates across these projects to explore how burden may change as duration increases (though with the confound of different substance-using populations). 

<!-- GEF: i would vote to put the abstinence paragraph here -->

<!-- GEF: i would vote to put the shortened version of the final paragraph about assessment version here, and then adjust the openings. these paragraphs are all about reducing assessment burden - they're not necessarily general future directions. i think they feel uncontextualized as is. -->

<!-- Several options exist to explore reducing assessment burden. First-->Second, the current study uses all four daily EMA surveys for risk prediction. All four EMAs contain the same 7 questions, but the morning EMA includes 3 additional questions asked only once daily. A future project could examine how well models perform using only the morning EMA survey, providing an estimate of how well we might predict using lower-burden data collection of only 1X daily EMA. 

<!-- Second-->Third, although the current study uses only active EMA and demographic characteristics as features, the broader parent project collected many other passive signals such as GPS location, cellular communications, and cellular metadata. Future projects will determine the predictive utility of these passive signals, both on their own and perhaps in conjunction with reduced actively sensed features, to understand whether burden could be lowered in these ways. 

<!-- Third-->Fourth, although it cannot be addressed with the current data, future projects could explore adaptations for long-term data collection. The sampling density of actively sensed signals could be reduced as individuals enter sustained recovery/remission. Active sensing could be adapted on an individual basis to focus on features that emerge as primary global features or frequently locally important for that individual, reducing the number of items assessed regularly. These and other opportunities for adaptation may improve long-term engagement, but future research will be needed to test these ideas and explore any impact on prediction accuracy or other outcomes. 

<!-- GEF: i would then end with the lag time paragraph -->


<!-- SS: What if we cut these paragraphs and instead discussed these limitations in the methods? One or two sentences each. We kind of already do this with abstinence and burden in our methods justification-->

<!-- GEF: personally i feel these are important topics to cover in the discussion, and i think they might feel out of place if we add more detail about these choices to the methods. plus, some are much more about future directions. i agree they can and should be shortened, but i don't think they should be cut. i'm providing shortened versions below each paragraph just in case y'all decide to keep them-->

<!--Although we varied the duration of the outcome windows (i.e., one hour, one day, one week), each model currently has a lag time of 0. This means that a predicted lapse might occur anytime from the moment the prediction is updated through the end of the prediction window. This method takes advantage of the most and most up-to-date information with regard to risk factors - all data collected until that point can be used, and the most recent data occur immediately prior to the onset of the prediction window. This is likely quite practical for our hour model when we consider just-in-time interventions - we want to intervene immediately in response to changes in lapse risk driven by very recent changes in proximal risk factors. However, a lag time of 0 is less well-suited to a longer outcome window - in particular, our week model. A high likelihood of a lapse predicted by our week model means that the lapse could occur in the next hour or 6 days later. This model is not saying that an individual is likely to lapse in a week (i.e., a week from now); rather, it says an individual is likely to lapse anytime in the next week beginning right now. No lag creates potential problems for recommendations that take longer to implement. Suggesting someone make an appointment with their therapist would not be helpful if someone was going to lapse in the next hour; however, it could be helpful if they were at risk of lapsing a week into the future. Therefore, it is important to consider how lag time and prediction windows interact when making intervention recommendations. Narrow windows (e.g., next hour) with no lag could be particularly useful for making immediate intervention recommendations (e.g., urge surfing, calling a supportive contact, stimulus control techniques). Lagged broad windows (e.g., next week starting one week from now) could also be especially useful for incorporating interventions that may take longer. We plan to explore models with not only different outcome windows but also different lag times. We will investigate how these models differ with respect to performance and what this might suggest for potential clinical utility.

GEF SHORTENED VERSION OF ABOVE PARAGRAPH: (i would vote for this to be the final paragraph as i denote above)
A final future direction concerns model lag time. Each model currently has a lag time of 0, which means a predicted lapse might occur anytime in the prediction window. This method uses all data collected until that point including data collected immediately prior to the prediction window onset. This lag time is practical for the one hour model when we consider just-in-time interventions, which intervene immediately in response to very recent changes in proximal risk factors. However, a lag time of 0 is less well-suited to longer prediction windows like the one week model. This model indicates an individual can lapse anytime in the next week beginning right now; it does not indicate an individual will lapse a week from now. Recommendations that take time to implement, such as making an appointment with a therapist, would not be helpful for next-hour risk; however, it could be helpful if they were at risk of lapsing a week into the future. We plan to explore models how models with different combinations of prediction windows and lag times differ with respect to performance and potential clinical utility.

Our prediction algorithm was trained using participants with a goal of abstinence. We view this as both a strength and a limitation. As described above, a homogeneous outcome of goal-inconsistent use likely improved model performance. This outcome also set our study apart from previous work that has examined other types of drinking outcomes (e.g., binge drinking among college students) that is less closely connected with the type of clinical sample who might eventually use our models embedded within a digital therapeutic. Additionally, many individuals in recovery from alcohol and other substance use disorders do pursue abstinence goals, and this approach is still strongly recommended by some treatments (e.g., Alcoholics Anonymous). However, some individuals prefer to pursue other recovery goals more in line with moderation or harm reduction. It is likely that the factors that predict alcohol use among individuals pursuing abstinence as in the current study differ from the factors that predict alcohol use among individuals pursuing moderation goals. Moreover, within that population, alcohol use instances would need to be differentiated as goal-consistent use (e.g., having one drink) and goal-inconsistent use (e.g., having two drinks when your goal is one; having one drink for the second time that week when your goal is to drink once per week). It is likely that this model-building approach can be adapted for other recovery goals (e.g., moderation); however, it needs to be first tested, and it may require a completely new sample for model development and evaluation.

GEF SHORTENED VERSION OF ABOVE PARAGRAPH: (i would vote for this to go before the assessment burden paragraphs [after the opening paragraph of the section] as i denote above)
Our prediction models were trained using a clinical sample with a goal of abstinence. This sample set our study apart from previous work that has examined other drinking outcomes among non-clinical samples (e.g., binge drinking among college students). Many individuals in recovery do pursue abstinence goals; however, others prefer moderation or harm reduction goals. The factors that predict lapses among individuals pursuing abstinence may differ from the factors that predict alcohol use among individuals pursuing moderation goals. It is likely that this model-building approach can be adapted for other recovery goals (e.g., moderation); however, alcohol use would need to be differentiated as goal-consistent use (e.g., having one drink) and goal-inconsistent use (e.g., exceeding intended frequency or amount of use).

A final consideration is the assessment burden required of individuals to provide the data used to build and maintain lapse risk prediction models. In the current study, we used 4X daily EMA surveys, where each survey ranged in length from 7 to 10 items. These EMA surveys are considered "active" sensing, in that they require action on the part of the individual to provide data. In a previous paper from our laboratory using these data, we examined the burden of providing data for this project [@wyantAcceptabilityPersonalSensing2022]. Overall, individuals found the burden to be acceptable and reported being (hypothetically) willing to continue providing data for a full year. Despite these promising results, uncertainty remains as to how long this response rate could be maintained. AUD is a chronic, relapsing disorder that requires lifelong monitoring. Currently, this burden currently falls on the individual or (intermittently) their treatment provider. A risk prediction model embedded in a digital therapeutic could shoulder this burden, but only if an individual could provide data long-term - and potentially indefinitely. 

GEF SHORTENED VERSION OF ABOVE PARAGRAPH (i think this should be moved to before the "second"/"third"/"fourth" paragraphs that deal with how to address issues of assessment burden)
In the current study, we built features from EMA surveys, which are considered "active" sensing because they require action on the part of the individual. In a previous paper from our laboratory using these data, we examined the burden of providing data for this project [@wyantAcceptabilityPersonalSensing2022]. Overall, individuals found the burden to be acceptable and reported being (hypothetically) willing to continue providing data for a full year. Despite these promising results, uncertainty remains as to how long this response rate could be maintained. The chronic, relapsing nature of AUD requires lifelong monitoring. A risk prediction model embedded in a digital therapeutic could shoulder this burden, but only if an individual could provide data long-term - and potentially indefinitely. 

-->


  
    
\newpage

# References