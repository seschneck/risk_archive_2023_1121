---
title: "Machine learning models for temporally precise lapse prediction in alcohol use disorder"
blank-lines-above-title: 2
shorttitle: Machine learning for lapse prediction
author:
  - name: Kendra Wyant*
    # role:
    #   - Conceptualization
    #   - Data curation
    #   - Formal analysis
    #   - Writing – original draft
    #   - Writing – review & editing
    affiliations:
      - id: id1
        name: University of Wisconsin-Madison
        department: Department of Psychology
        address: 1202 W Johnson St
        city: Madison
        region: WI
        postal-code: "53521"
  - name: Sarah J. Sant’Ana*
    # role:
    #   - Conceptualization
    #   - Data curation
    #   - Investigation
    #   - Writing – original draft
    #   - Writing – review & editing
    affiliations: 
      - ref: id1
  - name: Gaylen E. Fronk
    # role:
    #   - Conceptualization
    #   - Writing – original draft
    #   - Writing – review & editing
    affiliations: 
      - ref: id1
  - name: John J. Curtin
    corresponding: true
    email: jjcurtin@wisc.edu
    url: https://arc.psych.wisc.edu/
    # role:
    #   - Conceptualization
    #   - Data curation
    #   - Formal analysis
    #   - Funding acquisition
    #   - Investigation
    #   - Supervision
    #   - Writing – original draft
    #   - Writing – review & editing
    affiliations: 
      - ref: id1
author-note:
  blank-lines-above-author-note: 1
  status-changes: 
    affiliation-change: "*These authors contributed equally as co-first authors."
  # Disclosures condensed to one paragraph, but you can start a field with two line breaks to break them up: \n\nNew Paragraph
  disclosures:
    study-registration: ~
    data-sharing: "All data and materials have been made publicly available and can be accessed at [https://osf.io/w5h9y/](https://osf.io/w5h9y/)."   
    related-report: "All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study # 2015-0780)."
    financial-support: "This research was supported by grants from the NIAAA (R01 AA024391; JJC) and the NIDA (R01 DA047315; JJC)."
    gratitude: "The authors wish to thank Susan E. Wanta for her role as the project administrator and for her help with data curation. The authors also wish to thank Candace Lightheart, Jill Nagler, Kerry Keiser, and Megan Shultz for their contributions to data collection and Chris Gioia for the clinical supervision he provided to graduate students."
    authorship-agreements: ~
abstract: "We developed three separate models that provide hour-by-hour probabilities of a future lapse back to alcohol use with increasing temporal precision (i.e., lapses in the next week, next day, and next hour). Model features were based on raw scores and longitudinal change in theoretically implicated risk factors collected through ecological momentary assessment (EMA). Participants (*N*=151; 51% male; mean age = 41; 87% White, 97% Non-Hispanic) in early recovery (1–8 weeks of abstinence) from alcohol use disorder provided 4x daily EMA for up to three months. We used grouped, nested cross-validation, with 1 repeat of 10-fold cross-validation for the inner loop and 3 repeats of 10-fold cross-validation for the outer loop to train models, select best models, and evaluate those best models on auROC. Models yielded median areas under the receiver operating curves (auROCs) of .90, .91, and .94 in the 30 held-out test sets for week, day, and hour level models, respectively. Some feature categories consistently emerged as being globally important to lapse prediction across our week, day, and hour level models (i.e., past use, future efficacy). However, most of the more punctuate, time varying constructs (e.g., craving, past stressful events, arousal) appear to have greater impact within the next hour prediction model. This research represents an important step toward the development of a *smart* (machine learning guided) sensing system that can both identify periods of peak lapse risk and recommend specific supports to address factors contributing to this risk. \n\nGeneral scientific summary: This study suggests that densely sampled self-report data can be used to predict lapses back to alcohol use with varying degrees of temporal precision. Additionally, the contextual features contributing to risk of lapse may offer important insight for treatment matching through a digital therapeutic."
keywords: [ecological momentary assessment, digital therapeutics, alcohol use disorder]
bibliography: paper_ema.bib
format:
  apaquarto-pdf:
    documentmode: man
    fontsize: 12pt
    floatsintext: false
---


{{< include _extensions/wjschne/apaquarto/_apa_title.qmd >}}


<!-- total words: 9,515 words (will save about an extra 120 words by removing extra initials in references)
- Journal metadata (title page, author notes, disclosures, abstract, general scientific summary) = 468
- intro = 1445
- Method = 1929 
- Results (text only) = 907 
- Discussion = 2250
- References= 1735 
- Tables = 493 
- Fig captions = 260
-->


<!--Outstanding issues:
- get word count to 9000
- JC to add statement about power in methods
- JC to check all figure captions and table footnotes
- JC to check abstract/General scientific statement
- KW to update supplement with methods and add reference to it in methods
-->



<!--required author notes (already entered into metadata):

All data and materials have been made publicly available on Open Science Framework and can be accessed at [https://osf.io/w5h9y/](https://osf.io/w5h9y/).

All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study # 2015-0780).
-->



<!--Target Journal
Journal of Psychopathology and Clinical Science
https://www.apa.org/pubs/journals/abn 

ARTICLES
The manuscript should not exceed 9,000 words when including the abstract, body of the text, tables, table captions, figure captions, footnotes, author notes, appendices, and references in a word count.
-->

<!--terminology
Prediction window, window width, week, day, or hour model (always in that order)
relapse (a goal-inconsistent return to harmful substance use) and lapse (a single instance of goal-inconsistent substance use) 
-->

<!--Notes from lab meeting

Intro:
- language around AUD/SUD/psychiatric disorders - DONE by JJC
- reduced focus on digital therapeutics/increased focus on personal sensing & prediction algorithm/changed headings - PARTIALLY DONE BY JJC
- add information about specific features in “current study” and how they map onto relapse prevention literature -TBD; JC MARKED LOCATION FOR A FEW SENTENCES.

Methods:
- improve terminology and definitions of things like test set, validation set, etc - so that we can just use simpler language throughout

Results:
- shorten?/simplify?
- remove “methods” sentences from results
- determine how to “answer question” (how do we know if our AUC is good, did we answer the question about “can we predict lapses”/“how accurately can we predict lapses”)

Discussion:
-add to limitations/future directions: no existing method to say whether an AUC is clinically useful or whether a SHAP value (global value or local variability) is meaningful
-->

```{r knitr_settings}
options(knitr.kable.NA = '')
knitr::opts_chunk$set(fig.pos = "ht", out.extra = "", message = FALSE)
```


```{r setup}
#| warning: false

library(knitr)
# library(yardstick) # for roc_curve
library(kableExtra)
library(janitor)
# library(corx)
library(patchwork)
library(ggtext)
library(consort)
library(tidyverse)
library(tidymodels)
library(tidyposterior)
library(cowplot)

theme_set(theme_classic()) 
```


```{r paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- "P:/studydata/risk/chtc/ema"
          path_processed <- "P:/studydata/risk/data_processed/ema"
          path_models <- "P:/studydata/risk/models/ema"
          path_shared <- "P:/studydata/risk/data_processed/shared"
          path_manuscript <- "P:/studydata/risk/manuscripts/EMA"},

        # IOS paths
        Darwin = {
          path_input <- "/Volumes/private/studydata/risk/chtc/ema"
          path_processed <- "/Volumes/private/studydata/risk/data_processed/ema"
          path_models <- "/Volumes/private/studydata/risk/models/ema"
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"
          path_manuscript <- "/Volumes/private/studydata/risk/manuscripts/EMA"},
        
        # Linux paths
        Linux = {
          path_input <- "~/mnt/private/studydata/risk/chtc/ema"
          path_processed <- "~/mnt/private/studydata/risk/data_processed/ema"
          path_models <- "~/mnt/private/studydata/risk/models/ema"
          path_shared <- "~/mnt/private/studydata/risk/data_processed/shared"
          path_manuscript <- "~/mnt/private/studydata/risk/manuscripts/EMA"}
        )
```


```{r load_data}
# For table 1
disposition <- read_csv(file.path(path_processed, "disposition.csv"), 
                        col_types = "ccDDcccccccccc")
# For table 1
screen <- read_csv(file.path(path_shared, "screen.csv"), 
                   col_types = cols()) |>
  filter(subid %in% subset(disposition, analysis == "yes")$subid)

lapses <- read_csv(file.path(path_shared, "lapses.csv"), col_types = cols()) |>
  filter(exclude == FALSE)

# lapse labels
labels_week <- read_csv(file.path(path_processed, "labels_1week.csv"), col_types = cols())
labels_day <- read_csv(file.path(path_processed, "labels_1day.csv"), col_types = cols())
labels_hour <- read_csv(file.path(path_processed, "labels_1hour.csv"), col_types = cols())


# Predictions data
preds_week<- read_rds(file.path(path_models, "outer_preds_1week_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)
preds_day<- read_rds(file.path(path_models, "outer_preds_1day_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)
preds_hour<- read_rds(file.path(path_models, "outer_preds_1hour_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)

auc_week <- read_rds(file.path(path_models, "outer_metrics_1week_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))
auc_day <- read_rds(file.path(path_models, "outer_metrics_1day_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))
auc_hour <- read_rds(file.path(path_models, "outer_metrics_1hour_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))

# ROC curves
roc_week <- preds_week |>  
  roc_curve(prob, truth = label) |>  
  mutate(model = "1week")

roc_day <- preds_day |>  
  roc_curve(prob, truth = label) |>  
  mutate(model = "1day")

roc_hour <- preds_hour|>  
  roc_curve(prob, truth = label) |>  
  mutate(model = "1hour")

roc_all <- roc_week |>  
  bind_rows(roc_day) |>  
  bind_rows(roc_hour)

# PR curves
pr_week <- preds_week |>  
  pr_curve(prob, truth = label) |>  
  mutate(model = "1week")

pr_day <- preds_day |>  
  pr_curve(prob, truth = label) |>  
  mutate(model = "1day")

pr_hour <- preds_hour|>  
  pr_curve(prob, truth = label) |>  
  mutate(model = "1hour")

pr_all <- pr_week |>  
  bind_rows(pr_day) |>  
  bind_rows(pr_hour)


# posterior probabilities
pp <- read_rds(file.path(path_models, "posteriors_all_0_v5_nested.rds"))

pp_tidy <- pp |>  
  tidy(seed = 123)

q = c(.025, .5, .975)
ci <- pp_tidy |>  
  group_by(model) |>  
  summarize(median = quantile(posterior, probs = q[2]),
            lower = quantile(posterior, probs = q[1]), 
            upper = quantile(posterior, probs = q[3])) |>  
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour")),
         y = 1000) |> 
  arrange(model)

pp_diffs <- pp |>  
  contrast_models(list("hour","hour", "day"), 
                list("week", "day", "week"))

ci_diffs <- pp_diffs |> 
  group_by(contrast) |>  
  summarize(median = quantile(difference, probs = q[2]),
            lower = quantile(difference, probs = q[1]), 
            upper = quantile(difference, probs = q[3]))


# SHAPS
shap_local_week <- read_rds(file.path(path_models, "outer_shapsgrp_1week_0_v5_nested.rds")) 
shap_local_day <- read_rds(file.path(path_models, "outer_shapsgrp_1day_0_v5_nested.rds"))
shap_local_hour <- read_rds(file.path(path_models, "outer_shapsgrp_1hour_0_v5_nested.rds")) 

shap_global_week <- shap_local_week |>  
  group_by(variable_grp) |>  
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>  
  arrange(mean_value) |> 
  mutate(model = "Week")
shap_global_day <- shap_local_day |>  
  group_by(variable_grp) |>  
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>  
  arrange(mean_value) |> 
  mutate(model = "Day")
shap_global_hour <- shap_local_hour |>  
  group_by(variable_grp) |>  
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>  
  arrange(mean_value) |> 
  mutate(model = "Hour")
shap_global_all <- shap_global_week |> 
  bind_rows(shap_global_day) |> 
  bind_rows(shap_global_hour) |> 
  mutate(model = factor(model, levels = c("Week", "Day", "Hour"))) |>  
  mutate(variable_grp = factor(variable_grp, levels = c("past use (EMA item)", 
                                          "craving (EMA item)", 
                                          "past risky situation (EMA item)", 
                                          "past stressful event (EMA item)", 
                                          "past pleasant event (EMA item)", 
                                          "valence (EMA item)", 
                                          "arousal (EMA item)", 
                                          "future risky situation (EMA item)", 
                                          "future stressful event (EMA item)", 
                                          "future efficacy (EMA item)",
                                          "lapse day (other)",
                                          "lapse hour (other)",
                                          "missing surveys (other)",
                                          "age (demographic)",
                                          "sex (demographic)",
                                          "race (demographic)",
                                          "marital (demographic)",
                                          "education (demographic)")))
```

```{r table_1_calcs}

# Calcs to make df for table 1 (demographics and clinical characteristics)
n_total <- 151

dem <- screen |>  
  summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
            SD = as.character(round(sd(dem_1, na.rm = TRUE), 1)),
            min = as.character(min(dem_1, na.rm = TRUE)),
            max = as.character(max(dem_1, na.rm = TRUE))) |>  
  mutate(var = "Age",
         n = as.numeric(""),
         perc = as.numeric("")) |>  
  select(var, n, perc, everything()) |>  
  full_join(screen |>  
  select(var = dem_2) |>  
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_3) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("American Indian/Alaska Native", "Asian", "Black/African American",
                           "White/Caucasian", "Other/Multiracial")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_4) |>  
  mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
                         TRUE ~ "Yes"),
         var = fct_relevel(factor(var, c("Yes", "No")))) |>  
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_5) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("Less than high school or GED degree", "High school or GED", 
                           "Some college", "2-Year degree", "College degree", "Advanced degree")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_6, dem_6_1) |>  
  mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
                         dem_6_1 == "Part-time" ~ "Employed part-time",
                         TRUE ~ var)) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("Employed full-time", "Employed part-time", "Full-time student",
                           "Homemaker", "Disabled", "Retired", "Unemployed", 
                           "Temporarily laid off, sick leave, or maternity leave",
                           "Other, not otherwise specified")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  summarise(mean = format(round(mean(dem_7, na.rm = TRUE), 0), big.mark = ","),
            SD = format(round(sd(dem_7, na.rm = TRUE), 0), big.mark = ","),
            min =format(round(min(dem_7, na.rm = TRUE), 0), big.mark = ","),
            max = format(round(max(dem_7, na.rm = TRUE), 0), scientific = FALSE, big.mark = ",")) |>  
  mutate(var = "Personal Income",
        n = as.numeric(""),
        perc = as.numeric(""),
        mean = str_c("$", as.character(mean)),
        SD = str_c("$", as.character(SD)),
        min = str_c("$", as.character(min)),
        max = as.character(max)) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", "min", "max")) |>  
  full_join(screen |>  
  select(var = dem_8) |>  
  mutate(var = case_when(var == "Never Married" ~ "Never married",
                         TRUE ~ var)) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("Never married", "Married", "Divorced", "Separated",
                           "Widowed")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))

auh <- screen |>  
  summarise(mean = mean(auh_1, na.rm = TRUE),
            SD = sd(auh_1, na.rm = TRUE),
            min = min(auh_1, na.rm = TRUE),
            max = max(auh_1, na.rm = TRUE)) |>  
  mutate(var = "Age of first drink",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()) |>  
  full_join(screen |>  
  summarise(mean = mean(auh_2, na.rm = TRUE),
            SD = sd(auh_2, na.rm = TRUE),
            min = min(auh_2, na.rm = TRUE),
            max = max(auh_2, na.rm = TRUE)) |>  
  mutate(var = "Age of regular drinking",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", 
                                             "min", "max")) |>  
  full_join(screen |>  
  summarise(mean = mean(auh_3, na.rm = TRUE),
            SD = sd(auh_3, na.rm = TRUE),
            min = min(auh_3, na.rm = TRUE),
            max = max(auh_3, na.rm = TRUE)) |>  
  mutate(var = "Age at which drinking became problematic",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  summarise(mean = mean(auh_4, na.rm = TRUE),
            SD = sd(auh_4, na.rm = TRUE),
            min = min(auh_4, na.rm = TRUE),
            max = max(auh_4, na.rm = TRUE)) |>  
  mutate(var = "Age of first quit attempt",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  # filter out 2 people with 100 and 365 reported quit attempts - will make footnote in table
  filter(auh_5 < 100) |>  
  summarise(mean = mean(auh_5, na.rm = TRUE),
            SD = sd(auh_5, na.rm = TRUE),
            min = min(auh_5, na.rm = TRUE),
            max = max(auh_5, na.rm = TRUE)) |>  
  mutate(var = "Number of Quit Attempts*",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  select(var = auh_6_1) |> 
  mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ months)",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_2) |> 
  mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 months)",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_3) |> 
  mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_4) |> 
  mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_5) |> 
  mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_6) |> 
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_7) |> 
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_7) |>  
  mutate(var = fct_relevel(factor(var, c("Yes", "No")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) |>  
  rowwise() |>  
  # calculate dsm5 score by adding up dsm5_1 through dsm5_11
  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7, 
                            dsm5_8, dsm5_9, dsm5_10, dsm5_11))) |>  
  ungroup() |>  
  summarise(mean = mean(dsm5_total),
            SD = sd(dsm5_total),
            min = min(dsm5_total, na.rm = TRUE),
            max = max(dsm5_total, na.rm = TRUE)) |>  
  mutate(var = "DSM-5 Alcohol Use Disorder Symptom Count",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  select(var = assist_2_1) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_2) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Cannabis (marijuana, pot, grass, hash, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_3) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Cocaine (coke, crack, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_4) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_5) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Inhalants (nitrous, glue, petrol, paint thinner, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_6) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_7) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_8) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Opioids (heroin, morphine, methadone, codeine, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) 

lapses_per_subid <- screen |>  
  select(subid) |>  
  left_join(lapses |>  
  tabyl(subid) |>  
  select(-percent), by = "subid") |>  
  mutate(n = if_else(is.na(n), 0, n),
         lapse = if_else(n > 0, "yes", "no")) 

lapse_info <- lapses_per_subid |>  
  group_by(lapse) |>  
  rename(var = lapse) |>  
  mutate(var = factor(var, levels = c("yes", "no"), labels = c("Yes", "No"))) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100,
         mean = NA_real_,
         SD = NA_real_,
         min = NA_real_,
         max = NA_real_) |>  
  full_join(lapses_per_subid |>  
  summarise(mean = mean(n),
            SD = sd(n),
            min = min(n),
            max = max(n)) |>  
  mutate(var = "Number of reported lapses"), 
  by = c("var", "mean", "SD", "min", "max"))
```

```{r table_2_calcs}

# Calcs to create df for table 2 (performance metrics at Youden's Index)
j_thres_week <- roc_week |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)

j_thres_day <- roc_day |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)

j_thres_hour <- roc_hour |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)


metrics_week <- preds_week |> 
  mutate(estimate = if_else(prob > j_thres_week, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(week = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics_day <- preds_day |> 
  mutate(estimate = if_else(prob > j_thres_day, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(day = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics_hour <- preds_hour |> 
  mutate(estimate = if_else(prob > j_thres_hour, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(hour = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics <- metrics_week |>  
  full_join(metrics_day, by = "metric") |>  
  full_join(metrics_hour, by = "metric") |>  
  filter(metric %in% c("bal_accuracy", "sens", "spec", "ppv", "npv"))

auc <- tibble(metric = "auROC", 
              week = preds_week |>  roc_auc(prob, truth = label) |>   
                pull(.estimate) |>  round(3), 
              day = preds_day |>  roc_auc(prob, truth = label) |>   
                pull(.estimate) |>  round(3),
              hour = preds_hour |>  roc_auc(prob, truth = label) |>   
                pull(.estimate) |>  round(3))

metrics <- metrics |>  
  bind_rows(auc)

metrics <- metrics[c(6,1,2,5,3,4),]
```



```{r ppv_70_values}
# PPV and sens values at ppv = .70
ppv_70 <- pr_all |>  
  mutate(recall = round(recall, 3),
         precision = round(precision, 3),
         .threshold = round(.threshold, 3),
         model = factor(model,
                        levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) |>  
  filter(precision == .70) |>  
  group_by(model, precision) |>  
  summarise(recall = mean(recall),
            threshold = mean(.threshold),
            .groups = "drop")
```
# Introduction

Over 30 million adults in the United States (US) had an active alcohol use disorder (AUD) in 2021, and 23.3% reported engaging in past-month binge drinking [@samhsacenterforbehavioralhealthstatisticsandquality2021NSDUHDetailed2021]. Alcohol ranks as the third leading preventable cause of death in the US, accounting for approximately 140,000 fatalities [@centersfordiseasecontrolandpreventioncdcAnnualAverageUnited] and economic costs that exceed $249 billion annually [@substanceabuseandmentalhealthservicesadministrationusFacingAddictionAmerica2016].

Existing clinician-delivered treatments for AUD that were derived from Marlatt's relapse prevention <!-- SS: I've been seeing this as lowercase in most cited literature --> model [@marlattRelapsePreventionMaintenance1985] are effective when delivered (e.g., cognitive-behavioral therapy, mindfulness-based relapse prevention [@bowenRelativeEfficacyMindfulnessBased2014]). Unfortunately, fewer than 1 in 20 adults with an active AUD receive any treatment [@samhsa2021NSDUHDetailed2021].  Even more concerning, failure to access treatment is associated with demographic factors including race, ethnicity, geographic region, and socioeconomic status, which further increase mental health disparities [@officeofthesurgeongeneralusMentalHealthCulture2001]. This treatment gap and associated disparities stem from well-known barriers to receiving clinician-delivered mental healthcare related to affordability, accessibility, availability, and acceptability [@jacobsonDigitalTherapeuticsMental2022].

Digital therapeutics may help to overcome these barriers associated with in-person, clinician-delivered treatments. Digital therapeutics provide evidence-based interventions and other supports via smartphones to prevent, treat, or manage a medical disorder, either independently or in conjunction with traditional treatments [@jacobsonDigitalTherapeuticsMental2022].  They offer highly scalable, on-demand therapeutic support that is accessible whenever and wherever it is needed most. Several large, randomized controlled trials have confirmed that digital therapeutics for AUD improve clinical outcomes [@gustafsonSmartphoneApplicationSupport2014; @campbellInternetdeliveredTreatmentSubstance2014; @jacobsonDigitalTherapeuticsMental2022]. Additionally, US adults (including patients with AUD[@wyantAcceptabilityPersonalSensing2023]) display high rates of smartphone ownership (over 85% in 2021), with minimal variation across race, ethnicity, socioeconomic status, and geographic settings [@pewresearchcenterMobileFactSheet2021]. Therefore, digital therapeutics may not only mitigate in-person treatment barriers but also combat associated disparities[@jacobsonDigitalTherapeuticsMental2022].

## Improving Digital Therapeutics via Personal Sensing

Despite the documented benefits of digital therapeutics, their full potential has not yet been realized.  Patients often don't engage with digital therapeutics as developers intended, and long-term engagement may not be sustained or matched to patients' needs [@hatchExpertConsensusSurvey2018; @jacobsonDigitalTherapeuticsMental2022].  The substantial benefits of digital therapeutics come from easy, 24/7 access to their intervention and other support modules.  However, the burden falls primarily on the patient to identify the most appropriate modules for them in that specific moment during their recovery.

This difficulty is magnified by the dynamic, chronic, and relapsing nature of AUD [@brandonRelapseRelapsePrevention2007]. Numerous risk and protective factors interact in complex, non-linear ways to influence the probability, timing, and severity of relapse (i.e., a goal-inconsistent return to frequent, harmful alcohol use) [@witkiewitzModelingComplexityPosttreatment2007]. Factors such as urges, mood, lifestyle imbalances, self-efficacy, and motivation can all vary over time. Social networks may evolve to become more protective or risky, and high-risk situations can arise unexpectedly. Consequently, both relapse risk and the factors driving that risk fluctuate over time. 

Successful, continuous monitoring of risk for relapse and its contributing factors would enable patients to adapt their lifestyle, behaviors, and supports to their changing needs. Successful monitoring could also direct patients to engage with the most appropriate digital therapeutic modules, addressing the unique risks present at any given moment throughout their recovery. Such continuous monitoring is now feasible via personal sensing (i.e., in-situ data collection via sensors embedded in individuals' day to day lives) [@epsteinPredictionStressDrug2020; @soysterPooledPersonspecificMachine2022; @moshontzProspectivePredictionLapses2021; @wyantAcceptabilityPersonalSensing2023; @chihPredictiveModelingAddiction2014; @baeMobilePhoneSensors2018]. 

The current project focuses explicitly on using ecological momentary assessment (EMA) for monitoring risk of return to alcohol use. EMA can be easily implemented with only a smartphone. Moreover, comparable item responses can be collected consistently across different hardware and operating systems. Thus, EMA can be incorporated essentially identically into any existing or future smartphone-based digital therapeutic. EMA, like other personal sensing methods, can support the frequent, in situ, longitudinal measurement necessary for monitoring fluctuating relapse risk.  Long-term monitoring with EMA has been well-tolerated by individuals with AUD [@wyantAcceptabilityPersonalSensing2023].  Additionally, previous research has validated the use of EMA to measure known risk and protective factors for relapse, including craving [@dulinSmartphonebasedMomentaryIntervention2017], mood [@russellAffectRelativeDayLevel2020], stressors [@wemmDaybydayProspectiveAnalysis2019], positive life events [@dvorakTensionReductionAffect2018], and motivation/efficacy [@dvorakEcologicalMomentaryAssessment2014]. EMA offers privileged access into these and other subjective factors that may be difficult to quantify reliably through other sensing methods.

## Promising Preliminary Research
Preliminary research is now emerging that uses EMA responses as features in machine learning models to predict the probability of future alcohol use [@baeMobilePhoneSensors2018; @soysterPooledPersonspecificMachine2022; @waltersUsingMachineLearning2021; @chihPredictiveModelingAddiction2014].  This research is important because it rigorously required strict temporal ordering necessary for true prediction, with features measured before alcohol use outcomes. It also used resampling methods (e.g., cross-validation) that prioritize model generalizability to increase the likelihood these models will perform well with new patients.

Despite this initial promise, several important limitations exist.  Some prediction models have been trained using convenience samples (e.g., college students) [@soysterPooledPersonspecificMachine2022; @baeMobilePhoneSensors2018].  Other models have been developed to predict hazardous alcohol use in non-treatment-seeking populations [@waltersUsingMachineLearning2021]. In both these instances, features that predict planned or otherwise intentional alcohol use among individuals not motivated to change their behavior may not generalize to patients in AUD recovery.  Moreover, individuals who have not yet begun to contemplate and/or commit to behavior change regarding their alcohol use are unlikely to use digital therapeutics [@prochaskaSearchHowPeople1992].  

A handful of other models have been trained to predict putative precursors of substance use, such as craving [@burgess-hullTrajectoriesCravingMedicationassisted2022; @dumortierClassifyingSmokingUrges2016] and stress [@epsteinPredictionStressDrug2020]. Although craving and stress may be associated with substance use, their relationships with relapse are complex, inconsistent, and not always very strong [@fronkStressAllostasisSubstance2020, @sayetteRoleCravingSubstance2016], making these constructs less than ideal as prediction targets.

Models that predict lapses (i.e., single instances of goal-inconsistent alcohol use) may be preferred. Lapses are clearly defined, observable, and have temporally precise onsets and offsets. Conversely, definitions of relapse vary widely [@witkiewitzModelingComplexityPosttreatment2007], and it is difficult to delineate precisely when relapse begins or ends.  Lapses always precede relapse and therefore may serve as an early warning sign for intervention. Finally, maladaptive responses to a lapse (e.g., abstinence violation effects; [@marlattRelapsePreventionMaintenance1985]) can undermine recovery by themselves, making lapses clinically meaningful events to detect and address.

An early lapse prediction model developed by Gustafson and colleagues [@chihPredictiveModelingAddiction2014] provided the foundation on which our current project builds. Participants completed EMAs once per week for 8 months while using a digital therapeutic after discharge from an inpatient treatment program for AUD. These EMAs were used as features in a machine learning model to predict alcohol use lapses.  However, the temporal precision for both the features and outcome was coarse. Model predictions were updated only once per week at most, and lapse onsets could occur anytime within the next two weeks. This coarseness restricts the model from being used to implement *just-in-time* interventions (e.g., guided mindfulness or other stress reduction techniques, urge surfing) that are well-suited to digital therapeutics.

## The Current Study

The current study addresses these limitations of previously developed prediction models.  We trained our models using participants in early recovery from moderate to severe AUD who reported a goal of alcohol abstinence.  We developed three separate models that provide hour-by-hour probabilities of a future lapse back to alcohol use with increasing temporal precision: lapses in the next week, next day, and next hour.  Model features were engineered from raw scores and longitudinal change in responses to 4X daily EMAs.  These features were derived to measure theoretically-implicated risk factors and contexts[@marlattRelapsePreventionMaintenance1985] including past use, craving, past pleasant events, past and future risky situations, past and future stressful events, emotional valence and arousal, and self-efficacy. This research represents an important step toward the development of a "smart" (machine learning guided) sensing and prediction system that can be embedded within a digital therapeutic both to identify periods of peak lapse risk and to recommend specific supports to address factors contributing to this risk.  

# Method
<!--JC to add statement about power-->
<!--KW to add reference to methods in supplement-->

## Transparency and openness 
We adhere to research transparency principals that are crucial for robust and replicable science. We reported how we determined the sample size, all data exclusions, all manipulations, and all study measures. We completed a transparency report (see Supplement). Finally, we made the data, analysis scripts, annotated results, questionnaires, and other study materials publicly available [https://osf.io/w5h9y/](https://osf.io/w5h9y/). 


Our study design and analyses were not pre-registered. However, we restricted many researcher degrees of freedom via cross-validation. Cross-validation inherently includes replication; models are fit on held-in sets, decisions are made in held-out validation sets, and final performance is evaluated on held-out test sets.

## Participants
We recruited 151 participants in early recovery (1-8 weeks of abstinence) from AUD in Madison, Wisconsin, USA. Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required participants:

1.  were age 18 or older,
2.  could write and read in English,
3.  had at least moderate AUD (\>= 4 self-reported DSM-5 symptoms),
4.  were abstinent from alcohol for at least 1 week but no longer than 2 months,
5.  were willing to use a single smartphone (personal or study provided) while enrolled in the study.

We also excluded participants exhibiting severe symptoms of psychosis or paranoia. 

## Procedure
Participants completed five study visits over approximately three months. After an initial phone screen, participants attended an in-person screening visit for eligibility determination, informed consent, and collection self-report measures. Eligible and consented participants returned approximately one week later for an intake visit. Three additional follow-up visits occurred about every 30 days that participants remained on study. Participants were expected to complete four daily EMAs while on study. Other personal sensing data streams (geolocation, cellular communications, sleep quality, and audio check-ins) were collected as part of the parent grant's aims (R01 AA024391). 

## Measures
### EMA
<!--KW: we need to be consistent with how we refer to EMA constructs. I have edited to consistently use past use, future efficacy, craving, past and future stressful events, future and past risky situations, past pleasant events, arousal, and valence-->
<!--KW: Also, consider changing lapse day and lapse hour on SHAP plots to start day of prediction window and start hour of prediction window-->
Participants completed four brief (7-10 questions) EMAs daily following text message reminders. All EMAs included seven items that asked about any past alcohol use, current affective state (valence and arousal), craving, past stressful events, risky situations and pleasant events. The first EMA each day included three additional questions about the likelihood of encountering a risky situation, a stressful event, and drinking alcohol in the upcoming week (i.e., future efficacy). 

The first and last EMAs of the day were scheduled within one hour of participants' typical wake and sleep times. The other two EMAs were scheduled randomly within the first and second halves of the participants' typical day, with at least one hour between EMAs. 

### Individual Differences
We collected self-report information about demographics (age, sex, race, ethnicity, education, employment, income, and marital status) and clinical characteristics (AUD milestones, number of quit attempts, lifetime AUD treatment history, lifetime receipt of AUD medication, DSM-5 AUD symptom count, and current drug use [@whoassistworkinggroupAlcoholSmokingSubstance2002]). Only age, sex, race, education, and marital status were used as model features.

## Data Analytic Strategy
Data preprocessing, modeling and Bayesian analyses were done in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020]. All models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computing [@chtc].

### Lapse Labels
We predicted future lapses in three window widths that varied in their temporal precision: one week, one day, and one hour. Prediction windows were updated hourly. All classification models provide hour-by-hour predictions of future lapse probability for all three window widths.

We labeled each prediction window as *lapse* or *no lapse* using participants' reports from the EMA item "Have you drank any alcohol that you have not yet reported?". If participants answered yes to this question, they were prompted to enter the hour and date of the start and end of the drinking episode. These reports were validated by study staff during monthly followup visits. 

### Feature Engineering
Features were calculated using only data collected prior to the start of each prediction window. This ensured our models were making true *future predictions* versus identifying concurrent associations.  

Features were derived from three sources: baseline demographic characteristics (i.e., age, sex, race, marital status, education); day of the week and the time of day (daytime vs. evening/night) of the start of the prediction window; and previous EMA responses. We scored raw min, max, median, and count features from previous EMA items within varying lead up times (6, 12, 24, 48, 72, and 168 hours prior to start of window).  We scored change EMA response features by subtracting the mean response for each feature over all data prior to the start of the prediction window from the associated raw feature.  

### Model Training and Evaluation

#### Statistical Algorithm and Hyperparameters
We trained and evaluated three separate classification models: one each for week, day, and hour  prediction windows.  We initially considered four well-established statistical algorithms (XGBoost, Random Forest, K-Nearest Neighbors, and Elastic Net) that vary across characteristics expected to affect model performance (e.g., flexibility, complexity, and ability to handle higher-order interactions natively) [@kuhnAppliedPredictiveModeling2018]. However, preliminary exploratory analyses suggested that XGBoost consistently outperformed the other three algorithms.  Furthermore, the Shapley Additive Explanations (SHAP) method, which we planned to use for explanatory analyses of feature importance, is optimized for XGBoost.  For these reasons, we focused our primary model training and evaluation on the XGBoost algorithm only.  
  
Candidate XGBoost model configurations differed across sensible values for the hyperparameters mtry, tree depth, and learning rate using grid search.  All configurations used 500 trees with early stopping to prevent over-fitting.  All other hyperparameters were set to defaults established by the tidymodels packages.  Candidate model configurations also differed on outcome resampling method (i.e., up-sampling and down-sampling of the outcome using majority/no lapse to minority/lapse ratios ranging from 1:1 to 5:1).  We calibrated predicted probabilities using the beta distribution <!--KW: this second half of the sentence feels a little wordy but not familiar enough with the method to try to make more clear-->to support optimal decision-making under variable outcome distributions[@kullSigmoidsHowObtain2017].

#### Performance Metric
Our primary performance metric model selection and evaluation was area under the Receiver Operating Characteristic Curve (auROC) [@kuhnAppliedPredictiveModeling2018]. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (i.e., lapse) relative to a randomly selected negative case (i.e., no lapse). This metric was selected because it 1) combines sensitivity and specificity, which are both important characteristics to consider for clinical implementation; 2) is an aggregate metric across all decision thresholds, which is important because optimal decision thresholds may differ across settings and goals; and 3) is unaffected by class imbalance, which is important for comparing models with differing window widths and levels of class imbalance.

#### Cross-validation 
We used participant-grouped, nested cross-validation for model training, selection, and evaluation with auROC. Grouped cross-validation assigns all data from a participant as either held-in or held-out to avoid bias introduced when predicting a participant's data from their own data.  

Nested cross-validation uses two nested loops for dividing and holding out folds: an outer loop, where held-out folds serve as *test sets* for model evaluation; and inner loops, where held-out folds serve as *validation sets* for model selection. Importantly, these loops are independent, maintaining separation between data used to train the models, *validation sets*, and *test sets*. This separation removes optimization bias from the evaluation of model performance in the test sets and can yield lower variance performance estimates than single test set approaches [@jonathanUseCrossvalidationAssess2000].  

We used 1 repeat of 10-fold cross-validation for the inner loops and 3 repeats of 10-fold cross-validation for the outer loop.  Best model configurations were selected based on the median <!--JC will add footnote for why median-->auROC across the 10 *validation sets*<!--KW: 10 validation sets per held-out outer fold?-->.  Final performance evaluation of those best model configurations was based on the median auROC across the 30 *test sets*.  For completeness, we report median auROC for our best model configurations for each model (week, day, and hour) separately from both the validation and test sets. In addition, we report key additional performance metrics for the best model configurations including sensitivity, specificity, balanced accuracy, positive predictive value (PPV), and negative predictive value (NPV) from the test sets [@kuhnAppliedPredictiveModeling2018].

### Bayesian Estimation of auROC and Model Comparisons 

We used a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian confidence intervals (CIs) for auROC for the three best models (i.e., week, day, and hour). To determine the probability that these models' performance differed systematically from each other, we regressed the auROCs<!--KW: posterior probabilities?--> (logit transformed) from the 30 test sets for each model as a function of window width. Following recommendations from the tidymodels team [@kuhnTidyposteriorBayesianAnalysis2022; @kuhnBayesianAnalysisResampling], we set two random intercepts: one for the repeat, and another for the fold within repeat (folds are nested with repeats for auROCs collected with 3x10-fold cross-validation). We report the 95% (equal-tailed) Bayesian CIs from the posterior probability distributions for our models auROCs.  We also report 95% (equal-tailed) Bayesian CIs for the differences in performance among the three models.  



### Shapley Additive Explanations for Feature Importance

We computed Shapley Values [@lundbergUnifiedApproachInterpreting2017] to provide a consistent and objective explanation of the importance of categories of features across our three models. Shapley values are model-agnostic and possess several useful properties including: Additivity (Shapley values for each feature can be computed independently and summed); Efficiency (the sum of Shapley values across features must add up to the difference between predicted and observed outcomes for each observation); Symmetry (Shapley values for two features should be equal if the two features contribute equally to all possible coalitions); and Dummy (a feature that does not change the predicted value in any coalition will have a Shapley value of 0).  We calculated Shapley values from the 30 test sets using the SHAPforxgboost package that provides Shapley values in log-odds units for binary classification models.  We averaged the three Shapley values for each observation for each feature across the three repeats to increase their stability. To calculate the local (i.e., for each observation) impact of categories of features (e.g., all features associated with the EMA craving item), we added Shapley values across all features in a category, separately for each observation.  To calculate global importance for categories of features, we averaged the absolute value of the Shapley values of all features in the category across all observations.  



# Results

## Demographic and Clinical Characteristics

One hundred ninety-two participants were eligible for enrollment. Of these, 191 consented to participate and 169 subsequently enrolled in the study. Fifteen participants discontinued prior to the first monthly follow-up visit. We excluded data from one participant who did not maintain a goal of abstinence during their participation. We also excluded data from two participants due to evidence of careless responding and unusually low compliance. Our final sample consisted of 151 participants (see Figure S1 for more detail on enrollment and disposition). 

The final sample included approximately equal numbers of men (N=77; 51%) and women (N=74; 49%) who ranged in age from 21 - 72 years old.  The sample was majority White (N=131; 87%) and non-Hispanic (N=147; 97%).  Participants self-reported a mean of 8.9 DSM-5 symptoms of AUD (SD=5.8; range=4-11) and a mean of 5.5 previous quit attempts (SD=5.8, range=0-30).  Most participants (N=84; 56%) reported one or more lapses during their participation.  The mean number of lapses per participant during the study period was 6.8 (SD=12.0; range=0-75).  Table 1 provides more detail on demographic and clinical characteristics of the sample.


## EMA compliance, features, and prediction window labels

Participants on average completed 3.1 (SD=0.6) of the four daily EMAs each day (78% compliance overall).  Participants completed at least one EMA on 95% of days.  Across individual weeks in the study, EMA compliance percentages ranged from 75% to 87% completion for all of the 4x daily EMAs and from 92% - 99% for at least one daily EMA completed (see Figure S3).  

Using these EMA reports, we created datasets with 270,081, 274,179, and 267,287 future prediction windows for the week, day, and hour window widths, respectively.  Each dataset contained 286 features and an outcome labeled as *lapse* or *no lapse*. These datasets were unbalanced with respect to the outcome such that lapses were observed in 68,467 (25.3%) week windows, 21,107 (7.7%) day windows, and 1,017 (0.3%) hour windows. 

 
## Model Performance

### auROC

Best model configurations were selected via *validation set* performance. The median auROCs for the best configurations were high for the week (median=`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> median())`, IQR=`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> IQR())`, range=`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> min())`-`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> max())`), day (median=`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> median())`, IQR=`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> IQR())`, range=`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> min())`-`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> max())`), and hour (median=`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> median())`, IQR=`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> IQR())`, range=`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> min())`-`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> max())`) prediction windows. 

Best model configurations were evaluated via *test set* performance. The median auROC across the 30 test sets remained high for the week (median=`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> median())`, IQR=`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> IQR())`, range=`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> min())`-`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> max())`), day (median=`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> median())`, IQR=`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> IQR())`, range=`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> min())`-`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> max())`), and hour (median=`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> median())`, IQR=`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> IQR())`, range=`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> min())`-`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> max())`) prediction windows.  The left panel of Figure 1 displays the ROC curves by model (i.e., window width) derived by aggregating predicted lapse probabilities across all test sets.  Figure S4 presents the individual ROC curves from each test set.  

The right panel of Figure 1 displays posterior probability distributions for the auROC separately by model.  The median auROCs from these posterior distributions were `r sprintf("%1.2f", ci |> filter(model=="Week") |> pull(median) |> as.numeric())`,  `r sprintf("%1.2f", ci |> filter(model=="Day") |> pull(median) |> as.numeric())`, and `r sprintf("%1.2f", ci |> filter(model=="Hour") |> pull(median) |> as.numeric())` for the week, day, and hour models, respectively.  These values represent our best estimates for the magnitude of the auROC parameter for each models.  The 95% Bayesian CI for the auROCs for these models were relatively narrow and did not contain 0.5 (i.e., chance performance) for any of the three window widths: week [`r sprintf("%1.2f", ci |> filter(model=="Week") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci |> filter(model=="Week") |> pull(upper) |> as.numeric())`], day [`r sprintf("%1.2f", ci |> filter(model=="Day") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci |> filter(model=="Day") |> pull(upper) |> as.numeric())`], hour [`r sprintf("%1.2f", ci |> filter(model=="Hour") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci |> filter(model=="Hour") |> pull(upper) |> as.numeric())`].  

We used these posterior probability distributions for the auROCs to formally compare the differences in performance of these models.  The median increase in auROC for the hour vs. the day model was `r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. day") |> pull(median) |> as.numeric())` (95% CI=[`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. day") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. day") |> pull(upper) |> as.numeric())`], yielding a probability of `r sprintf("%1.3f", pp_diffs |> filter(contrast=="hour vs. day") |> pull(difference) |> (\(x) mean(x>0))())` that the hour model had superior performance relative to the day model.  The median increase in auROC for the hour vs. the week model was `r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. week") |> pull(median) |> as.numeric())` (95% CI=[`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. week") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. week") |> pull(upper) |> as.numeric())`], yielding a probability of `r sprintf("%1.3f", pp_diffs |> filter(contrast=="hour vs. week") |> pull(difference) |> (\(x) mean(x>0))())` that the hour model had superior performance relative to the week model.  The median increase in auROC for the day vs. the week model was `r sprintf("%1.2f", ci_diffs |> filter(contrast == "day vs. week") |> pull(median) |> as.numeric())` (95% CI=[`r sprintf("%1.2f", ci_diffs |> filter(contrast == "day vs. week") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci_diffs |> filter(contrast == "day vs. week") |> pull(upper) |> as.numeric())`], yielding a probability of `r sprintf("%1.3f", pp_diffs |> filter(contrast=="day vs. week") |> pull(difference) |> (\(x) mean(x>0))())` that the day model had superior performance relative to the week model.  Figure S5 presents histograms of the posterior probability distributions for these model contrasts on auROC.


<!--************************************************************************-->
<!-- Figure 1: ROC and Posterior probability histograms for auROC by model-->  
<!--JC: check fig caption and fig note-->

```{r fig_1_roc}
#| output: true
#| apa-cap: ROC curves and posterior probabilities for auROCs
#| apa-note: The left panel depicts the aggregate receiver operating characteristic (ROC) curve for each model, derived by concatenating predicted lapse probabilities across all test sets. The dotted line represents the expected ROC curve for a random classifier. The histograms on the right depict the posterior probabilities for the areas under the receiver operating characteristic curves (auROCs) for each model. The vertical lines represent the median posterior probability and the horizontal line represents the 95\% credible interval.
roc_plot <- roc_all |>  
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"), 
                        labels = c("Week", "Day", "Hour"))) |>  
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path(linewidth = 1.25, show.legend = FALSE) +
  geom_abline(lty = 3) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Specificity",
       y = "Sensitivity") +
  scale_x_continuous(breaks = seq(0,1,.25),
                     labels = sprintf("%.2f", seq(1,0,-.25))) 

pp_plot <- pp_tidy |>  
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour"))) |> 
  ggplot() + 
  geom_histogram(aes(x = posterior, fill = model), color = "black", alpha = .4, 
                 bins = 30) +
  geom_segment(mapping = aes(y = y+100, yend = y-100, x = median, xend = median,
                           color = model), show.legend = FALSE, data = ci) +
  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, color = model),
              show.legend = FALSE, data = ci) +
  # geom_text(data = ci, x = c(.93, .907, .92), y = 1000, 
  #           label = str_c(round(ci$median, 2), " [", round(ci$lower, 2), ", ", round(ci$upper, 2), "]")) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous("Count", breaks = c(0, 500, 1000)) +
  xlab("Posterior probability for auROC") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank())

roc_plot + pp_plot +
  plot_layout (guides = "collect") &
  theme(legend.position = "bottom")
```




### Other Performance Metrics

Figure S6 displays histograms for the predicted probabilities of lapse for all observations in the 30 *test sets* separately by model and true outcome.  We evaluated the sensitivity, specificity, balanced accuracy, PPV, and NPV when these predicted lapse probabilities were used for binary classification (*lapse* vs. *no lapse*) with decision thresholds identified by Youden's Index (see Table 2).   

We created Precision-Recall curves by aggregating predicted lapse probabilities across the 30 test sets to evaluate the trade-off between PPV (i.e., precision) and sensitivity (i.e., recall) across decision thresholds (see Figure 2). The PPV of any model can be increased by increasing the decision threshold; however, increasing the decision threshold will also lower the model's sensitivity. For example, the dotted lines in Figure 2 depict the sensitivities (`r sprintf("%1.2f", ppv_70 |> filter(model == "Week") |> pull(recall))`, `r sprintf("%1.2f", ppv_70 |> filter(model == "Day") |> pull(recall))`, and `r sprintf("%1.2f", ppv_70 |> filter(model == "Hour") |> pull(recall))` for week, day, and hour models, respectively) associated with decision thresholds that yield 0.70 positive predictive value for each model.  

<!--************************************************************************-->
<!-- Table 2: Performance metrics-->
<!--John: check footnote-->

```{r table_2}
#| output: true

footnote_table_metrics <- "Areas under the receiver operating characteristic curves (auROCs) summarize the model's sensitivity and specificity over all possible decision thresholds. Sensitivity, specificity, balanced accuracy, positive predictive value, and negative predictive value are performance metrics calculated at a single decision threshold for each model. Our decision thresholds were determined with Youden’s index."

metrics |> 
 mutate(metric = case_when(metric == "auROC" ~ "auROC",
                           metric == "sens" ~ "sensitivity",
                           metric == "spec" ~ "specificity",
                           metric == "bal_accuracy" ~ "balanced accuracy",
                           metric == "ppv" ~ "positive predictive value",
                           metric == "npv" ~ "negative predictive value")) |> 
 kbl(col.names = c("Metric", "Week", "Day", "Hour"),
     booktabs = TRUE,
     digits = 2,
     align = c("l", "l", "l", "l"),
     linesep = "",
     caption = "Performance Metrics by Model") |>  
  kable_styling(position = "left", latex_options = c("HOLD_position")) |>  
  column_spec(column = 1, width = "25em") |> 
  kableExtra::footnote(general = c(footnote_table_metrics), threeparttable = TRUE)
```


<!--************************************************************************-->
<!-- Figure 2: PR curves by model-->
<!--JOHN: check fig caption-->
```{r fig_2_pr}
#| output: true
#| apa-cap: Precision-recall curves by model 
#| apa-note: The plot depicts the aggregate precision-recall curves for each model, derived by concatenating predicted lapse probabilities across all test sets. The dotted lines depict the sensitivities (0.72, 0.47, and 0.33 for week, day, and hour models, respectively) associated with decision thresholds that yield 0.70 positive predictive value for each of those models.


pr_all |>  
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) |> 
  ggplot(aes(x = recall, y = precision, color = model)) +
  geom_path(linewidth = 1.25) +
  geom_segment(mapping = aes(y = .7, yend = .7, x = -.5, xend = recall,
                           color = model),
               linetype = "dashed",
               alpha = .8,
               show.legend = FALSE,
               data = ppv_70) +
  geom_segment(mapping = aes(y = -.5, yend = .7, x = recall, xend = recall,
                           color = model),
               linetype = "dashed",
               alpha = .8,
               show.legend = FALSE,
               data = ppv_70) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Sensitivity (Recall)",
       y = "Positive Predictive Value (Precision)")
```



## Feature Importance

We display the global importance (mean |Shapley value|) for feature categories for each of the three models in Panel A of Figure 3.  These feature categories are ordered by their aggregate global importance (i.e., total bar length) across the three models.  The importance of each feature category for specific models is displayed separately by color.  

We display local Shapley values that quantify the influence of feature categories on individual observations (i.e., a single prediction window for a specific participant) for each model in Panels B-D of Figure 3.  


<!--************************************************************************-->
<!-- Figure 3: SHAP Importance figure-->
<!--NEED TO ADD HEAT COLORS FOR DIRECTION TO LOCAL PLOTS - KENDRA?-->
<!--CONSIDER PANEL TITLES?-->
<!--CONSIDER WHERE LEGEND GOES-->
<!--JOHN: check fig caption. Also, I add panel titles and moved legend. The code is commented out for knitting the paper though right now because it takes so long. Fig is instead saved out and read in.-->
```{r make_global_shap}
# panel_shap_global <- shap_global_all |>  
#   mutate(variable_grp = reorder(variable_grp, mean_value, sum)) |>  
#   ggplot() +
#   geom_bar(aes(x = variable_grp, y = mean_value, fill = model), stat = "identity", alpha = .4) +
#   ylab("Mean(|Shapley Value|)") +
#   xlab("") +
#   theme(axis.text = element_text(size = 9.5)) +
#   coord_flip()
```

```{r make_shap_local}
# # get colors for models to match other figures that use faceting or grouping
# colors_hex <- scales::hue_pal()(3) # week = 1, day = 2, hour = 3
# 
# # order features to match global plot
# shap_levels <- shap_global_all |>
#   mutate(variable_grp = reorder(variable_grp, mean_value, sum)) |>
#   pull(variable_grp) |>
#   levels()
# 
# # downsample to 10% of observations for each plot
# downsample_ratio <- .10
# ids_week <- shap_local_week |>
#   pull(id_obs) |>
#   unique()
# ids_week <- ids_week |> sample(size = round(length(ids_week)/(1/downsample_ratio)))
# ids_day <- shap_local_day |>
#   pull(id_obs) |>
#   unique()
# ids_day <- ids_day |> sample(size = round(length(ids_day)/(1/downsample_ratio)))
# ids_hour <- shap_local_hour |>
#   pull(id_obs) |>
#   unique()
# ids_hour <- ids_hour |> sample(size = round(length(ids_hour)/(1/downsample_ratio)))
# 
# # week panel
# panel_shap_local_week <- shap_local_week |>
#   filter(id_obs %in% ids_week) |>
#   mutate(variable_grp = factor(variable_grp, levels = shap_levels)) |>
#   ggplot(mapping = aes(x = variable_grp, y = value)) +
#   ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4,
#                      color = colors_hex[1]) +
#   geom_hline(yintercept = 0) +
#   scale_y_continuous(limits = c(-2, 5), breaks = seq(-2, 5)) +
#   ylab("Shapley Value") +
#   xlab("") +
#   theme(axis.text = element_text(size = 9.5)) +
#   coord_flip()
# 
# # day panel
# panel_shap_local_day <- shap_local_day |>
#   filter(id_obs %in% ids_day) |>
#   mutate(variable_grp = factor(variable_grp, levels = shap_levels)) |>
#   ggplot(mapping = aes(x = variable_grp, y = value)) +
#   ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4,
#                      color = colors_hex[2]) +
#   geom_hline(yintercept = 0) +
#   scale_y_continuous(limits = c(-2, 5), breaks = seq(-2, 5)) +
#   ylab("Shapley Value") +
#   xlab("") +
#   theme(axis.text = element_text(size = 9.5)) +
#   coord_flip()
# 
# # hour panel
# panel_shap_local_hour <- shap_local_hour |>
#   filter(id_obs %in% ids_hour) |>
#   mutate(variable_grp = factor(variable_grp, levels = shap_levels)) |>
#   ggplot(mapping = aes(x = variable_grp, y = value)) +
#   ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4,
#                      color = colors_hex[3]) +
#   geom_hline(yintercept = 0) +
#   scale_y_continuous(limits = c(-2, 5), breaks = seq(-2, 5)) +
#   ylab("Shapley Value") +
#   xlab("") +
#   theme(axis.text = element_text(size = 9.5)) +
#   coord_flip()
```


```{r fig_3_shap}
#| output: true
#| apa-cap: Variable Importance (Shapley Values) by Model
#| apa-note: Panel A displays the global importance (mean |Shapley value|) for feature categories for each model. Raw EMA features are grouped into categories by the original item from the EMA. Features from demographics and the day and hour for the start of the prediction window are also included. Feature categories are ordered by their aggregate global importance (i.e., total bar length) across the three models.  The importance of each feature category for specific models is displayed separately by color. Panels B-D display local Shapley values that quantify the influence of feature categories on individual observations (i.e., a single prediction window for a specific participant) for each model.

knitr::include_graphics(path = "figures/fig_3.jpeg", dpi = 130)

# panel_shap_global + panel_shap_local_week + panel_shap_local_day + panel_shap_local_hour +
#    plot_annotation(tag_levels = 'A') +
#    plot_layout (ncol = 2, guides = "collect", width = c(1,1)) 
```



# Discussion

## Model Performance

All three of our models performed exceptionally well, yielding auROCs of .90, .91, and .94 for week, day, and hour level models, respectively. auROCs above .9 are generally described as having "excellent" performance, meaning that the model will correctly assign a higher probability to a positive case (e.g., lapse) than a negative case 90% of the time [@mandrekarReceiverOperatingCharacteristic2010]. This confirms that EMA can be used in our models to predict future alcohol lapses in the next week, next day, and next hour with high levels of sensitivity and specificity for new patients that were not used to train these models. 

This study addressed several important limitations of previous research to advance us toward robust sensing and prediction models that can be embedded within digital therapeutics.  First, our models were trained on a relatively large, treatment-seeking sample of adults in early recovery from AUD that more closely matches the individuals most likely to benefit from such models within a digital therapeutic. Second, we explicitly predicted episodes of goal-inconsistent alcohol use (i.e., lapses) because features that predict goal-inconsistent use likely differ from those that predict other types of alcohol use (e.g., episodes of binge drinking among college students, intentional instances of drinking among people not in recovery). Third, we measured EMA predictors and alcohol use with sufficient frequency and granularity to train well-performing models with high temporal resolution - specifically hour-by-hour predicted probabilities for lapses in the next week, day and hour.  Fourth, we collected predictors and outcomes over a clinically meaningful duration (up to three months) during a high risk period (initial remission[@hagmanDefiningRecoveryAlcohol2022] from AUD).  Fifth, we used cutting edge resampling methods (grouped, nested, k-fold cross-validation) to provide valid estimates of how our models would perform with new participants that were not used to train these models.  Finally, we used methods from interpretable machine learning (i.e. SHAP[@lundbergUnifiedApproachInterpreting2017; @molnarInterpretableMachineLearning2022]) to better understand how our models made predictions globally and locally for specific participants at discrete moments in time. 

## Understanding & Contextualizing Model Performance

As noted, we used SHAP to explore how key relapse prevention model constructs (represented by categories of features) contributed to predicted lapses. Some constructs consistently emerged as globally important across week, day, and hour level models. Unsurprisingly, the largest contribution to lapse prediction was past use. An individual who reported lapsing frequently was more likely to lapse at any given observation in the future. This is consistent with decades of research on relapse precipitants and our understanding of human behavior more generally (i.e., past behavior predicts future behavior) [@marlattRelapsePreventionMaintenance1985].  Additionally, decreases in self-efficacy were also strongly associated with increased probability of future lapses across all three models.

The contribution of some constructs differed depending on the width of the prediction window. For example, many of the more punctuate, time-varying constructs (e.g., craving, past stressful events, arousal) had greater impact on predicted lapse probabilities in the next hour model. The next hour model was also better able to exploit features for the time and day of the week for the start of the prediction window.  Of course, prediction window start time and start day were not useful features in the next week model because its associated prediction window spanned all days and times.  The increased global importance for these categories of features to immediate lapse risk likely contributed to the next hour model outperforming the day and week models. These important global differences in next hour lapse risk also highlight the need for just-in-time interventions that can address these imminent but short-lived risks.

The individual, local SHAP values also shed light on the multidimensional and heterogeneous nature of lapse risk in our sample. Sina plots of local SHAP values (Figure 3) display meaningful ranges of scores for most feature categories.  This means that even feature categories with lower global importance (e.g., past pleasant events, future stressful events) still consequentially impacted lapse probability predictions for some individuals at specific times. This variability in locally important features speaks to the need for personalized recommendations about optimal interventions and other supports to address the unique lapse risks for that person at that moment in time.

The demographic variables included in our models did not display either high global importance or local importance for specific predictions. Despite the diversity in socioeconomic status, gender, and age in our sample, these features did not significantly contribute to lapse prediction. While this does not rule out these features' predictive utility, it does suggest that other EMA feature categories (e.g., past use, future efficacy, craving) may be more relevant for lapse prediction than these characteristics. Race and ethnicity also did not emerge as globally or locally important features. However, the limited representation of participants of color in our sample warrants caution in drawing conclusions about the predictive utility of race and ethnicity at this time. 

## Considerations for Clinical Implementation

### Smart Digital Therapeutics.

We believe these models may be most effective when embedded in a "smart" digital therapeutic that can guide patients toward optimal, adaptive engagement to address their ongoing and momentary risks.  These models can provide the patient's predicted future lapse probability and the features that meaningfully contribute to that predicted probability.  We consciously selected EMA items to map onto known risk factors from the Relapse Prevention model. Consequently, these outputs can be used to recommend specific intervention and support modules that are risk-relevant for each patient - much like a clinician would do if they were available in the moment.  For example, during sensed periods of high stress, modules that can lower stress (e.g., guided mindfulness, guided body scans) could be recommended.  If increased time with risky people or locations is driving lapse risk, the digital therapeutic can support patients to find and attend AA or other support meetings. They could also be encouraged to participate in the in-app discussion board to build a healthy community there.  

These module recommendations can also be tuned more precisely using the patient's current lapse probability.  If increased craving yields a high predicted lapse probability, stimulus control modules would be recommended (e.g., immediate removal of drinking cues from environment, leave unsafe environments).  Conversely, if craving is detected but lapse probability is lower, urge management modules that permit coping with the craving in-place could be recommended (e.g., urge surfing, distracting activities/games).

Of course, we must first determine how best to provide module recommendations such that patients trust and follow the recommendation.  Increasing the interpretability and transparency of otherwise "black box" machine learning prediction models can improve perceptions of them, but providing complex or otherwise unnecessary information may instead undermine trust in these models[@molnarInterpretableMachineLearning2022]. Therefore, additional research using appropriate research designs is needed to optimize recommendation messaging to increase adherence and associated clinical outcomes[@collinsOptimizationBehavioralBiobehavioral2018].

A smart digital therapeutic can potentially improve clinical outcomes in multiple ways.  First, feedback from the prediction model could improve patient insight and self-monitoring by connecting their daily experiences to changes in their lapse risk. Second, it can remove patient uncertainty about how to use the digital therapeutic, which could otherwise present a barrier to engagement for some due to the substantial content available. Third, a smart digital therapeutic could encourage risk-relevant engagement.  Rather than simply trying to increase overall time using the digital therapeutic, patients could be guided to engage with the interventions and other supports that specifically target their personal risk factors at that moment in time.  <!-- GEF: I was going to add a clarifying sentence to the second point re: removing patient uncertainty to say something about HOW it would remove this uncertainty (i.e., providing tailored recommendations), but then that feels too overlapping with the third point. That second point feels a bit unfinished/rushed to me right now, but I'm not sure how to provide that clarification without repeating content from the third point about personally relevant engagement -->Thus, smart digital therapeutics are well-positioned to pursue the precision mental health goal to "provide the right treatment to the right patient at the right time, every time"[@kaiserObamaGivesEast2015]. 

### Categorical Lapse Predictions.

Our models natively provide quantitative predictions of lapse probabilities.  These lapse probabilities can also be used to make specific categorical predictions (lapse vs. no-lapse) for the relevant future prediction windows.  This is accomplished by applying a decision threshold to the quantitative predicted lapse probabilities; i.e.,  when the probability exceeds the decision threshold, a lapse is predicted.  

We observed high sensitivity and specificity for these categorical predictions at a decision threshold selected to balance these two performance metrics.  However, the PPV (i.e., proportion of predicted lapses that were true lapses) of these categorical predictions was moderate to very low at this threshold (ranging from .60 down to .02 across models).  For this reason, we believe that these categorical predictions should be provided to patients with extreme caution (if at all for the models with lower PPV).  Instead, we favor use of the quantitative lapse probabilities as a risk indicator as proposed above to guide intervention and support recommendations.

If categorical predictions are necessary, PPV can be improved by raising the decision threshold, but this comes at the cost of reduced sensitivity.  We explored this trade-off in the precision-recall curves displayed in Figure 2.  From these curves, it is clear decision thresholds that yield higher PPV (e.g., .70) exist for all three models, but the associated sensitivity will be lower (e.g., 0.72, 0.47, and 0.33 for the week, day, and hour models, respectively, at this threshold).  Clinical implementation of categorical predictions will require selecting an optimal decision threshold after weighing the cost of missing true lapses (low sensitivity) vs. predicting lapses that subsequently do not occur (low PPV).  Different thresholds could be used depending on the purpose, context, available resources, or even patient preference.  

## Additional Limitions and Future Directions

Successful clinical implementation of our models will require several important steps to address limitations in our work to date.  To start, we need to enrich the training data for these models to include diversity across race, ethnicity, and geographic region.  The prediction models in our study may not work well with Black and Brown people or people from rural communities.  Prediction models must be trained on diverse samples of individuals. Otherwise, their use may exacerbate rather than reduce existing mental healthcare disparities. We must also collect data from individuals in later stages of recovery beyond initial remission because the features that predict lapses may differ in these later periods as individuals become more stable.  We are intentionally addressing both of these issues in a current NIH protocol that recruits for demographic and geographic diversity across the US and follows these participants for up to 1.5 years into their recovery[@moshontzProspectivePredictionLapses2021]. 

The chronic nature of AUD may require sustained use of a sensing and prediction system.  However, this means that the burden of using such systems must be considered.  Participants with AUD find three months of 4x daily EMA to be generally acceptable and report that they could hypothetically sustain this for at least a year if there were clinical benefits to them[@wyantAcceptabilityPersonalSensing2023]. However, they also report that 1x daily EMA may be more feasible still[@wyantAcceptabilityPersonalSensing2023].  We plan to develop future prediction models that use only the single morning EMA. This would allow us to contrast the assessment burden vs. model performance trade-off between our current models and putatively lower burden models using only 1x daily EMA.  We also plan to train models that use features based on passively sensed geolocation and cellular communications data-streams (i.e., meta-data from call and text messages; text message content) that were also collected from our participants.  It may be that these passively sensed signals are sufficient as inputs to an exceptionally low burden prediction model.  Alternatively, they can be added to models that also include EMA to increase model performance further and/or to reduce the frequency or length of the EMA surveys while maintaining comparable performance.

Our current models predict probability of imminent lapses.  The next hour and day models are well-positioned to identify and recommend just-in-time interventions to address these immediate risks.  However, the next week model may not have sufficient temporal specificity to recommend immediate patient action.  Instead, its clinical utility may be improved if we shifted this coarser window duration into the future.  For example, we could train a model to predict the probability of lapse at any point during a week window that began two weeks in the future.  This "time-lagged" model could provide patients with increased lead time to implement supports that might not be immediately available to them (e.g., schedule an appointment with a therapist, request support from an AA sponsor or appropriate family and friends).  

In this study, we have demonstrated that sensing and prediction systems can now be developed to predict future lapses with high temporal resolution.  Important steps still remain before these systems can be embedded within smart digital therapeutics and delivered to patients.  However, the necessary steps are clear and when completed  these smart digital therapeutics hold promise to advance us toward precision mental health solutions that may reduce both barriers and disparities in the treatment of AUD.

\newpage

# References
::: {#refs}
:::


\newpage
<!--************************************************************************-->
<!-- Table 1: Demographics and clinical characteristics-->

<!--UPDATE -JJC handle description of outliers for quit attempts -->
```{r table_1}
#| output: true


footnote_table_dem_a <- "N = 151"

footnote_table_dem_b <- "Two participants reported 100 or more quit attempts. We removed these outliers prior"

footnote_table_dem_c <- "to calculating the mean (M), standard deviation (SD), and range."

dem  |>  
  bind_rows(auh |>  
              mutate(across(mean:max, ~round(.x, 1))) |>  
              mutate(across(mean:max, ~as.character(.x)))) |>  
  bind_rows(lapse_info |>  
              mutate(across(mean:max, ~round(.x, 1))) |>  
              mutate(across(mean:max, ~as.character(.x)))) |>  
  mutate(range = str_c(min, "-", max)) |> 
  select(-c(min, max)) |>  
  kbl(longtable = TRUE,
      booktabs = TRUE,
      col.names = c("", "N", "%", "M", "SD", "Range"),
      align = c("l", "c", "c", "c", "c", "c"),
      digits = 1,
      caption = "Demographics and clinical characteristics") |> 
  kable_styling(position = "l") |>  
  row_spec(row = 0, align = "c", italic = TRUE) |>  
  column_spec(column = 1, width = "18em") |> 
  pack_rows("Sex", 2, 3, bold = FALSE) |>  
  pack_rows("Race", 4, 8, bold = FALSE) |>  
  pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) |>  
  pack_rows("Education", 11, 16, bold = FALSE) |>  
  pack_rows("Employment", 17, 25, bold = FALSE) |>  
  pack_rows("Marital Status", 27, 31, bold = FALSE) |>  
  pack_rows("Alcohol Use Disorder Milestones", 32, 35, bold = FALSE) |>  
  pack_rows("Lifetime History of Treatment (Can choose more than 1)", 37, 43, bold = FALSE) |>  
  pack_rows("Received Medication for Alcohol Use Disorder", 44, 45, bold = FALSE) |>  
  pack_rows("Current (Past 3 Month) Drug Use", 47, 54, bold = FALSE) |>  
  pack_rows("Reported 1 or More Lapse During Study Period", 55, 56, bold = FALSE) |> 
  kableExtra::footnote(general = c(footnote_table_dem_a, footnote_table_dem_b, footnote_table_dem_c), escape=FALSE)
```


\newpage
