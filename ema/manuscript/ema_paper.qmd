---
title: "Untitled"
format: pdf
csl: https://raw.githubusercontent.com/jjcurtin/lab_support/main/rmd_templates/csl/national-library-of-medicine-grant-proposals.csl
geometry: margin=.5in
fontsize: 11pt
bibliography: paper_ema.bib
---



<!--General notes
Considering American Journal of Psychiatry
https://ajp.psychiatryonline.org/ajp_ifora

ARTICLES
Articles are reports of original work that embodies scientific excellence in psychiatric medicine and advances in clinical research. Typically, articles will contain new data derived from a sizable series of patients or subjects. The text is usually within 3,500 words, which does not include an abstract of no more than 250 words, a maximum of 5 tables and figures (total), and up to 40 references. Word count includes only the main body of text (i.e., not tables, figures, abstracts or references). Additional tables can be submitted in a separate file as supplemental data for posting online. (See Supplemental Data for what types of data and formats are acceptable for posting online.)
-->

```{r knitr_settings, include = FALSE}
# settings
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = FALSE, 
                      message = FALSE)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(fig.pos = "ht", out.extra = "")
```

```{r setup, include = FALSE}
library(knitr)
library(yardstick) # for roc_curve
library(kableExtra)
library(janitor)
library(corx)
library(patchwork)
library(ggtext)
library(consort)
library(tidyverse)
library(tidymodels)
library(tidyposterior)
library(cowplot)

theme_set(theme_classic()) 
```


```{r paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- str_c("P:/studydata/risk/chtc/", study)
          path_processed <- str_c("P:/studydata/risk/data_processed/", study)
          path_models <- str_c("P:/studydata/risk/models/", study)},

        # IOS paths
        Darwin = {
          path_input <- str_c("/Volumes/private/studydata/risk/chtc/", study)
          path_processed <- str_c("/Volumes/private/studydata/risk/data_processed/",
                                  study)
          path_models <- str_c("/Volumes/private/studydata/risk/models/", study)},
        
        # Linux paths
        Linux = {
          path_input <- str_c("~/mnt/private/studydata/risk/chtc/", study)
          path_processed <- str_c("~/mnt/private/studydata/risk/data_processed/",
                                  study)
          path_models <- str_c("~/mnt/private/studydata/risk/models/", study)}
        )
```

```{r load_data}
# Table data
disposition <- read_csv(file.path(path_data_ema, "disposition.csv"), col_types = "ccDDcccccccccc")
screen <- read_csv(file.path(path_data_shared, "screen.csv"), 
                   col_types = vroom::cols()) %>% 
  filter(subid %in% subset(disposition, analysis == "yes")$subid)

# Predictions data
preds_week<- readRDS(file.path(path_models, 
                               "resample_preds_best_all_1week_0_v4_kfold.rds"))
preds_day<- readRDS(file.path(path_models, 
                              "resample_preds_best_all_1day_0_v4_kfold.rds"))
preds_hour<- readRDS(file.path(path_models, 
                               "resample_preds_best_all_1hour_0_v4_kfold.rds")) 

# posterior probabilites
pp <- readRDS(file.path(path_models, "posteriors_all_allwindows_0_v4_kfold.rds"))

# ROC curves
roc_week <- preds_week %>% 
  roc_curve(prob, truth = truth) %>% 
  mutate(model = "1week")

roc_day <- preds_day %>% 
  roc_curve(prob, truth = truth) %>% 
  mutate(model = "1day")

roc_hour <- preds_hour%>% 
  roc_curve(prob, truth = truth) %>% 
  mutate(model = "1hour")

roc_all <- roc_week %>% 
  bind_rows(roc_day) %>% 
  bind_rows(roc_hour)

# PR curves
pr_week <- preds_week %>% 
  pr_curve(prob, truth = truth) %>% 
  mutate(model = "1week")

pr_day <- preds_day %>% 
  pr_curve(prob, truth = truth) %>% 
  mutate(model = "1day")

pr_hour <- preds_hour%>% 
  pr_curve(prob, truth = truth) %>% 
  mutate(model = "1hour")

pr_all <- pr_week %>% 
  bind_rows(pr_day) %>% 
  bind_rows(pr_hour)

# Local and Global Grouped SHAPS
shap_local_week <- readRDS(file.path(path_models, "imp_shap_grouped_all_1week_0_v4.rds")) 
shap_local_day <- readRDS(file.path(path_models, "imp_shap_grouped_all_1day_0_v4.rds"))
shap_local_hour <- readRDS(file.path(path_models, "imp_shap_grouped_all_1hour_0_v4.rds")) 

shap_global_week <- shap_local_week %>% 
  group_by(group) %>% 
  summarize(mean_value = mean(abs(shap)), .groups = "drop") %>% 
  arrange(mean_value)
shap_global_day <- shap_local_day %>% 
  group_by(group) %>% 
  summarize(mean_value = mean(abs(shap)), .groups = "drop") %>% 
  arrange(mean_value)
shap_global_hour <- shap_local_hour %>% 
  group_by(group) %>% 
  summarize(mean_value = mean(abs(shap)), .groups = "drop") %>% 
  arrange(mean_value)
```

# Introduction

<!--GEF: i have added definitions of relapse (a goal-inconsistent return to substance use) and lapse (a single instance of goal-inconsistent substance use) the first time these terms are used per our group discussion. i will say that we seem to be using the terms somewhat interchangeably, which is perhaps ok until we start talking about our study, which should be specifically and exclusively about LAPSE risk. something to monitor in a final read.-->

Alcohol and other substance use disorders (SUDs) are highly prevalent and costly.  In 2019, the National Survey on Drug Use and Health estimated that over 20 million adults in the United States had some form of active substance use disorder within that year [@substanceabuseandmentalhealthservicesadministrationKeySubstanceUse2020]. Nearly 15 million of those adults had an active alcohol use disorder (AUD) [@samhsa2019NationalSurvey; @samhsa2019NationalSurveyb]. Furthermore, a substantial 25.8% of adults reported engaging in hazardous alcohol misuse within the past month [@samhsa2019NationalSurveya]. Alcohol ranks as the third leading preventable cause of death, accounting for approximately 140,000 fatalities per year [@centersfordiseasecontrolandpreventionAlcoholPublicHealth; @esserEstimatedDeathsAttributable2022]. In economic terms, the US Surgeon General disclosed that alcohol misuse cost the United States $249 billion in 2016 alone [@administrationusFacingAddictionAmerica2016].

Existing clinician-delivered treatments for AUD such as cognitive-behavioral therapy [@mchughCognitiveBehavioralTherapySubstance2010; @lieseCognitiveBehavioralTherapyAddictive2022],  mindfulness-based relapse prevention [@bowenMindfulnessBasedRelapsePrevention2021; @goldbergMindfulnessbasedInterventionsPsychiatric2018], motivational interviewing [@millerMotivationalInterviewingHelping2012], and contingency management [@bigelowTheoreticalEmpiricalFoundations1999; @dutraMetaAnalyticReviewPsychosocial2008<!--can cut some refs eventually-->] are effective when provided to patients.  Unfortunately, fewer than 1 in 13 adults with an active AUD in 2019 received any treatment[@samhsa2019NationalSurveyc].  Even more concerning, the failure to access treatment is associated with demographic factors including race, ethnicity, geographic region, and socioeconomic status, which further increase mental health disparities [@wangFailureDelayInitial2005; @MentalHealthReport1999; @generalusMentalHealthCulture2001; @mauraMentalHealthDisparities2017; @novakChangesHealthInsurance2018]. This treatment gap and associated disparities stem from well-known barriers to receiving clinician-delivered mental healthcare related to their affordability, accessibility, availability, and acceptability[@jacobsonUsingDigitalTherapeutics2023].

In recent years, digital therapeutics have emerged as an alternative method to deliver evidence-based treatments and other support to patients either independently or in conjunction with medications or clinician-administered mental healthcare[@jacobsonDigitalTherapeuticsMental2022 <!--Can re-use this reference later rather than chapters if needed to get cites <= 40-->]. Digital therapeutics are web-based or smartphone "apps" that are used to prevent, treat, or manage a medical disorder including AUD or other mental illnesses.  Several large randomized controlled trials have confirmed that digital therapeutics for alcohol or other SUDs improve clinical outcomes (e.g., abstinence, heavy drinking days[@gustafsonSmartphoneApplicationSupport2014; @campbellInternetdeliveredTreatmentSubstance2014]; see Campbell et al. [@campbellFirstWaveScalable2023] for review). They also may enhance treatment retention, reduce readmissions, and support engagement with medication-assisted treatments [@bottsMPOWERProjectResults2017; @japuntichSmokingCessationInternet2006; @pattenRandomizedClinicalTrial2006; @campbellInternetdeliveredTreatmentSubstance2014].

Digital therapeutics can mitigate or eliminate many of the affordability, accessibility, availability, and acceptability barriers associated with in-person, clinician-delivered mental healthcare because they are typically provided to patients on their smartphones[@jacobsonUsingDigitalTherapeutics2023].  Recent Pew Research Center survey data indicate high rates of smartphone ownership among US adults (approximately 85% in April 2021), with minimal variation across race, ethnicity, socioeconomic status, and geographic settings (e.g., urban, suburban, rural) [@pewresearchcenterMobileFactSheet2021]. Moreover, individuals with SUDs generally exhibit high rates of mobile technology use [@collinsFactorsAssociatedPatterns2016]. Consequently, digital therapeutics can offer highly scalable, on-demand therapeutic support that is accessible whenever and wherever it is needed most. 

Despite the documented clinical and other benefits, the full potential of these digital therapeutics has not yet been realized because patients do not use them optimally.  Patients often don't engage with them as developers intended, and long-term engagement may not be sustained or matched to patients' needs [@hatchExpertConsensusSurvey2018; @lattieDigitalMentalHealth2019; @ngUserEngagementMental2019; @yeagerIfWeBuild2018].  The substantial benefits of digital therapeutics come from easy, 24/7 access to their many modules - their treatments, tools, and other support services.  However, this also presents patients with a challenge.  They may be uncertain about when to use the app, which of the numerous modules within the app best suit their needs, and more specifically, which modules are most appropriate for their current situation.

Identifying these most appropriate modules can be difficult because of the dynamic nature of recovery from AUD over time. AUD is a chronic, relapsing disorder [@brandonRelapseRelapsePrevention2007; @witkiewitzModelingComplexityPosttreatment2007; @mclellanDrugDependenceChronic2000]<!--GEF: i moved the final clause of the previous paragraph into the beginning of this paragraph and split the topic sentence into two. didn't move any refs, so may need to move some refs to first sentence if they are about dynamic nature of recovery from AUD rather than AUD being a chronic relapsing disorder -->. Numerous risk and protective factors interact in complex, non-linear ways to influence the probability, timing, and severity of relapse (i.e., a goal-inconsistent return to substance use) [@hendershotRelapsePreventionAddictive2011; @witkiewitzRelapsePreventionAlcohol2004; @huffordRelapseNonlinearDynamic2003; @witkiewitzNonnormalityDivergencePosttreatment2007; @witkiewitzModelingComplexityPosttreatment2007]. Many of these factors are transient, leading to fluctuating relapse risk.  Factors such as urges, mood, lifestyle imbalances, self-efficacy, and motivation can all vary over time.  Social networks may evolve to be more protective or risky, and high risk situations can arise unexpectedly.

Clinical observations and research indicate that successful recovery necessitates life-long monitoring [@hendershotRelapsePreventionAddictive2011; @brandonRelapseRelapsePrevention2007; @huffordRelapseNonlinearDynamic2003; @witkiewitzTherapistGuideEvidenceBased2007; @witkiewitzModelingComplexityPosttreatment2007]. Continuous monitoring of risk for lapse (i.e., a single instance of goal-inconsistent substance use) and its contributing factors, if feasible, would enable patients to adapt their lifestyle, behaviors, and supports to their changing needs. In the context of digital therapeutic use, successful monitoring could direct patients to engage with the most appropriate specific modules in an app, addressing the unique risks present at any given moment throughout their recovery. This guided, adaptive engagement could potentially enhance the app's effectiveness. Thus far, however, the continuous, multi-factor monitoring required for this guidance has proved challenging due to the dynamic and complex interplay of various factors over time. <!--GEF: this final sentence was originally part of the first sentence; i felt it flowed better to talk about benefits and then end with not being able to do it yet since we then move into "but now we can!". didn't move any refs though so check if some from first sentence need to go here -->

<!--relevant review: mohrPersonalSensingUnderstanding2017-->
Moment-by-moment personal sensing of intra- and interpersonal risk factors to support both long-term monitoring and forecasting of mental health functioning is now feasible[@epsteinPredictionStressDrug2020; @suchtingUsingElasticNet2019; @hebertPredictingFirstSmoking2021a; @engelhardPredictingSmokingEvents2018; @businelleUsingIntensiveLongitudinal2016; @soysterPooledPersonspecificMachine2022; @hebertEcologicalMomentaryIntervention2018; @moshontzProspectivePredictionLapses2021; @wyantAcceptabilityPersonalSensing2022; @chihPredictiveModelingAddiction2014; @baeMobilePhoneSensors2018]. Mohr et al. [@mohrPersonalSensingUnderstanding2017] define personal sensing as "collecting and analyzing raw data from sensors embedded in the context of daily life with the aim of identifying human behaviors, thoughts, feelings, and traits".  <!--GEF: need a page number with a direct quotation-->The widespread proliferation of smartphones has made personal sensing both powerful and practical.  Smartphones can be used for ecological momentary assessment (EMA; i.e., repeated, brief self-reports) and also passive, continuous sensing of geolocation, cellular communications (e.g., phone calls and text messages), activity level, sleep, and other raw signals that can predict meaningful clinical outcomes. 

The current project focuses explicitly on the use of EMA to monitor lapse risk for both practical and strategic reasons.  EMA can be easily implemented with only a smartphone; no additional hardware is necessary.  Moreover, comparable raw data (items and responses) can be used consistently across different hardware, operating systems, and sensing apps.  For these reasons, EMA can be incorporated essentially identically into any existing or future smartphone-based digital therapeutic.  EMA can support frequent, in situ longitudinal measurement that will likely be necessary for monitoring episodic or otherwise fluctuating relapse risk.  Long-term monitoring with EMA has been well-tolerated by individuals with SUD [@wyantAcceptabilityPersonalSensing2022; @moshontzProspectivePredictionLapses2021].  Previous research has validated the use of EMA to measure theoretically implicated risk and protective factors for relapse including craving [@dulinSmartphoneBasedMomentaryInterventionCraving2017], mood [@russellAffectRelativeDayLevelDrinking2020], stressors[@wemmDayByDayStressAlcohol], positive life events[@dvorakTensionReductionAffectReg2018], and motivation/efficacy [@dvorakEMAAcuteAUDSymptoms2014].  Furthermore, EMA may provide privileged access to many of these more subjective constructs that may be difficult to quantify reliably through other sensing methods.  

Preliminary research is now emerging that uses EMA as features in machine learning models to predict the probability of future alcohol lapses or related outcomes [<!--Sarah - insert  refs-->].  This research is potentially important because it rigorously establishes temporal ordering between the predictors (features engineered from EMAs) and the outcomes. Additionally, the models are evaluated on data from participants that were not used to train the models, thus prioritizing model generalizability. Both of these criteria are necessary to develop models that can be clinically useful to truly predict future lapses when implemented with new patients rather than simply confirm associations between putative risk factors and alcohol lapses among existing research participants.  

Despite this initial promise, several important gaps in this preliminary research restrict its generalizability and utility.  First, there are limitations regarding the samples used to train and evaluate these models.  Prediction models developed with convenience samples (e.g., college students, non-treament seeking hazardous drinking populations<!--Sarah - insert college student or other convenience samples-->) provide important proof of concept but may not generalize well to clinical samples of patients with AUD.  Similarly, models that have been developed to predict alcohol use in non-treatment seeking patients with AUD may be less clinically useful.  These individuals are unlikely to use digital therapeutics to support their recovery until they begin to contemplate and/or commit to behavior change regarding their alcohol use [<!--JJC to insert stage of change reference-->].  Moreover, features that predict planned or otherwise intentional alcohol use among individuals that are not motivated to change their behavior may not generalize to predict goal-inconsistent alcohol use (i.e., lapses) among patients in recovery.  

<!--Sarah - what about models predicting outcomes other than lapse (e.g., craving, stress)?  What can you tell us about those studies?

Added: Limited to studies that use EMA as a predictor, target SUD related outcomes (some smoking related...), utilized machine learning methods-->

<!--JOHN: review following paragraph re: including non-alcohol studies -->

A second limitation concerns the types of outcomes models are trained to predict. A handful of studies have trained models to predict precursors of substance use, such as craving [@burgess-hullTrajectoriesCravingMedicationassisted2022; @dumortierClassifyingSmokingUrges2016] and stress (Sarker et al., 2016). Although craving and stress are strongly associated with substance use, they do not reliably predict instances of substance use, limiting their utility for identifying specific instances for substance use treatment <!--GEF: probably need citations to make this claim-->. Moreover, due to the increased frequency of craving and stress events relative to substance use episodes, higher model performance can be achieved with less effort [@epsteinPredictionStressDrug2020] <!--GEF: "with less effort"?... is this about outcome class distribution? not quite sure what this means-->. Other studies have predicted temporally coarse clinical outcomes such as binary SUD treatment success [@acionUseMachineLearning2017; @coughlinMachineLearningApproachPredicting2020] or probability of treatment dropout for individuals in a medication assisted treatment program for opioid use disorder [@gottliebMachineLearningPredicting2022]. These prediction models may be useful for identifying individuals who would benefit from increased SUD treatment; however, they do not provide enough temporal specificity to assist in identifying when resources should be directed to these individuals. <!--GEF: Sarah - I did my best to find the references you were citing & add them to Zotero; please double check that I found the studies you were thinking of! I was too uncertain about the Sarker 2016 paper to make a guess on that one.-->

<!--GEF: removed these two sentences from previous & just added citations to the second sentence - maintaining in case this level of detail is desired
For example, EMA has been demonstrated to predict levels of craving in individuals attempting to quit smoking (Durmortier et al 2016) as well as individuals in medication-assisted treatment programs for opioid use disorder (Burguess-Hall et al 2022). EMA combined with physiological data yielded prospective prediction of stressful event episodes in a small sample of individuals endorsing polysubstance use (Sarker et al 2016). 
-->


<!--JJC will justify lapse after we have above paragraph out other outcomes drafted.  - temporally precise, clearly defined, necessary for relapse, often early warning, can produce AVEs which can further undermine recovery. Abstinence is the FDA indicated outcome for clinical trials.  Describe here or in the current study below-->

An early but exemplary prediction model developed by Gustafson et al. [@chihPredictiveModelingAddiction2014] provided the foundation on which our current project builds.  Participants enrolled in their study as they completed a residential treatment program for AUD.  Alcohol lapses were recorded for 8 months following program discharge while they received continuing care through the use of a digital therapeutic for AUD (A-CHESS). As such, the model was developed to predict a clinically meaningful outcome using individuals targeted for digital therapeutic use (i.e., clinical sample committed to AUD recovery-related behavior change).  However, the temporal precision for both the machine learning features and alcohol lapses was coarse because they were measured through a weekly survey.  Therefore, model predictions updated only once per week at best, and lapse onsets could occur anytime within the next two weeks.  This limits the utility of the model for "just-in-time" micro-interventions (e.g., guided mindfulness or other stress reduction techniques, urge surfing) that are well-supported by digital therapeutics but should be used in moments of peak risk. Furthermore, evaluation of this model should be considered both preliminary and optimistically biased because periods with missing surveys were excluded.  

We designed the current project to address these gaps and limitations of previous machine learning prediction models from Gustafson [@chihPredictiveModelingAddiction2014] and others [<!--insert refs from above-->].  We developed our models using participants in early recovery (1-8 weeks since last alcohol use at study enrollment) from moderate to severe AUD who reported a goal of alcohol abstinence.  Consistent with this goal, these models predict future lapses back to alcohol use (i.e., instances of goal-inconsistent alcohol use).  We developed three separate models that provide hour-by-hour probabilities of a future lapse with increasing temporal precision (i.e., lapses in the next week, next day, and next hour, respectively).  Model features were based on raw and longitudinal change in theoretically-implicated risk factors[<!--insert relapse prevention model cites-->] derived from 4x daily EMAs. This research represents an important step toward the development of a "smart" (machine learning guided) sensing system that can both identify periods of peak lapse risk and recommend specific supports to address factors contributing to this risk.

# Method

## Research Transparency
We value the principles of research transparency that are fundamental to the robustness and replicability of science and took several steps to follow emerging open science guidelines [@schonbrodtVoluntaryCommitmentResearch2015]. We reported how we determined our sample size, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]. We completed a transparency checklist (see Supplement; [@aczelConsensusbasedTransparencyChecklist2019]). Finally, we made the data, analysis scripts and annotated results, questionnaires, and other study materials associated with this report publicly available [<!--insert project OSF link-->]. 

Throughout this project, we continued to improve our understanding of statistical methods that remain somewhat novel in our laboratory, and our knowledge of the data increased with each iteration of our analysis plan. These factors made our analyses inherently exploratory and thus inappropriate for preregistration. However, we restricted many researcher degrees of freedom via cross-validation procedures that can robustly guide decision-making. Replication is built into cross-validation: models are fit in an exploratory held-in sample and assessed in a confirmatory held-out sample. Internal analytic decision that improve model fit in the held-in sample are only valuable to the extent that they improve prediction in the held-out sample. 

## Participants
We recruited participants in early recovery (1-8 weeks of abstinence) from AUD in Madison, Wisconsin, USA, to participate in a 3-month longitudinal study. Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required that participants:

1.  were 18 years of age or older,
2.  were able to write and read in English,
3.  had at least moderate AUD (\>= 4 DSM-5 symptoms^[We measured DSM-5 symptoms with a self-report survey administered to participants during the in-person screening visit.]),
4.  were abstinent from alcohol for at least 1 week but no longer than 2 months,
5.  were willing to use a single smartphone (their personal phone or one provided by us) while enrolled in the study.

We also excluded participants if they exhibited severe symptoms of psychosis or paranoia^[Psychosis and paranoia were defined as scores greater than 2.2 or 2.8, respectively, on the psychosis or paranoia scales of the on the Symptom Checklist – 90 (SCL-90) [@derogatisSCL90OutpatientPsychiatric1973].]. 

One hundred ninety-two participants were eligible for enrollment <!--GEF: suggest referring here to consort diagram so readers can track participant flow as they read the next few paragraphs-->. Of these participants, 191 consented to participate in the study at the screening session and 169 subsequently enrolled in the study at the enrollment visit which occurred approximately one week later. Fifteen participants discontinued prior to the first monthly follow-up visit. <!--GEF: i'm left wondering why these people didn't enroll - though not sure if we have that information. would maybe be answered in consort diagram-->

We excluded data from one participant who appeared not to have a goal of abstinence during their participation (i.e., they had lapses every day on study except for one day and reported they were uncertain if their goal was abstinence on the daily EMA and monthly follow-up surveys). We also excluded data from two participants who showed evidence of careless responding (e.g., completing 2-4 EMAs within an hour and providing different responses) and unusually low compliance (e.g., only 5 EMAs completed over one month), rendering their lapse labels unusable. 

Our final sample consisted of 151 participants. Participants provided study measures for one (N = 14), two (N = 6) or three (N = 131) months. 

<!--KW: Discuss if we want any demographic/AUD history/mental health tables or flowchart of participant retention.-->
<!--GEF: see my comment above, i think the flowchart would be helpful! demographic/AUD history/other participant characteristic tables probably most relevant in results-->

## Procedure
Participants completed five study visits over approximately three months. After an initial phone screen, participants attended an in-person screening visit where we determined eligibility, obtained informed consent, and collected self-report measures of individual differences (e.g., demographics, mental health, and alcohol use history; see Measures below). Eligible and consented participants returned approximately one week later to enroll in the study. Three additional follow-up visits occurred about every 30 days participants were on study. At each follow-up visit, we collected additional self-report and interview measures. 

For the entire duration on study, participants were expected to complete EMAs four times each day. Other personal sensing data streams (geolocation, cellular communications, sleep quality, and audio check-ins) were collected as part of the larger grant's aims (R01 AA024391). A full description of the procedure and data collected at each visit can be found at the study's OSF page [<!--Insert link here-->]. All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

## Measures
### EMA
<!--citation for validity of self-reported alcohol use: https://pubmed.ncbi.nlm.nih.gov/26160523/-->
Participants completed a brief (7-10 questions) EMA four times each day following pushed text message reminders. These text messages included a link to a Qualtrics survey, optimized for completion on their smartphone. 

All four EMAs included seven items that asked about alcohol use not yet reported, current affective state (pleasantness and arousal), greatest urge to drink alcohol since the last EMA, any pleasant or positive events, any hassles or stressful events, and any exposure to risky situations (i.e., people, places, or things) that occurred since the last EMA. The first EMA each day asked an additional three questions about how likely participants were to encounter a risky situation, encounter a stressful event, and drink alcohol in the upcoming week. 

The first and last EMAs of the day were scheduled within one hour of participants' typical wake and sleep times. The other two EMAs were each scheduled randomly within the first and second halves of the participants' typical day. All EMAs were separated from each other by at least one hour.

### Individual Differences
At the screening visit we collected self-report information about demographics, mental health, and drug and alcohol use history. These measures are used to describe our sample. Only age, sex, race, education, and marital status are used as features for our analyses. <!--Refer to OSF for full list of measures--> <!--GEF: if you're using them to describe the sample, do they need to be listed/mentioned here? -->

## Data Analytic Strategy
Data preprocessing and modeling were done in RStudio, using the tidyverse and tidymodels ecosystems. <!--KW: Will add version numbers and references-->

### Lapse Labels
We created rolling lapse windows that varied in width (i.e., 1 hour, 24 hours [one day], and 168 hours [one week]). Each window shifted hour by hour for prediction. Each window was labeled as "lapse" or "no lapse." We only included lapse and no lapse windows that we were confident were accurately labeled. 

A valid lapse window must contain a lapse observation. We derived lapse labels from the first item of each EMA ("Have you drank any alcohol that you have not yet reported?"). If participants answered yes to this item, they were prompted to enter the hour and date of the first unreported drink (i.e., lapse onset) and the hour and date of their last drink (i.e., lapse offset). To be labeled as a lapse, the observation must have an hour associated with it, not be in the future, and the onset and offset of lapse must be ordered correctly. 

A valid no lapse window must have all observations labeled as no lapse (i.e., no excluded observations). We used the EMA and other data (e.g., Alcohol Timeline Follow-back) to label no lapse observations. Observations that we could not definitively label as no lapse were excluded from sampling (e.g., occurred within 24 hours of lapse onset, contained lapse reported retrospectively via Alcohol Timeline Follow-back).

### Feature Engineering
Features were calculated from different periods of data (6, 12, 24, 48, 72, and 168 hours prior to observation). Features were derived from EMA questions, demographics (i.e., age, white vs. other race, sex, education, and marital status), history of lapses while on study, and date and time of observation (i.e., evening vs. other hour, and day of week). 

We created features using both raw (e.g., min., max., median, most recent response, and total counts) and change (e.g., within-subject baseline comparisons) scores. Generic (e.g., handling of missing data and zero-variance variables) and algorithmic-specific (e.g., dummy coding and normalization) pre-processing steps were estimated using held in data and applied to held out data (see Model Training procedures below)<!--KW: possibly reference supplemental recipe code here-->.

This process resulted in a total of 267,283 features for 1-hour lapse windows, 274,175 features for 24-hour lapse windows, and 270,077 features for 168-hour lapse windows.

### Model Training and Evaluation
We considered three candidate classification statistical algorithms (elasticnet, random forest, xgboost) that differed across various characteristics expected to affect model performance (e.g., flexibility, ability to handle higher-order interactions natively, complexity, linear vs. non-linear). These algorithms are well-established with documented good "out of box" performance, and they vary with respect to the degree of feature selection performed automatically during model fitting [@kuhnAppliedPredictiveModeling2018]. 
  
Each candidate algorithm was tuned for its associated hyperparameters (i.e., model tuning parameters) and resampled using two methods (up- and downsampling). Resampling approaches are designed to address class imbalances by sampling additional minority class observations (upsampling) or removing majority class observations (downsampling) within held-in data.

We trained all possible model configurations (i.e., combination of algorithm, hyperparameter values, and resampling method) using nested k-fold cross validation. In all varieties of k-fold cross validation, the full dataset is partitioned into k approximately equal-sized, non-overlapping samples referred to as folds. For each model configuration, a model is trained with observations in k - 1 folds. This model is used to make predictions for observations in the kth held-out fold. This process is iterated k times such that each fold is held out once. The model configuration with the best relative mean performance across the k validation sets is selected as the best model configuration. 

In nested cross validation, there are two nested loops for dividing and holding out folds: an outer loop with k<sub>outer</sub> folds, where held-out folds serve as test sets for model evaluation; and inner loops with k<sub>inner</sub> folds, where held-out folds serve as validation sets for model selection. The full dataset is divided into k<sub>outer</sub> approximately equal-sized, non-overlapping folds. Each k<sub>outer</sub> folds serves once as an independent test set to evaluate the model configuration trained and selected using the remaining data (i.e., the k<sub>outer</sub> - 1 outer folds). The model configurations are trained and selected by dividing that remaining data into k<sub>inner</sub> folds and performing k-fold cross validation as described above. Nested cross validation maintains separation between training sets (k<sub>inner</sub> - 1 folds), validation sets (k<sub>inner</sub>th held-out fold), and test sets (k<sub>outer</sub>th held-out fold). This separation offers the significant advantage of reducing optimization bias: the systematic error in estimates of model performance in new data [@jonathanUseCrossvalidationAssess2000]. However, it comes at the cost of dramatically increased computation resources. 

Specifically, we used repeated, nested cross-validation, where the entire process is repeated with multiple data partitions. The inner and outer loops both used 10-fold cross validation and were each repeated 3 times. Cross-validation folds were grouped by participant ID (i.e., all observations from a single participant must be held-in or held-out). 

The best model configuration was selected based on the primary performance metric of interest, area under the receiver operating characteristic curve (AUC ROC). AUC is the probability that the model will predict a higher score for a randomly selected positive case relative to a randomly selected negative case. An AUC of 0.5 indicates chance performance; an AUC of 1.0 perfectly discriminates between positive and negative outcome classes. AUC is a commonly used performance metric that is unaffected by outcome class distribution [@kuhnAppliedPredictiveModeling2018]. 

We evaluated performance of our best model configuration by averaging the AUC ROC across the 10 held-out outer folds. In addition to our primary performance metric, we report the average sensitivity, specificity, balanced accuracy, and positive predictive value (PPV) from all held-out outer folds<!--KW: will cite source for these metrics - tidymodels reference or IAML-->. We also provide the ROC and Precision-Recall (PR) curves. 

We calculated Shapley Additive Explanations (SHAP) scores to provide a global (i.e., across participants) index of feature importance.<!--KW: Not sure best spot for this section yet-->


\newpage

# Results
<!--Information for results: Participants were on study for an average of 85 days out of the possible 90 days. Participants had endorsed using on average 4 other types of drugs (not including alcohol) over their lifetime. Additionally, participants on average scored a 9 on a self-report version of the DSM-5 symptom criteria for alcohol use disorder. Generally, scores of 2-3 are considered mild, 4-5 are considered moderate, and 6+ considered severe alcohol use disorder.-->

<!--Move to results: Across participants there were a total of 1029 unique lapses. There was variation in the frequency of lapses, ranging from 0-75 lapses per participant (M = 6.8, SD = 12.0). Only 56% of participants (N = 84) reported a lapse. However, this was expected since our participants all had a goal of abstinence from alcohol.-->

<!--Citation for ROC cutoffs - https://journals.copmadrid.org/ejpalc/art/ejpalc2018a5 (.58 = small effect size, .69 = medium effect size, .79 = large effect size, corresponding to Cohen's d of .2, .5, .8 respectively).-->

<!--From methods: Participants were mostly white (87%), roughly half were male (51%), and the mean age was 41 years (SD = 12). -->


<!-- Demographics table-->
```{r}
footnote_table_dem_a <- "N = 151"
footnote_table_dem_b <- "Two participants reported 100 or more quit attempts. We removed these outliers prior to calculating the mean (M), standard deviation (SD), and range."
```

```{r table_demo}
options(knitr.kable.NA = "")

dem <- screen %>% 
  summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
            SD = as.character(round(sd(dem_1, na.rm = TRUE), 1)),
            min = as.character(min(dem_1, na.rm = TRUE)),
            max = as.character(max(dem_1, na.rm = TRUE))) %>% 
  mutate(var = "Age",
         n = as.numeric(""),
         perc = as.numeric("")) %>% 
  select(var, n, perc, everything()) %>% 
  full_join(screen %>% 
  select(var = dem_2) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_3) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("American Indian/Alaska Native", "Asian", "Black/African American",
                           "White/Caucasian", "Other/Multiracial")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_4) %>% 
  mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
                         TRUE ~ "Yes"),
         var = fct_relevel(factor(var, c("Yes", "No")))) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_5) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Less than high school or GED degree", "High school or GED", 
                           "Some college", "2-Year degree", "College degree", "Advanced degree")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_6, dem_6_1) %>% 
  mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
                         dem_6_1 == "Part-time" ~ "Employed part-time",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Employed full-time", "Employed part-time", "Full-time student",
                           "Homemaker", "Disabled", "Retired", "Unemployed", 
                           "Temporarily laid off, sick leave, or maternity leave",
                           "Other, not otherwise specified")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  summarise(mean = format(round(mean(dem_7, na.rm = TRUE), 0), big.mark = ","),
            SD = format(round(sd(dem_7, na.rm = TRUE), 0), big.mark = ","),
            min =format(round(min(dem_7, na.rm = TRUE), 0), big.mark = ","),
            max = format(round(max(dem_7, na.rm = TRUE), 0), scientific = FALSE, big.mark = ",")) %>% 
  mutate(var = "Personal Income",
        n = as.numeric(""),
        perc = as.numeric(""),
        mean = str_c("$", as.character(mean)),
        SD = str_c("$", as.character(SD)),
        min = str_c("$", as.character(min)),
        max = as.character(max)) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", "min", "max")) %>% 
  full_join(screen %>% 
  select(var = dem_8) %>% 
  mutate(var = case_when(var == "Never Married" ~ "Never married",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Never married", "Married", "Divorced", "Separated",
                           "Widowed")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))

auh <- screen %>% 
  summarise(mean = mean(auh_1, na.rm = TRUE),
            SD = sd(auh_1, na.rm = TRUE),
            min = min(auh_1, na.rm = TRUE),
            max = max(auh_1, na.rm = TRUE)) %>% 
  mutate(var = "Age of first drink",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()) %>% 
  full_join(screen %>% 
  summarise(mean = mean(auh_2, na.rm = TRUE),
            SD = sd(auh_2, na.rm = TRUE),
            min = min(auh_2, na.rm = TRUE),
            max = max(auh_2, na.rm = TRUE)) %>% 
  mutate(var = "Age of regular drinking",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", 
                                             "min", "max")) %>% 
  full_join(screen %>% 
  summarise(mean = mean(auh_3, na.rm = TRUE),
            SD = sd(auh_3, na.rm = TRUE),
            min = min(auh_3, na.rm = TRUE),
            max = max(auh_3, na.rm = TRUE)) %>% 
  mutate(var = "Age at which drinking became problematic",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  summarise(mean = mean(auh_4, na.rm = TRUE),
            SD = sd(auh_4, na.rm = TRUE),
            min = min(auh_4, na.rm = TRUE),
            max = max(auh_4, na.rm = TRUE)) %>% 
  mutate(var = "Age of first quit attempt",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  # filter out 2 people with 100 and 365 reported quit attempts - will make footnote in table
  filter(auh_5 < 100) %>% 
  summarise(mean = mean(auh_5, na.rm = TRUE),
            SD = sd(auh_5, na.rm = TRUE),
            min = min(auh_5, na.rm = TRUE),
            max = max(auh_5, na.rm = TRUE)) %>% 
  mutate(var = "Number of Quit Attempts*",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  select(var = auh_6_1) %>%
  mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ months)",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_2) %>%
  mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 months)",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_3) %>%
  mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_4) %>%
  mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_5) %>%
  mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_6) %>%
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_7) %>%
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_7) %>% 
  mutate(var = fct_relevel(factor(var, c("Yes", "No")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) %>% 
  rowwise() %>% 
  # calculate dsm5 score by adding up dsm5_1 through dsm5_11
  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7, 
                            dsm5_8, dsm5_9, dsm5_10, dsm5_11))) %>% 
  ungroup() %>% 
  summarise(mean = mean(dsm5_total),
            SD = sd(dsm5_total),
            min = min(dsm5_total, na.rm = TRUE),
            max = max(dsm5_total, na.rm = TRUE)) %>% 
  mutate(var = "Alcohol Use Disorder DSM-5 Symptom Count",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  select(var = assist_2_1) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_2) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Cannabis (marijuana, pot, grass, hash, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_3) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Cocaine (coke, crack, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_4) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_5) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Inhalants (nitrous, glue, petrol, paint thinner, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_6) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_7) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_8) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Opioids (heroin, morphine, methadone, codeine, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) 
```


```{r}
# display and format table
dem %>% 
  bind_rows(auh %>% 
              mutate(across(mean:max, ~round(.x, 1))) %>% 
              mutate(across(mean:max, ~as.character(.x)))) %>% 
  mutate(range = str_c(min, "-", max)) %>%
  select(-c(min, max)) %>% 
  kbl(longtable = TRUE,
      booktabs = TRUE,
      col.names = c("", "N", "%", "M", "SD", "Range"),
      align = c("l", "c", "c", "c", "c", "c"),
      digits = 1,
      caption = "Demographics and Alcohol Use Information") %>%
  kable_styling() %>% 
  row_spec(row = 0, align = "c", italic = TRUE) %>% 
  pack_rows("Sex", 2, 3, bold = FALSE) %>% 
  pack_rows("Race", 4, 8, bold = FALSE) %>% 
  pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) %>% 
  pack_rows("Education", 11, 16, bold = FALSE) %>% 
  pack_rows("Employment", 17, 25, bold = FALSE) %>% 
  pack_rows("Marital Status", 27, 31, bold = FALSE) %>% 
  pack_rows("Alcohol Use Disorder Milestones", 32, 35, bold = FALSE) %>% 
  pack_rows("Lifetime History of Treatment (Can choose more than 1)", 37, 43, bold = FALSE) %>% 
  pack_rows("Received Medication for Alcohol Use Disorder", 44, 45, bold = FALSE) %>% 
  pack_rows("Current (Past 3 Month) Drug Use", 47, 54, bold = FALSE) %>% 
  footnote(general=footnote_table_dem_a, symbol = c(footnote_table_dem_b), 
           threeparttable = TRUE)
```


\newpage

```{r}
footnote_table_perf_metrics <- "Insert footnote"
```

```{r table_metrics}
metrics_week <- preds_week %>%   
  conf_mat(truth, estimate) %>% 
  summary() %>% 
  mutate(.estimate = round(.estimate, 3)) %>% 
  rename(week = .estimate,
         metric = .metric) %>% 
  select(-.estimator)

metrics_day <- preds_day %>%   
  conf_mat(truth, estimate) %>% 
  summary() %>% 
  mutate(.estimate = round(.estimate, 3)) %>% 
  rename(day = .estimate,
         metric = .metric) %>% 
  select(-.estimator)

metrics_hour <- preds_hour %>%   
  conf_mat(truth, estimate) %>% 
  summary() %>% 
  mutate(.estimate = round(.estimate, 3)) %>% 
  rename(hour = .estimate,
         metric = .metric) %>% 
  select(-.estimator)

metrics <- metrics_week %>% 
  full_join(metrics_day, by = "metric") %>% 
  full_join(metrics_hour, by = "metric") %>% 
  filter(metric %in% c("accuracy", "sens", "spec", "ppv", "npv"))

auc <- tibble(metric = "auc", 
              week = preds_week %>% roc_auc(prob, truth = truth) %>%  
                pull(.estimate) %>% round(3), 
              day = preds_day %>% roc_auc(prob, truth = truth) %>%  
                pull(.estimate) %>% round(3),
              hour = preds_hour %>% roc_auc(prob, truth = truth) %>%  
                pull(.estimate) %>% round(3))

metrics <- metrics %>% 
  bind_rows(auc)

metrics <- metrics[c(6,1,2,3,4,5),]

metrics %>%
 kbl(col.names = c("Metric", "Week", "Day", "Hour"),
      booktabs = TRUE,
      digits = 3,
      align = c("l", "c", "c", "c"),
      caption = "Performance Metrics by Model",
     linesep = "") %>% 
  row_spec(row = 0, align = "c") %>% 
  kable_styling(position = "left") %>% 
  footnote(general=footnote_table_perf_metrics)
```

\clearpage




<!-- AUC figure by model w/posterior probabilities-->  
```{r caption_roc_pp}
fig_caption_roc_pp <- "Insert note here"
```

```{r fig_roc_pp, fig.cap = fig_caption_roc_pp, fig.height = 4.5, fig.width = 7}

roc_plot <- roc_all %>% 
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"), 
                        labels = c("Week", "Day", "Hour"))) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path(show.legend = FALSE) +
  geom_abline(lty = 3) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Specificity",
       y = "Sensitivity") +
  scale_x_continuous(breaks = seq(0,1,.25),
    labels = sprintf("%.2f", seq(1,0,-.25))) 

pp_tidy <- pp %>% 
  tidy(seed = 123)

ci <- pp_tidy %>% 
  summary() %>% 
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour")),
         y = 1000)

pp_plot <- pp_tidy %>% 
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour"))) %>%
  ggplot() + 
  geom_histogram(aes(x = posterior, fill = model), color = "black", alpha = .4, 
                 bins = 30) +
  geom_segment(mapping = aes(y = y+100, yend = y-100, x = mean, xend = mean,
                           color = model),
               show.legend = FALSE,
               data = ci) +
  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, color = model),
              show.legend = FALSE,
               data = ci) +
  geom_text(data = ci, x = c(.93, .907, .92), y = 1000, 
            label = str_c(round(ci$mean, 2), " [", round(ci$lower, 2), ", ", round(ci$upper, 2), "]")) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous("Posterior Probability", breaks = c(0, 500, 1000)) +
  xlab("Area Under ROC Curve") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank())

roc_plot + pp_plot +
  plot_layout (guides = "collect") &
  theme(legend.position = "bottom")
```




<!-- PR-->
<!-- get exact sensitivity at .75 PPV -->
```{r}
pr_.75_cutoff <- pr_all %>% 
  mutate(recall = round(recall, 3),
         precision = round(precision, 3),
         .threshold = round(.threshold, 3),
         model = factor(model,
                        levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) %>% 
  filter(precision == .75) %>% 
  group_by(model, precision) %>% 
  summarise(recall = mean(recall),
            threshold = mean(.threshold),
            .groups = "drop")
```

<!-- Week sensitivity at .75 PPV =  `r round(pull(subset(pr_.75_cutoff, model == "week"), recall), 3)` -->

<!-- Day sensitivity at .75 PPV =  `r round(pull(subset(pr_.75_cutoff, model == "day"), recall), 3)` -->

<!-- Hour sensitivity at .75 PPV =  `r round(pull(subset(pr_.75_cutoff, model == "hour"), recall), 3)` -->



<!-- SHAP Importance figure-->
```{r caption_shap}
fig_caption_shapgrouped <- "Variable Importance (SHAP Values) for each Model.  Raw EMA features are grouped by the original item from the EMA. Features from demographics and the day and hour for the start of the  prediction window are also included."
```

```{r fig_shap, fig.cap = fig_caption_shapgrouped, fig.height=7}
shap_global_all <- shap_global_week %>% 
  mutate(window = "Week") %>% 
  bind_rows(shap_global_day %>% 
              mutate(window = "Day")) %>% 
  bind_rows(shap_global_hour %>% 
              mutate(window = "Hour")) %>% 
  mutate(window = factor(window, levels = c("Week", "Day", "Hour"))) %>% 
  mutate(group = factor(group, levels = c("past use (EMA item)", 
                                          "craving (EMA item)", 
                                          "past risky situation (EMA item)", 
                                          "past stressful event (EMA item)", 
                                          "past pleasant event (EMA item)", 
                                          "valence (EMA item)", 
                                          "arousal (EMA item)", 
                                          "future risky situation (EMA item)", 
                                          "future stressful event (EMA item)", 
                                          "future efficacy (EMA item)",
                                          "lapse day (other)",
                                          "lapse hour (other)",
                                          "missing surveys (other)",
                                          "age (demographic)",
                                          "sex (demographic)",
                                          "race (demographic)",
                                          "marital (demographic)",
                                          "education (demographic)")))

shap_grouped_all %>% 
  mutate(group = reorder(group, mean_value, sum)) %>% 
  ggplot() +
  geom_bar(aes(x = group, y = mean_value, fill = window), stat = "identity", alpha = .4) +
  ylab("Mean |SHAP| value") +
  xlab("") +
  coord_flip()
```

<!-- Local SHAP values -->
```{r caption_shap_local}
fig_caption_shap_local <- "Insert Note here."
```

```{r fig_shap_local, fig.cap = fig_caption_shap_local, fig.width = 10, fig.height = 7, out.extra='angle=90'}

shap_levels <- shap_global_all |> 
  mutate(group = reorder(group, mean_value, sum)) |> 
  pull(group) |> 
  levels()

# downsample to 2% of observations for each plot (COULD INCREASE?)
ids_week <- shap_local_week |> 
  pull(ID) |> 
  unique()
ids_week <- ids_week |> sample(size = round(length(ids_week)/50))
ids_day <- shap_local_day |> 
  pull(ID) |> 
  unique()
ids_day <- ids_day |> sample(size = round(length(ids_day)/50))
ids_hour <- shap_local_hour |> 
  pull(ID) |> 
  unique()
ids_hour <- ids_hour |> sample(size = round(length(ids_hour)/50))

plot_shap_week <- shap_local_week |> 
  filter(ID %in% ids_week) |> 
  mutate(group = factor(group, levels = shap_levels)) |> 
  ggplot(mapping = aes(x = group, y = shap)) +
  ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4, 
                     color = "orange") +
  geom_hline(yintercept = 0) +
  geom_text(data = shap_global_all |> filter(window == "week"),
            aes(x = group, y = -1.75, label = round(mean_value, 2)),
            color = "orange") +
  scale_y_continuous(limits = c(-2, 4.5), breaks = seq(-2, 4)) +
  ylab("SHAP value") +
  xlab("") +
  coord_flip() 

plot_shap_day <- shap_local_day |> 
  filter(ID %in% ids_day) |> 
  mutate(group = factor(group, levels = shap_levels)) |> 
  ggplot(mapping = aes(x = group, y = shap)) +
  ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4, 
                     color = "green") +
  geom_hline(yintercept = 0) +
  geom_text(data = shap_global_all |> filter(window == "day"),
            aes(x = group, y = -2, label = round(mean_value, 2)),
            color = "green") +
  scale_y_continuous(limits = c(-2, 4.5), breaks = seq(-2, 4)) +
  ylab("SHAP value") +
  xlab("") +
  coord_flip() 

plot_shap_hour <- shap_local_hour |> 
  filter(ID %in% ids_hour) |> 
  mutate(group = factor(group, levels = shap_levels)) |> 
  ggplot(mapping = aes(x = group, y = shap)) +
  ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4, 
                     color = "blue") +
  geom_hline(yintercept = 0) +
  geom_text(data = shap_global_all |> filter(window == "hour"),
            aes(x = group, y = -1.75, label = round(mean_value, 2)),
            color = "black") +
  scale_y_continuous(limits = c(-2, 4.5), breaks = seq(-2, 4)) +
  ylab("SHAP value") +
  xlab("") +
  coord_flip() 

plot_shap_week + plot_shap_day + plot_shap_hour
```


# Discussion

## 1. Model Performance, SHAP, and Model Comparisons

<!-- Overall model performance -->
<!-- * Talk about solid overall performance of three models
- ROC, sensitivity and specificity of three models -->
Models across all three time windows performed exceptionally well, yielding AUC ROCs of [], [], and [] for hour level, day level, and week level models respectively. AUCs above .9 are generally described as having "excellent" performance (cite), and all three of our models perform as well as or better than existing alcohol lapse prediction models to date (Berenholtz et al,  2020). This indicates EMA data can be used to predict alcohol lapse in the next hour, next day, and next week with high levels of sensitivity and specificity.

<!-- - Likely hit a ceiling with model performance. Can talk about low difference between inner and outer fold performance as potential support that we reached a ceiling/arent capitalizing on noise --> 

* Likely result of benefit of using nonlinear algorithms, time baked into features, etc, discussed in methods. High dimensional data set, longitiudinal measurement. BUT our method of model selection did not include comparison to lower dimensional feature sets, consideration of other algorithms. So conclusion cannot be that this is more superior. But conclusion is our stated performance. 
* Reminder that these are held out performance metrics in new people -- i.e. grouped nested kfold cv. So we expect these high performance estimates to generalize


SHAP
* Describe enough so people know what it is
* Difference between global and local importance
* Even features with low overall global importance can be significant if they have high local importance -- indicating these things can be very important for some people at some times
*Demographic variables not really important, but conclusions are dependent on representativeness of our sample. Despite having wide representation of SES, gender, and age, these did not typically emerge in SHAPs. While race/ethnicity did not emerge, we had a limimted representation of black and brown populations so we should be cautious of drawing conclusion of the lack of race as a highly predictive feature.


Model comparisons
* Reasons why hour level models are the best -- closest to lapse event, inclusion of temporally based features (day and hour)
* While sensitivity, specificity, and balanced accuracy are encouragingly high, PPV is low
    * PPV and NPC are significantly effected by outcome distribution, often not reported in ML models.
    * Adjusting the decision threshold can increase PPV (at cost of reducing sensitivity)
    * Some interventions are more appropriate for low PPV, high sensitivity scenarioes (e.g. false alarms are not necessarily bad if you are just encouraging increased use of the dtx). Higher cost interventions (e.g. contacting a provider) may be better suited for higher decision thresholds to increase PPV



## 2. Clinical implementation


There are three ways a model like ours might be clinically implemented. 

First, it might be used to communicate a patient's lapse risk to their treatment provider (e.g., therapist, primary care provider). This might look like a clinician receiving notifications about which patients are at a high risk of lapsing at the start of each shift. They could then decide if they wanted to connect with a patient or use this information to triage their availability (i.e., which patients should be seen first). While this information would likely be helpful to the clinician, it hinders much of the benefit digital therapeutics have to offer. It reduces the scalability <!--or accessibility?--> by placing an additional load on the provider (monitoring, evaluating, and taking action based on lapse risk scores for a high number of patients). It shifts the availability of treatment from being at the individuals moment of greatest need to a moment of high risk and when a treatment provider is available. Finally, it removes the benefits of acceptability in that it requires patients to share information related to their lapse risk with providers <!-- elaborate on issues/citations re: trust?-->.    

Second, it might be used to communicate lapse risk directly to the individual. This removes the treatment provider as a gatekeeper of the risk information. As a result, individuals can be alerted to their lapse risk at anytime and at any threshold (i.e., not just at a threshold warranting clinician intervention). Additionally, this may allow individuals to feel they have more control over their data and alleviate potential concerns about the documentation and unauthorized use of their lapse risk information. Unfortunately, this information on its own may not be helpful to individuals monitoring their lapse risk and could even have unintended harmful consequences. One, it requires an individual to know what action to take, or what module to use in a digital therapeutic, in response to the lapse risk score. Second, being alerted to a high risk of lapsing, without any accompanying intervention, could result in a self-fulfilling prophecy due to feelings of loss of control, failure, or associating their lapse risk with internal, stable, and global factors (e.g., abstinence violation effect).     

Third, it might be used to recommend an action that an individual could take to reduce their risk of a lapse. <!--consider introducing JITAI term here--> In this situation, it could tailor treatment recommendations based on top features contributing to their lapse risk (e.g., recommending an urge surfing activity in response to reported high cravings). Recommendations could be mapped onto existing therapeutic frameworks shown to be effective for alcohol use disorder (e.g., CBT, mindfulness-based relapse). A recommendation-guided digital therapeutic would reduce the risk of iatrogenic effects since suggested actions based on predicted risk would likely be helpful and positive regardless of one's actual risk. It would also share benefits of existing self-guided digital therapeutics (e.g., reaching people not connected with a treatment provider). <!--connect back to issues of affordability, accessibility, acceptability, and availability?-->

<!-- other possible things to discuss here
- recommendation algorithm likely can be adapted for other recovery goals (e.g., moderation) - needs to be tested first though.

Potential drawbacks:
- missing feedback on effectiveness/helpfulness of recommendations - this can be incorporated into the recommendation system
- not intended to replace traditional avenues of treatment
-->


Additional considerations for tailoring a recommendation model include the decision threshold, prediction window (e.g., hour, day, week), and lag time. 

Decision threshold
- Adjusting the decision threshold can increase PPV (at cost of reducing sensitivity)
- different thresholds depending on overall costs of recommendation
<!-- Some interventions are more appropriate for low PPV, high sensitivity scenarioes (e.g. false alarms are not necessarily bad if you are just encouraging increased use of the dtx). Higher cost interventions (e.g. contacting a provider) may be better suited for higher decision thresholds to increase PPV -->

Prediction window
- broader windows have higher PPV
- windows could offer some guidance for recommendations, but likely have limited value on their own (see lag time below)  

Lag time
- Our model had a lag time of 0. 
<!--This means week level model could predict a lapse in the next week, or the next hour. (Potential problems for recommending a treatment that takes a lot of time if lapse can happen sooner)-->
- interacts with prediction window for intervention recommendation
   * narrow windows (e.g., next hour) with no lag - immediate interventions/recommendations  
   * broader windows (e.g., next week) can be used as well but not as useful for in the moment  
   * lagged broad windows (e.g., next week starting one week from now) could be used for incorporating interventions that may take longer.




## 3. Remaining future directions/limitations

- KW:


- SS:











Outline

<!-- Summary of model performance - Our models perform well and this is why -->

*1. The performance of our models across all three prediction windows is higher than any published literature to date.*
  * Break down meanings of sensitivity, specificity, AUC
    * We are doing really well...Likely have reached a performance ceiling (due to irreducible error)
  * Week and hour models perform comparably, Day level model is a significant improvement in classifier performance (via Baysian anlaysis of posteriors) 
  * Highlight how we build upon previous research -- large N, treatment seeking sample, temporally precise lapse outcome, grouped resampling

<!--JJC: its the hour level model that is the best but otherwise, this is good.   Comment on why the hour level model is likely superior? though perhaps we need to have discussed SHAP to do that?-->

*2. Our top performing model was an xgboost, which incorporates multiple elements that likely contribute to our succesful prediction.*
  * Boosted tree models can handle non linear interactive effects natively within the algorithm
  * We trained configurations up to tree depths of 4, allowing 4 way interactions among features
  * This allows for incorporation of within subject variation to make precise predictions. We build time into our features with change scores, most recent features, etc. With the ability to consider 4 way interactions with these types of features, we can identify highly complex scenarios. Example: we can consider how past use ratings interact with current future efficacy ratings immediately following a lapse episode (You might expect lower future efficacy right after a lapse episode)
  * XGBoost is able to handle highly correlated features (which is expected for this data)

<!--KW: I don't think we need to a whole paragraph on xgboost in discussion but it might be good to bring out some of these points in other areas. For example, our intro puts a lot of focus on the dynamic nature of lapse risk. We could use bullet point 3 to emphasize this point (see below).

2. Ability to capture individual fluctuations in lapse risk
  * XGBoost contribution....This allows for incorporation of within subject variation to make precise predictions. We build time into our features with change scores, most recent features, etc. With the ability to consider 4 way interactions with these types of features, we can identify highly complex scenarios. Example: we can consider how past use ratings interact with current future efficacy ratings immediately following a lapse episode (You might expect lower future efficacy right after a lapse episode)
-->

<!-- JJC: Agree with kendra that we try to keep the machine learning in the background and the model and its performance, uses, limtations etc in the foreground.   No need to focus on XGBoost-->


  
*3. SHAP values provide interpretability as to why our models are performing well*
  	* Emerging standard for interpretability
    * Additive decomposition of the contributions that each of these features make to the prediction for each observation 
    *Difference between global versus local Shaps
    *Highest SHAPs are generally in agreement with relapse prevention model.
    
<!-- JJC: Again, I dont want to confuse the reader with a focus on SHAP other than enough to understand what is needed to intepret the results.   We do need them to know the difference between global vs local b/c we will want to make clear that even features that arent generally influential (i.e., high global SHAP) are importnat for some participants at some times - local SHAP.  We will need to talk about the top features in a bit more detail.  We need to comment on the demos not having much/any impact -->

<!-- Considerations for Clinical Application -->
*4. It is important to consider multiple aspects of model performance when identifying the clinical utility of prediction models.*
    * While sensitivity, specificity, and balanced accuracy are encouragingly high, PPV is low
    * PPV and NPC are significantly effected by outcome distribution, often not reported in ML models.
    * Adjusting the decision threshold can increase PPV (at cost of reducing sensitivity)
    * Some interventions are more appropriate for low PPV, high sensitivity scenarioes (e.g. false alarms are not necessarily bad if you are just encouraging increased use of the dtx). Higher cost interventions (e.g. contacting a provider) may be better suited for higher decision thresholds to increase PPV

Decision thresholds we choose will impact the types of feedback participants are given.
    * Potential risks of reporting binary lapse/no lapse, versus lapse probability, versus no report (just intervention)
    * Literature on impact of false predictions on trust in ML recommendations

*5. Three prediction windows allows for increased personalization of treatment recommendations.*
  *Different treatments may be more appropriate/feasible when considering risk of lapse in next hour, next day, next week.
  *e.g. urge surfing for lapse risk in the next hour, scheduling appointment with provider if elevated risk in next week <!--KW: To be even more helpful we would want to be able to predict with different lag times as well.-->
  
  <!-- this should go earlier when we first talk about the three models and their performance.  Very important points.  I agree with KW about the need for lags too.  Not sure if that is raised here as a future direction or saved for a later future directions section-->

*6. Local importance scores can provide information about what treatments to recommend during high risk periods.*
  * Explain how features map to specific observation for specific individual
  * Across observations, generally all features become important at some time
  * Many of our top local SHAPs align with Marlatt's relapse prevention model -- align nicely with modules you would expect to see in AUD treatment app
  
<!-- JJC:  Good, this can follow SHAP.  Need to consider how it should be ordered relative to the ideas about lag that KW raised.  I think it may be able to be kept separate.  -->
  
<!-- Limitations and future directions -->

*7. Certain aspects of our study design may limit generalizability.*
  * Results only applicable to individuals with AUD with an abstinence only goal. Future work can incorporate harm reduction approaches to identify goal-inconsistent drinking behavior that is not limited to abstinence only. 
  
  <!--KW: also mention our sample is in early recovery and our study is 3 months long. It would be important to also look at people stable in their recovery as well if we wanted to use our algorithm for long-term monitoring. -->

<!--JJC: not really high N. High on observations but still only N=151, which is just adequate.  I might just avoid this topic.  We DO need to talk about the other participant characteristics for which we did have variability and for which we didnt.   Race/ethniticy, geography, and stage of recovery seem the most important to highlight-->

  * Current sample was high N, but limited due to Madison based and low diversity sample. Importance of diverse training sets to offset racial bias in ML models
  * Data collection currently underway to recruit a national, more diverse sample, with OUD
  
*8. Important considerations for implementation*
  * Lapse prediction windows currently have a lead time of 0. This means week level model could predict a lapse in the next week, or the next hour.
      * Potential problems for recommending a treatment that takes a lot of time if lapse can happen sooner.
      * Utilization all three prediction windows concurrently and recommend treatments based on shortest window
  * Current models built on active sensing via EMA, which may increase burden.
    * EMA requires more effort on part of participant, some uncertaintly about how long response rate can be maintained for
    
    <!--JJC:  Need to consider where point 8 is raised.   It might have come up earlier but maybe lag is saved until here.   Need to consider flow-->
    
<!--KW: Future direction might be to see if it is possible to predict as well with a one time daily EMA-->
<!--JJC: Agree with Kendra that we need to have a limitation/direction about burden.   Can cite burden and say well tolerated.   But can talk about potential difficulty with longer term use.  Can talk about ways to address which include 1x daily and added passive signals like GPS-->

    * Current project also collected gps, cellular communications with context, and other passive measures. We will be able to see if we get comparable performance passively

<!--KW: Another consideration for considering different risk windows/implementation is the user experience (e.g., It might be alarming to get a notification that you will lapse in the next hour)-->

<!-- JJC:  We do need a section where we talk about what we do with these signals.   This should follow the PPV so we can acknowledge those issues.   Can talk about feedback to clinicians but highglight they may want it.  Can talk about risk feedback to patients but need to consider what info is provided.  Can talk about use to recommend IRL and DTx treatments.  
  
    





\clearpage
# References