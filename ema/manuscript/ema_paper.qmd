---
title: "Machine learning models for temporally precise lapse prediction in alcohol use disorder"
blank-lines-above-title: 2
shorttitle: Machine learning for lapse prediction
author:
  - name: Kendra Wyant*
    # role:
    #   - Conceptualization
    #   - Data curation
    #   - Formal analysis
    #   - Writing – original draft
    #   - Writing – review & editing
    affiliations:
      - id: id1
        name: University of Wisconsin-Madison
        department: Department of Psychology
        address: 1202 W Johnson St
        city: Madison
        region: WI
        postal-code: "53521"
  - name: Sarah J. Sant’Ana*
    # role:
    #   - Conceptualization
    #   - Data curation
    #   - Investigation
    #   - Writing – original draft
    #   - Writing – review & editing
    affiliations: 
      - ref: id1
  - name: Gaylen E. Fronk
    # role:
    #   - Conceptualization
    #   - Writing – original draft
    #   - Writing – review & editing
    affiliations: 
      - ref: id1
  - name: John J. Curtin
    corresponding: true
    email: jjcurtin@wisc.edu
    url: https://arc.psych.wisc.edu/
    # role:
    #   - Conceptualization
    #   - Data curation
    #   - Formal analysis
    #   - Funding acquisition
    #   - Investigation
    #   - Supervision
    #   - Writing – original draft
    #   - Writing – review & editing
    affiliations: 
      - ref: id1
author-note:
  blank-lines-above-author-note: 1
  status-changes: 
    affiliation-change: "*These authors contributed equally as co-first authors."
  # Disclosures condensed to one paragraph, but you can start a field with two line breaks to break them up: \n\nNew Paragraph
  disclosures:
    study-registration: ~
    data-sharing: "All data and materials have been made publicly available on Open Science Framework and can be accessed at [https://osf.io/w5h9y/](https://osf.io/w5h9y/)."   
    related-report: "All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study # 2015-0780)."
    financial-support: "This research was supported by grants from the National Institute on Alcohol Abuse and Alcoholism (R01 AA024391; JJC) and the National Institute on Drug Abuse (R01 DA047315; JJC)."
    gratitude: "The authors wish to thank Susan E. Wanta for her role as the project administrator and for her help with data curation. The authors also wish to thank Candace Lightheart, Jill Nagler, Kerry Keiser, and Megan Shultz for their contributions to data collection and Chris Gioia for the clinical supervision he provided to graduate students."
    authorship-agreements: ~
abstract: "We developed three separate models that provide hour-by-hour probabilities of a future lapse back to alcohol use with increasing temporal precision (i.e., lapses in the next week, next day, and next hour). Model features were based on raw scores and longitudinal change in theoretically implicated risk factors collected through ecological momentary assessment (EMA). Participants (*N*=151; 51% male; mean age = 41; 87% White, 97% Non-Hispanic) in early recovery (1–8 weeks of abstinence) from alcohol use disorder provided 4x daily EMA for up to three months. We used grouped, nested cross-validation, with 1 repeat of 10-fold cross-validation for the inner loop and 3 repeats of 10-fold cross-validation for the outer loop to train models, select best models, and evaluate those best models on auROC. Models yielded median areas under the receiver operating curves (auROCs) of .90, .91, and .94 in the 30 held-out test sets for week, day, and hour level models, respectively. Some feature categories consistently emerged as being globally important to lapse prediction across our week, day, and hour level models (i.e., past use, future efficacy). However, most of the more punctuate, time varying constructs (e.g., craving, past stressful events, arousal) appear to have greater impact within the next hour prediction model. This research represents an important step toward the development of a *smart* (machine learning guided) sensing system that can both identify periods of peak lapse risk and recommend specific supports to address factors contributing to this risk. \n\nGeneral scientific summary: This study suggests that densely sampled self-report data can be used to predict lapses back to alcohol use with varying degrees of temporal precision. Additionally, the contextual features contributing to risk of lapse may offer important insight for treatment matching through a digital therapeutic."
keywords: [ecological momentary assessment, digital therapeutics, alcohol use disorder]
bibliography: paper_ema.bib
format:
  apaquarto-pdf:
    documentmode: man
    fontsize: 12pt
    floatsintext: false
---


{{< include _extensions/wjschne/apaquarto/_apa_title.qmd >}}


<!-- total words: 11416
- Journal metadata (title page, author notes, disclosures, abstract, general scientific summary) = 496
- intro = 1488
- Method (including 2 footnotes) = 2965 (still need to add statement about power)
- Results (text only) = 1300
- Discussion = 2286
- References (currently 65 references, a few more may be cut depending on what gets moved to supplement from methods) = 2133
- Tables = 493 (still need to add note about lapse outliers to Table 1)
- Fig captions = 255
-->




<!--required author notes (already entered into metadata):

All data and materials have been made publicly available on Open Science Framework and can be accessed at [https://osf.io/w5h9y/](https://osf.io/w5h9y/).

All procedures were approved by the University of Wisconsin-Madison Institutional Review Board (Study # 2015-0780).
-->


<!--Still need to add figure captions and check all figures are rendering correctly

JOHN: Since we are submitting word version. I think I am going to save out the final figures and import them into the word doc just to have better control over them.-->



<!--Target Journal
Journal of Psychopathology and Clinical Science
https://www.apa.org/pubs/journals/abn 

ARTICLES
The manuscript should not exceed 9,000 words when including the abstract, body of the text, tables, table captions, figure captions, footnotes, author notes, appendices, and references in a word count.
-->

<!--terminology
Prediction window, window width, week, day, or hour model (always in that order)
relapse (a goal-inconsistent return to harmful substance use) and lapse (a single instance of goal-inconsistent substance use) 
-->

<!--Notes from lab meeting

Intro:
- language around AUD/SUD/psychiatric disorders - DONE by JJC
- reduced focus on digital therapeutics/increased focus on personal sensing & prediction algorithm/changed headings - PARTIALLY DONE BY JJC
- add information about specific features in “current study” and how they map onto relapse prevention literature -TBD; JC MARKED LOCATION FOR A FEW SENTENCES.

Methods:
- improve terminology and definitions of things like test set, validation set, etc - so that we can just use simpler language throughout

Results:
- shorten?/simplify?
- remove “methods” sentences from results
- determine how to “answer question” (how do we know if our AUC is good, did we answer the question about “can we predict lapses”/“how accurately can we predict lapses”)

Discussion:
-add to limitations/future directions: no existing method to say whether an AUC is clinically useful or whether a SHAP value (global value or local variability) is meaningful
-->

```{r knitr_settings}
options(knitr.kable.NA = '')
knitr::opts_chunk$set(fig.pos = "ht", out.extra = "", message = FALSE)
```


```{r setup}
library(knitr)
# library(yardstick) # for roc_curve
library(kableExtra)
library(janitor)
# library(corx)
library(patchwork)
library(ggtext)
library(consort)
library(tidyverse)
library(tidymodels)
library(tidyposterior)
library(cowplot)

theme_set(theme_classic()) 
```


```{r paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- "P:/studydata/risk/chtc/ema"
          path_processed <- "P:/studydata/risk/data_processed/ema"
          path_models <- "P:/studydata/risk/models/ema"
          path_shared <- "P:/studydata/risk/data_processed/shared"
          path_manuscript <- "P:/studydata/risk/manuscripts/EMA"},

        # IOS paths
        Darwin = {
          path_input <- "/Volumes/private/studydata/risk/chtc/ema"
          path_processed <- "/Volumes/private/studydata/risk/data_processed/ema"
          path_models <- "/Volumes/private/studydata/risk/models/ema"
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"
          path_manuscript <- "/Volumes/private/studydata/risk/manuscripts/EMA"},
        
        # Linux paths
        Linux = {
          path_input <- "~/mnt/private/studydata/risk/chtc/ema"
          path_processed <- "~/mnt/private/studydata/risk/data_processed/ema"
          path_models <- "~/mnt/private/studydata/risk/models/ema"
          path_shared <- "~/mnt/private/studydata/risk/data_processed/shared"
          path_manuscript <- "~/mnt/private/studydata/risk/manuscripts/EMA"}
        )
```


```{r load_data}
# For table 1
disposition <- read_csv(file.path(path_processed, "disposition.csv"), 
                        col_types = "ccDDcccccccccc")
# For table 1
screen <- read_csv(file.path(path_shared, "screen.csv"), 
                   col_types = cols()) |>
  filter(subid %in% subset(disposition, analysis == "yes")$subid)

lapses <- read_csv(file.path(path_shared, "lapses.csv"), col_types = cols()) |>
  filter(exclude == FALSE)

# lapse labels
labels_week <- read_csv(file.path(path_processed, "labels_1week.csv"), col_types = cols())
labels_day <- read_csv(file.path(path_processed, "labels_1day.csv"), col_types = cols())
labels_hour <- read_csv(file.path(path_processed, "labels_1hour.csv"), col_types = cols())


# Predictions data
preds_week<- read_rds(file.path(path_models, "outer_preds_1week_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)
preds_day<- read_rds(file.path(path_models, "outer_preds_1day_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)
preds_hour<- read_rds(file.path(path_models, "outer_preds_1hour_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)

auc_week <- read_rds(file.path(path_models, "outer_metrics_1week_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))
auc_day <- read_rds(file.path(path_models, "outer_metrics_1day_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))
auc_hour <- read_rds(file.path(path_models, "outer_metrics_1hour_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))

# ROC curves
roc_week <- preds_week |>  
  roc_curve(prob, truth = label) |>  
  mutate(model = "1week")

roc_day <- preds_day |>  
  roc_curve(prob, truth = label) |>  
  mutate(model = "1day")

roc_hour <- preds_hour|>  
  roc_curve(prob, truth = label) |>  
  mutate(model = "1hour")

roc_all <- roc_week |>  
  bind_rows(roc_day) |>  
  bind_rows(roc_hour)

# PR curves
pr_week <- preds_week |>  
  pr_curve(prob, truth = label) |>  
  mutate(model = "1week")

pr_day <- preds_day |>  
  pr_curve(prob, truth = label) |>  
  mutate(model = "1day")

pr_hour <- preds_hour|>  
  pr_curve(prob, truth = label) |>  
  mutate(model = "1hour")

pr_all <- pr_week |>  
  bind_rows(pr_day) |>  
  bind_rows(pr_hour)


# posterior probabilities
pp <- read_rds(file.path(path_models, "posteriors_all_0_v5_nested.rds"))

pp_tidy <- pp |>  
  tidy(seed = 123)

q = c(.025, .5, .975)
ci <- pp_tidy |>  
  group_by(model) |>  
  summarize(median = quantile(posterior, probs = q[2]),
            lower = quantile(posterior, probs = q[1]), 
            upper = quantile(posterior, probs = q[3])) |>  
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour")),
         y = 1000) |> 
  arrange(model)

pp_diffs <- pp |>  
  contrast_models(list("hour","hour", "day"), 
                list("week", "day", "week"))

ci_diffs <- pp_diffs |> 
  group_by(contrast) |>  
  summarize(median = quantile(difference, probs = q[2]),
            lower = quantile(difference, probs = q[1]), 
            upper = quantile(difference, probs = q[3]))


# SHAPS
shap_local_week <- read_rds(file.path(path_models, "outer_shapsgrp_1week_0_v5_nested.rds")) 
shap_local_day <- read_rds(file.path(path_models, "outer_shapsgrp_1day_0_v5_nested.rds"))
shap_local_hour <- read_rds(file.path(path_models, "outer_shapsgrp_1hour_0_v5_nested.rds")) 

shap_global_week <- shap_local_week |>  
  group_by(variable_grp) |>  
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>  
  arrange(mean_value) |> 
  mutate(model = "Week")
shap_global_day <- shap_local_day |>  
  group_by(variable_grp) |>  
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>  
  arrange(mean_value) |> 
  mutate(model = "Day")
shap_global_hour <- shap_local_hour |>  
  group_by(variable_grp) |>  
  summarize(mean_value = mean(abs(value)), .groups = "drop") |>  
  arrange(mean_value) |> 
  mutate(model = "Hour")
shap_global_all <- shap_global_week |> 
  bind_rows(shap_global_day) |> 
  bind_rows(shap_global_hour) |> 
  mutate(model = factor(model, levels = c("Week", "Day", "Hour"))) |>  
  mutate(variable_grp = factor(variable_grp, levels = c("past use (EMA item)", 
                                          "craving (EMA item)", 
                                          "past risky situation (EMA item)", 
                                          "past stressful event (EMA item)", 
                                          "past pleasant event (EMA item)", 
                                          "valence (EMA item)", 
                                          "arousal (EMA item)", 
                                          "future risky situation (EMA item)", 
                                          "future stressful event (EMA item)", 
                                          "future efficacy (EMA item)",
                                          "lapse day (other)",
                                          "lapse hour (other)",
                                          "missing surveys (other)",
                                          "age (demographic)",
                                          "sex (demographic)",
                                          "race (demographic)",
                                          "marital (demographic)",
                                          "education (demographic)")))
```

```{r table_1_calcs}

# Calcs to make df for table 1 (demographics and clinical characteristics)
n_total <- 151

dem <- screen |>  
  summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
            SD = as.character(round(sd(dem_1, na.rm = TRUE), 1)),
            min = as.character(min(dem_1, na.rm = TRUE)),
            max = as.character(max(dem_1, na.rm = TRUE))) |>  
  mutate(var = "Age",
         n = as.numeric(""),
         perc = as.numeric("")) |>  
  select(var, n, perc, everything()) |>  
  full_join(screen |>  
  select(var = dem_2) |>  
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_3) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("American Indian/Alaska Native", "Asian", "Black/African American",
                           "White/Caucasian", "Other/Multiracial")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_4) |>  
  mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
                         TRUE ~ "Yes"),
         var = fct_relevel(factor(var, c("Yes", "No")))) |>  
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_5) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("Less than high school or GED degree", "High school or GED", 
                           "Some college", "2-Year degree", "College degree", "Advanced degree")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = dem_6, dem_6_1) |>  
  mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
                         dem_6_1 == "Part-time" ~ "Employed part-time",
                         TRUE ~ var)) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("Employed full-time", "Employed part-time", "Full-time student",
                           "Homemaker", "Disabled", "Retired", "Unemployed", 
                           "Temporarily laid off, sick leave, or maternity leave",
                           "Other, not otherwise specified")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  summarise(mean = format(round(mean(dem_7, na.rm = TRUE), 0), big.mark = ","),
            SD = format(round(sd(dem_7, na.rm = TRUE), 0), big.mark = ","),
            min =format(round(min(dem_7, na.rm = TRUE), 0), big.mark = ","),
            max = format(round(max(dem_7, na.rm = TRUE), 0), scientific = FALSE, big.mark = ",")) |>  
  mutate(var = "Personal Income",
        n = as.numeric(""),
        perc = as.numeric(""),
        mean = str_c("$", as.character(mean)),
        SD = str_c("$", as.character(SD)),
        min = str_c("$", as.character(min)),
        max = as.character(max)) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", "min", "max")) |>  
  full_join(screen |>  
  select(var = dem_8) |>  
  mutate(var = case_when(var == "Never Married" ~ "Never married",
                         TRUE ~ var)) |>  
  mutate(var = fct_relevel(factor(var, 
                         c("Never married", "Married", "Divorced", "Separated",
                           "Widowed")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))

auh <- screen |>  
  summarise(mean = mean(auh_1, na.rm = TRUE),
            SD = sd(auh_1, na.rm = TRUE),
            min = min(auh_1, na.rm = TRUE),
            max = max(auh_1, na.rm = TRUE)) |>  
  mutate(var = "Age of first drink",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()) |>  
  full_join(screen |>  
  summarise(mean = mean(auh_2, na.rm = TRUE),
            SD = sd(auh_2, na.rm = TRUE),
            min = min(auh_2, na.rm = TRUE),
            max = max(auh_2, na.rm = TRUE)) |>  
  mutate(var = "Age of regular drinking",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", 
                                             "min", "max")) |>  
  full_join(screen |>  
  summarise(mean = mean(auh_3, na.rm = TRUE),
            SD = sd(auh_3, na.rm = TRUE),
            min = min(auh_3, na.rm = TRUE),
            max = max(auh_3, na.rm = TRUE)) |>  
  mutate(var = "Age at which drinking became problematic",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  summarise(mean = mean(auh_4, na.rm = TRUE),
            SD = sd(auh_4, na.rm = TRUE),
            min = min(auh_4, na.rm = TRUE),
            max = max(auh_4, na.rm = TRUE)) |>  
  mutate(var = "Age of first quit attempt",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  # filter out 2 people with 100 and 365 reported quit attempts - will make footnote in table
  filter(auh_5 < 100) |>  
  summarise(mean = mean(auh_5, na.rm = TRUE),
            SD = sd(auh_5, na.rm = TRUE),
            min = min(auh_5, na.rm = TRUE),
            max = max(auh_5, na.rm = TRUE)) |>  
  mutate(var = "Number of Quit Attempts*",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  select(var = auh_6_1) |> 
  mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ months)",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_2) |> 
  mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 months)",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_3) |> 
  mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_4) |> 
  mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_5) |> 
  mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
                         TRUE ~ var)) |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_6) |> 
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_6_7) |> 
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = auh_7) |>  
  mutate(var = fct_relevel(factor(var, c("Yes", "No")))) |> 
  group_by(var) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) |>  
  rowwise() |>  
  # calculate dsm5 score by adding up dsm5_1 through dsm5_11
  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7, 
                            dsm5_8, dsm5_9, dsm5_10, dsm5_11))) |>  
  ungroup() |>  
  summarise(mean = mean(dsm5_total),
            SD = sd(dsm5_total),
            min = min(dsm5_total, na.rm = TRUE),
            max = max(dsm5_total, na.rm = TRUE)) |>  
  mutate(var = "DSM-5 Alcohol Use Disorder Symptom Count",
        n = as.numeric(""),
        perc = as.numeric("")) |>  
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) |>  
  full_join(screen |>  
  select(var = assist_2_1) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_2) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Cannabis (marijuana, pot, grass, hash, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_3) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Cocaine (coke, crack, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_4) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_5) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Inhalants (nitrous, glue, petrol, paint thinner, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_6) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_7) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) |>  
  full_join(screen |>  
  select(var = assist_2_8) |> 
  filter(var != "Never" & !is.na(var)) |>  
  mutate(var = "Opioids (heroin, morphine, methadone, codeine, etc.)") |>  
  group_by(var) |>  
  drop_na() |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) 

lapses_per_subid <- screen |>  
  select(subid) |>  
  left_join(lapses |>  
  tabyl(subid) |>  
  select(-percent), by = "subid") |>  
  mutate(n = if_else(is.na(n), 0, n),
         lapse = if_else(n > 0, "yes", "no")) 

lapse_info <- lapses_per_subid |>  
  group_by(lapse) |>  
  rename(var = lapse) |>  
  mutate(var = factor(var, levels = c("yes", "no"), labels = c("Yes", "No"))) |>  
  summarise(n = n()) |>  
  mutate(perc = (n / n_total) * 100,
         mean = NA_real_,
         SD = NA_real_,
         min = NA_real_,
         max = NA_real_) |>  
  full_join(lapses_per_subid |>  
  summarise(mean = mean(n),
            SD = sd(n),
            min = min(n),
            max = max(n)) |>  
  mutate(var = "Number of reported lapses"), 
  by = c("var", "mean", "SD", "min", "max"))
```

```{r table_2_calcs}

# Calcs to create df for table 2 (performance metrics at Youden's Index)
j_thres_week <- roc_week |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)

j_thres_day <- roc_day |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)

j_thres_hour <- roc_hour |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)


metrics_week <- preds_week |> 
  mutate(estimate = if_else(prob > j_thres_week, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(week = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics_day <- preds_day |> 
  mutate(estimate = if_else(prob > j_thres_day, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(day = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics_hour <- preds_hour |> 
  mutate(estimate = if_else(prob > j_thres_hour, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(hour = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics <- metrics_week |>  
  full_join(metrics_day, by = "metric") |>  
  full_join(metrics_hour, by = "metric") |>  
  filter(metric %in% c("bal_accuracy", "sens", "spec", "ppv", "npv"))

auc <- tibble(metric = "auROC", 
              week = preds_week |>  roc_auc(prob, truth = label) |>   
                pull(.estimate) |>  round(3), 
              day = preds_day |>  roc_auc(prob, truth = label) |>   
                pull(.estimate) |>  round(3),
              hour = preds_hour |>  roc_auc(prob, truth = label) |>   
                pull(.estimate) |>  round(3))

metrics <- metrics |>  
  bind_rows(auc)

metrics <- metrics[c(6,1,2,5,3,4),]
```



```{r ppv_70_values}
# PPV and sens values at ppv = .70
ppv_70 <- pr_all |>  
  mutate(recall = round(recall, 3),
         precision = round(precision, 3),
         .threshold = round(.threshold, 3),
         model = factor(model,
                        levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) |>  
  filter(precision == .70) |>  
  group_by(model, precision) |>  
  summarise(recall = mean(recall),
            threshold = mean(.threshold),
            .groups = "drop")
```
# Introduction

Alcohol use disorder (AUD) is highly prevalent and costly. Among United States (U.S.) adults, over 30 million had an active AUD in 2021, and 23.3% reported engaging in past-month binge drinking [@samhsa2021NSDUHDetailed2021]. Alcohol ranks as the third leading preventable cause of death, accounting for approximately 140,000 fatalities per year [@centersfordiseasecontrolandpreventionAlcoholPublicHealth]. Economically, alcohol misuse cost the United States $249 billion in 2016 alone [@administrationusFacingAddictionAmerica2016]. 

Existing clinician-delivered treatments for AUD such as cognitive-behavioral therapy [@mchughCognitiveBehavioralTherapySubstance2010], mindfulness-based relapse prevention [@goldbergMindfulnessbasedInterventionsPsychiatric2018], motivational enhancement therapy [@millerEnhancingMotivationChange1993], and contingency management [@silvaContingencyManagementApplied2022] are effective when provided to patients.  Unfortunately, fewer than 1 in 20 adults with an active AUD receive any treatment [@samhsa2021NSDUHDetailed2021].  Even more concerning, the failure to access treatment is associated with demographic factors including race, ethnicity, geographic region, and socioeconomic status, which further increase mental health disparities [@wangFailureDelayInitial2005; @generalusMentalHealthCulture2001; @mauraMentalHealthDisparities2017]. This treatment gap and associated disparities stem from well-known barriers to receiving clinician-delivered mental healthcare related to affordability, accessibility, availability, and acceptability [@jacobsonUsingDigitalTherapeutics2023].

Digital therapeutics may help to combat these disparities. Digital therapeutics deliver evidence-based treatments through a smartphone "app" and are used to prevent, treat, or manage a medical disorder, either independently or in conjunction with traditional treatments [@jacobsonDigitalTherapeuticsMental2022]. They offer highly scalable, on-demand therapeutic support that is accessible whenever and wherever it is needed most. Several large, randomized controlled trials have confirmed that digital therapeutics for AUD improve clinical outcomes (e.g., abstinence, heavy drinking days [@gustafsonSmartphoneApplicationSupport2014; @campbellInternetdeliveredTreatmentSubstance2014]; see [@campbellFirstWaveScalable2023] for a review). Additionally, U.S. adults display high rates of smartphone ownership (approximately 85% in April 2021), with minimal variation across race, ethnicity, socioeconomic status, geographic settings (e.g., urban, suburban, rural), or AUD diagnosis [@pewresearchcenterMobileFactSheet2021; @collinsFactorsAssociatedPatterns2016]. Therefore, digital therapeutics may mitigate many of the barriers associated with in-person, clinician-delivered treatments.

## Improving Digital Therapeutics via Personal Sensing

Despite the documented benefits of digital therapeutics, their full potential has not yet been realized.  Patients often don't engage with them as developers intended, and long-term engagement may not be sustained or matched to patients' needs [@hatchExpertConsensusSurvey2018; @yeagerIfWeBuild2018].  The substantial benefits of digital therapeutics come from easy, 24/7 access to their treatment tools and other support services.  However, identifying the most appropriate tool remains a burden primarily placed on the patient. 

This difficulty is magnified by the dynamic, chronic, and relapsing nature of AUD [@brandonRelapseRelapsePrevention2007; @mclellanDrugDependenceChronic2000]. Numerous risk and protective factors interact in complex, non-linear ways to influence the probability, timing, and severity of relapse (i.e., a goal-inconsistent return to frequent, harmful substance use) [@huffordRelapseNonlinearDynamic2003; @witkiewitzNonnormalityDivergencePosttreatment2007; @witkiewitzModelingComplexityPosttreatment2007]. Factors such as urges, mood, lifestyle imbalances, self-efficacy, and motivation can all vary over time. Social networks may evolve to be more protective or risky, and high-risk situations can arise unexpectedly. Consequently, relapse risk fluctuates. 

Successful continuous monitoring of risk for lapse (i.e., a single instance of goal-inconsistent substance use), relapse, and their contributing factors would enable patients to adapt their lifestyle, behaviors, and supports to their changing needs. Successful monitoring could also direct patients to engage with the most appropriate treatment tools within a digital therapeutic, addressing the unique risks present at any given moment throughout their recovery. Until recently, the continuous monitoring of intra- and interpersonal risk factors required to track and forecast psychiatric disorders has proved challenging. However, this continuous monitoring is now feasible via moment-by-moment personal sensing (i.e., in-situ data collection via sensors embedded in individuals' day to day lives) [@epsteinPredictionStressDrug2020; @hebertPredictingFirstSmoking2021a; @businelleUsingIntensiveLongitudinal2016; @soysterPooledPersonspecificMachine2022; @moshontzProspectivePredictionLapses2021; @wyantAcceptabilityPersonalSensing2023; @chihPredictiveModelingAddiction2014; @baeMobilePhoneSensors2018]. 

The current project focuses explicitly on the use of ecological momentary assessment (EMA) to monitor AUD lapse risk. EMA can be easily implemented with only a smartphone. Moreover, comparable raw data (items and responses) can be used consistently across different hardware and operating systems. Thus, EMA can be incorporated essentially identically into any existing or future smartphone-based digital therapeutic. EMA, like other personal sensing methods, can support the frequent, in situ longitudinal measurement necessary for monitoring episodic or otherwise fluctuating relapse risk.  Long-term monitoring with EMA has been well-tolerated by individuals with AUD [@wyantAcceptabilityPersonalSensing2023].  Previous research has validated the use of EMA to measure theoretically implicated risk and protective factors for relapse including craving [@dulinSmartphonebasedMomentaryIntervention2017], mood [@russellAffectRelativeDayLevel2020], stressors [@wemmDaybydayProspectiveAnalysis2019], positive life events [@dvorakTensionReductionAffect2018], and motivation/efficacy [@dvorakEcologicalMomentaryAssessment2014].  Furthermore, EMA provides privileged access to these subjective constructs that may be difficult to quantify reliably through other sensing methods.  

## Promising Preliminary Research
Preliminary research is now emerging that uses EMA as features in machine learning models to predict the probability of future alcohol use [@baeMobilePhoneSensors2018; @soysterPooledPersonspecificMachine2022; @waltersUsingMachineLearning2021].  This research is important because it rigorously establishes temporal ordering between the predictors (features engineered from EMAs) and the outcomes. Additionally, the use of resampling methods prioritizes model generalizability (i.e., models are evaluated on data not used for training).

Despite this initial promise, several important gaps exist.  Prediction models developed with convenience samples (e.g., college students) [@soysterPooledPersonspecificMachine2022; @baeMobilePhoneSensors2018] may not generalize to clinical samples (i.e., people with AUD).  Similarly, models that have been developed to predict alcohol use in non-treatment-seeking hazardous drinking populations may be less clinically useful [@waltersUsingMachineLearning2021]. These individuals are unlikely to use digital therapeutics to support their recovery until they begin to contemplate and/or commit to behavior change regarding their alcohol use [@prochaskaSearchHowPeople1992].  Moreover, features that predict planned or otherwise intentional alcohol use among individuals that are not motivated to change their behavior may not generalize to patients in recovery.  

A handful of studies have trained models to predict putative precursors of substance use, such as craving [@burgess-hullTrajectoriesCravingMedicationassisted2022; @dumortierClassifyingSmokingUrges2016] and stress [@epsteinPredictionStressDrug2020]. However, even though craving and stress can be associated with substance use, their relationships with lapse and relapse are complex, inconsistent, and not always very strong [@fronkStressAllostasisSubstance2020, @sayetteRoleCravingSubstance2016]. 

Models that predict lapses may be preferred. Lapses are clearly defined, observable, and finite (with crisp onset and offset) instances of goal-inconsistent behavior. Definitions of relapse vary [@sliedrechtVarietyAlcoholUse2022], and it is difficult to clearly delineate when a lapse or series of lapses progresses to full relapse.  Furthermore, in many instances, a single or series of lapses may precede full relapse and therefore serve as early warnings signs that provide opportunity for intervention. Finally, maladaptive responses to individual lapses (e.g., abstinence violation effects; [@marlattRelapsePreventionMaintenance1985]) can undermine recovery by themselves, making them clinically meaningful events to detect and address.

<!--KW: Sarah, is Chih et al the only "alcohol lapse prediction" paper?-->
An early lapse prediction model developed by Gustafson et al. [@chihPredictiveModelingAddiction2014] provided the foundation on which our current project builds. Participants with AUD completed weekly EMAs for 8 months while using a digital therapeutic (A-CHESS). This study used a clinical sample committed to AUD recovery-related behavior change to predict a clinically meaningful outcome. However, the temporal precision for both the machine learning features and outcome was coarse. Model predictions were updated only once per week at best, and lapse onsets could occur anytime within the next two weeks. This coarseness restricts the model from being used to implement *just-in-time* interventions (e.g., guided mindfulness or other stress reduction techniques, urge surfing), well-suited to digital therapeutics.

## The Current Study

We designed the current study to address the gaps and limitations of previous prediction models.  We developed our models using participants in early recovery from moderate to severe AUD who reported a goal of alcohol abstinence.  We developed three separate models that provide hour-by-hour probabilities of a future lapse back to alcohol use with increasing temporal precision (i.e., lapses in the next week, next day, and next hour).  Model features were based on raw scores and longitudinal change in theoretically implicated risk factors. Relapse prevention literature suggests lapses are preceded by a high-risk context that increases one's vulnerability to lapsing (e.g., emotional or cognitive states, environmental contingencies, and physiological states) [@marlattRelapsePreventionMaintenance1985; @hendershotRelapsePreventionAddictive2011]. We measured various high-risk contexts including past use, craving, past pleasant events, past and future risky situations, past and future stressful events, valence, arousal, and future efficacy via 4x daily EMAs. This research represents an important step toward the development of a *smart* (machine learning guided) sensing system that can both identify periods of peak lapse risk and recommend specific supports to address factors contributing to this risk.  <!--KW: I added info on RP constructs-->  <!--JC to edit-->


# Method
<!--JC to add statement about power-->

## Transparency and openness 
<!--KW: note that this subheading wording is required by JPCS-->
We value the principles of research transparency that are fundamental to the robustness and replicability of science and took several steps to follow open science guidelines. We reported how we determined our sample size, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]. We completed a transparency report (see Supplement). Finally, we made the data, analysis scripts, annotated results, questionnaires, and other study materials associated with this report publicly available [https://osf.io/w5h9y/](https://osf.io/w5h9y/). 


This study’s design and its analyses were not pre-registered. Throughout this project, we iteratively improved machine learning methods that are rapidly evolving in the social sciences and used in this study. However, we restricted many researcher degrees of freedom via cross-validation procedures (See Cross validation). <!--Replication is built into cross-validation; models are fit using held-in training sets, decisions are made using held-out validation sets, and final model performance is evaluated in a confirmatory manner using held-out test sets.-->

## Participants
We recruited 151 participants in early recovery (1-8 weeks of abstinence) from AUD in Madison, Wisconsin, USA, to participate in a three-month longitudinal study. Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required that participants:

1.  were 18 years of age or older,
2.  were able to write and read in English,
3.  had at least moderate AUD (\>= 4 self-reported DSM-5 symptoms),
4.  were abstinent from alcohol for at least 1 week but no longer than 2 months,
5.  were willing to use a single smartphone (their personal phone or one provided by us) while enrolled in the study.

We also excluded participants if they exhibited severe symptoms of psychosis or paranoia <!--(scores > 2.2 or 2.8 on the psychosis or paranoia scales, respectively, of the Symptom Checklist – 90 [@derogatisSCL90OutpatientPsychiatric1973])-->. 

## Procedure
Participants completed five study visits over approximately three months. After an initial phone screen, participants attended an in-person screening visit where we determined eligibility, obtained informed consent, and collected self-report measures of individual differences (e.g., demographics and clinical characteristics; see Measures below). Eligible and consented participants returned approximately one week later to enroll in the study at an intake visit. Three additional follow-up visits occurred about every 30 days that participants remained on study. Participants were expected to complete EMAs four times each day while on study. Other personal sensing data streams (geolocation, cellular communications, sleep quality, and audio check-ins) were collected as part of the parent grant's aims (R01 AA024391). <!--KW: suggest moving compensation breakdown to supplement or to our OSF page.--><!-- We compensated participants for all time spent in the laboratory (\$20/hour) and paid them \$66/month to offset costs associated with their cellular plan.We paid participants a \$99 bonus if they completed the study for the full 3-month duration. We also paid participants bonuses (ranging from \$10-\$25) if they had 10% or less missing data for that method each month (e.g., participants were paid a bonus of \$25 for each month that they completed at least 90% of their EMAs).-->

## Measures
### EMA
<!--KW: we need to be consistent with how we refer to EMA constructs. I have edited to consistently use past use, future efficacy, craving, past and future stressful events, future and past risky situations, past pleasant events, arousal, and valence-->
<!--KW: Also, consider changing lapse day and lapse hour on SHAP plots to start day of prediction window and start hour of prediction window-->
Participants completed a brief (7-10 questions) EMA four times each day following pushed text message reminders. These text messages included a link to a Qualtrics survey, optimized for completion on their smartphone. 

All four EMAs included seven items that asked about any past alcohol use, current affective state (valence and arousal), craving, past stressful events, risky situations and pleasant events. The first EMA each day asked three additional questions about how likely participants were to encounter a risky situation, encounter a stressful event, and drink alcohol in the upcoming week (i.e., future efficacy). 

The first and last EMAs of the day were scheduled within one hour of participants' typical wake and sleep times. The other two EMAs were each scheduled randomly within the first and second halves of the participants' typical day. All EMAs were separated from each other by at least one hour.

### Individual Differences
We collected self-report information about demographics (age, sex, race, ethnicity, education, employment, personal income, and marital status) and clinical characteristics (AUD milestones, number of quit attempts, lifetime history of treatment for AUD, lifetime receipt of medication for AUD, DSM-5 AUD symptom count, and current drug use [@whoassistworkinggroupAlcoholSmokingSubstance2002]) to describe our sample. Only age, sex, race, education, and marital status are used as features in our machine learning models. 

## Data Analytic Strategy
Data preprocessing and modeling were done in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020]. Bayesian analyses were accomplished using the tidyposterior [@kuhnTidyposteriorBayesianAnalysis2022] and rstanarm [@goodrichBayesianAppliedRegression2023] packages in R. All models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computer [@chtc].

### Lapse Labels
We created future prediction windows that varied in their temporal precision by using three distinct window widths<!--(i.e., duration between window start and end time)-->: one week, one day, and one hour.<!--KW: move to supplement--><!-- For each participant, the start of the first prediction window for all three widths began at midnight on their second day of participation and ended one week, one day, or one hour later. By beginning at the end of the second day, we were assured that there would be at least 24 hours of past EMAs to use for future lapse prediction in these first windows.  Subsequent windows for each participant were created for all three widths by repeatedly rolling the window start/end forward one hour until the end of their study participation was reached (i.e., each participant's last prediction window for each width ended at the date and hour of their last recorded EMA).--> Prediction windows were updated each hour. Therefore, we were able to develop classification models that provided hour-by-hour predictions of future lapse probability for all three window widths.

We labeled each prediction window as *lapse* or *no lapse* using participants' reports from the EMA item "Have you drank any alcohol that you have not yet reported?". If participants answered yes to this question, they were prompted to enter the hour and date of the start and end of the drinking episode.  These reports were also validated by study staff during the monthly followup visits. 

<!--KW: move to supplement--><!--A prediction window was labeled *lapse* if the start date/hour of any drinking episode fell within that window.  Conversely, a window was labeled *no lapse* if no alcohol use occurred within 24 hours of the window start/end.  If no alcohol use occurred within the window but did occur within the 24 hours of the start or end of the window, the window was excluded. We used this conservative 24-hour fence for labeling windows as *no lapse* (vs. excluded) to increase the fidelity of these labels.  Given that most windows were labeled *no lapse* (i.e., *no lapse* was the majority class, and the outcome was highly unbalanced), it was not problematic to exclude some *no lapse* events to further increase confidence in those labels.-->

### Feature Engineering
Features were calculated using only data that were collected prior to the start of each prediction window to ensure that our models were true *prediction* models (i.e., making future predictions rather than identifying concurrent associations).  

Features were derived from three sources.   The first source of features included demographic characteristics (i.e., age, sex, race, marital status, education) collected at baseline.  The second source of features used previous EMA responses.  We created EMA features using both raw (e.g., min., max., median, most recent response, and total counts) and change (e.g., within-subject baseline comparisons) scores. <!--KW: move to supplement--><!--We scored raw min, max, median, and count features within a small set of periods prior to the start of the prediction window (6, 12, 24, 48, 72, and 168 hours prior to start of window).  We scored change features by subtracting the mean response for each feature over all data prior to the start of the prediction window from the associated raw feature.-->  The third source of features was based on the day of the week and the time of day (daytime vs. evening/night) of the start of the prediction window.

<!--KW: move to supplement--><!--Other generic feature engineering steps included: 1) imputation for missing data for features (median imputation for numeric features, mode imputation for nominal features); 2) dummy coding for nominal features; and 3) removal of any zero variance features. A sample feature engineering script (i.e., tidymodels recipe) containing all feature engineering steps is available on our OSF study page.  Medians/mode for missing data imputation and identification of zero variance features were derived from training (held-in) data and applied to held out (validation and test) data to prevent issues associated with data leakage (see Model Training procedures below). -->

### Model Training and Evaluation

#### Statistical Algorithm and Hyperparameters
We trained and evaluated three separate machine learning classification models (i.e., one for each of the three prediction window widths - week, day, and hour).  We initially considered four statistical algorithms (XGBoost, Random Forest, K-Nearest Neighbors, and Elastic Net). These algorithms are well-established with documented good "out of box" performance, and they vary with respect to various characteristics expected to affect model performance (e.g., flexibility, complexity, and ability to handle higher-order interactions natively) [@kuhnAppliedPredictiveModeling2018]. However, preliminary exploratory analyses suggested that XGBoost consistently outperformed the other three algorithms.  Furthermore, the Shapley Additive Explanations (SHAP) method, which we planned to use for explanatory analyses of feature importance, is optimized for XGBoost.  For these reasons, we focused our primary model training and evaluation on the XGBoost algorithm only.  
  
We trained candidate XGBoost model configurations that differed across sensible values for the hyperparameters mtry, tree depth, and learning rate using grid search.  All configurations used 500 trees combined with early stopping to prevent over-fitting.  All other hyperparameters were set to defaults established by the tidymodels packages in R.  Candidate model configurations also differed with respect to the outcome resampling method (i.e., up-sampling and down-sampling of the outcome using majority/no lapse to minority/lapse ratios that ranged from 1:1 to 5:1).  We calibrated the predicted probabilities from these XGBoost models using the beta distribution <!--KW: this second half of the sentence feels a little wordy but not familiar enough with the method to try to make more clear-->to support optimal decision-making under variable outcome class distributions [@kullSigmoidsHowObtain2017].

#### Performance Metric
Our primary performance metric for selecting and evaluating the best model configurations was the area under the Receiver Operating Characteristic Curve (auROC) [@kuhnAppliedPredictiveModeling2018]. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (i.e., lapse) relative to a randomly selected negative case (i.e., no lapse).  <!--KW: I don't think this is needed. We already say its a probability and we explain what an auROC of .9 means clearly in discussion--><!-- An auROC of 0.5 indicates chance performance; an AUC of 1.0 perfectly discriminates between positive and negative outcome classes.-->  This metric was selected because it 1) combines sensitivity and specificity, which are both important characteristics to consider for clinical implementation; 2) is an aggregate metric across all possible decision thresholds, which is important because the optimal decision threshold may differ depending on setting and goals; and 3) is not affected by class imbalance, which is important when comparing models across the three different prediction window widths that have different levels of class imbalance.  

#### Cross-validation 
We used grouped (by participant), nested cross-validation to train models, select best models, and evaluate those best models on auROC.  With grouped cross-validation, all a participant's data is either held-in or held-out to avoid bias that is introduced when predicting a participant's data from their own data [@saebVoodooMachineLearning2016].  

<!--KW: I am struggling a bit with how to simplify this cv section. I think we should approach it as we are not going to be able to explain how nested cv works to someone not familiar with it already. Instead we just need to get across the point that we have separate validation and test sets. I commented out some info that we can maybe get rid of-->
In nested cross-validation, there are two nested loops for dividing and holding out folds: an outer loop<!-- with k~outer~ folds-->, where held-out folds serve as *test sets* for model evaluation; and inner loops<!-- with k~inner~ folds-->, where held-out folds serve as *validation sets* for model selection. <!--The full dataset is divided into k~outer~ approximately equal-sized, non-overlapping folds. Each k~outer~ fold serves once as an independent *test set* for model evaluation. The remaining data (i.e., the k~outer~ - 1 outer folds) are divided into k~inner~ folds. Each k~inner~ fold is used once as a *validation set* for models trained on k~inner~ - 1 inner folds (i.e., *training set*).-->  Importantly, these loops are independent, maintaining separation between data used to train the models<!-- (k~inner~ - 1 folds)-->, *validation sets*<!-- (k~inner~th held-out fold)-->, and *test sets*<!-- (k~outer~th held-out fold)-->. This separation removes optimization bias from the evaluation of model performance in the test sets and can also yield lower variance performance estimates than when a single independent test set is used [@jonathanUseCrossvalidationAssess2000].  

In this study, we used 1 repeat of 10-fold cross-validation for the inner loop<!--s?--> and 3 repeats of 10-fold cross-validation for the outer loop.  Therefore, best model configurations were selected based on the median <!--JC will add footnote for why median-->auROC across the 10 *validation sets*<!--KW: 10 validation sets per held-out outer fold?-->.  Final evaluation of the performance of those best model configurations was based on the median auROC across the 30 *test sets*.  For completeness, we report median auROC for our best model configurations for each model (week, day, and hour) separately from both the validation and test sets. In addition, we report key additional performance metrics for the best model configurations including sensitivity, specificity, balanced accuracy, positive predictive value (PPV), and negative predictive value (NPV) from the test sets [@kuhnAppliedPredictiveModeling2018].

### Bayesian Estimation of auROC and Model Comparisons 

We used a Bayesian hierarchical generalized linear model [@mcelreathStatisticalRethinkingBayesian2020] to estimate the posterior probability distributions and 95% Bayesian confidence intervals (CIs) for auROC for the three best models (i.e., week, day, and hour). To determine the probability that these models' performance differed systematically from each other, we regressed the auROCs<!--posterior probabilities?--> (logit transformed to address bounded, skewed distribution) from the 30 test sets for each model as a function of window width. Following recommendations from the tidymodels team [@kuhnTidyposteriorBayesianAnalysis2022; @kuhnBayesianAnalysisResampling], we set two random intercepts: one for the repeat, and another for the fold within repeat (folds are nested with repeats for auROCs collected with 3x10-fold cross-validation).  <!--KW: suggest moving to supplement--><!--Following recommendations from the rstanarm team and others [@rstudioteamRStudioIntegratedDevelopment2020; @gabryPriorDistributionsRstanarm2023], we used the rstanarm default autoscaled, weakly informative, data-dependent priors that take into account the order of magnitude of the variables to provide some regularization to stabilize computation and avoid over-fitting.  Specifically, the priors were set as follows: residual standard deviation ~ normal(location=0, scale=exp(2)), intercept (after centering predictors) ~ normal(location=2.3, scale=1.3), the two coefficients for window width contrasts ~ normal (location=0, scale=2.69), and covariance ~ decov(regularization=1, concentration=1, shape=1, scale=1).--> We report the 95% (equal-tailed) Bayesian CIs (i.e., intervals that have 0.95 posterior probability of containing the true population value for the parameter) from the posterior probability distributions for auROC for the models.  We also report 95% (equal-tailed) Bayesian CIs for the differences in performance among the three models.  



### Shapley Additive Explanations for Feature Importance

We used the SHAP method [@lundbergUnifiedApproachInterpreting2017] to provide a consistent and objective explanation of the importance for model predictions associated with categories of features from the three models.  SHAP computes Shapley values, which are model-agnostic and possess several useful properties including: Additivity (Shapley values for each feature can be computed independently and then summed); Efficiency (the sum of Shapley values across features must add up to the difference between the predicted and observed outcome for each observation); Symmetry (Shapley values for two features should be equal if the two features contribute equally to all possible coalitions); and Dummy (a feature that does not change the predicted value in any coalition will have a Shapley value of 0).  <!--KW: suggest removing this sentence - Finally, SHAP has a fast implementation for tree-based models that makes its use computationally feasible for the XGBoost algorithm, even with large sample sizes.-->  We calculated Shapley values from the 30 test sets using the SHAPforxgboost package that provides Shapley values in log-odds units for binary classification models.  We averaged the three Shapley values for each observation for each feature across the three repeats to increase their stability. To calculate the local (i.e., for each observation) impact of categories of features (e.g., all features associated with the EMA craving item), we added Shapley values across all features in a category, separately for each observation.  To calculate global importance for categories of features, we averaged the absolute value of the Shapley values of all features in the category across all observations.  



# Results

## Demographic and Clinical Characteristics

One hundred ninety-two participants were eligible for enrollment. Of these participants, 191 consented to participate in the study at the screening session, and 169 subsequently enrolled in the study at the intake visit which occurred approximately one week later. Fifteen participants discontinued prior to the first monthly follow-up visit. 

We excluded data from one participant who did not maintain a goal of abstinence during their participation (i.e., they reported they were uncertain if their goal was abstinence on the daily EMA and monthly follow-up surveys). We also excluded data from two participants who showed evidence of careless responding and unusually low compliance, rendering their lapse labels unusable. Our final sample consisted of 151 participants. These participants provided study measures for one (N = 14), two (N = 6) or three (N = 131) months. Figure S1 presents a CONSORT diagram that displays more detail on enrollment and disposition for all eligible participants. 

The final sample of 151 participants included approximately equal numbers of men (N=77; 51%) and women (N=74; 49%) who ranged in age from 21 - 72 years old.  The sample was majority White (N=131; 87%) and non-Hispanic (N=147; 97%).  Participants self-reported a mean of 8.9 DSM-5 symptoms of alcohol use disorder (SD=5.8; range=4-11), a mean of 5.5 previous quit attempts (SD=5.8, range=0-30), and many reported using medications for AUD (N=59; 39%).  Most participants (N=84; 56%) reported one or more alcohol lapses during the study period.  The mean number of lapses per participant during the study period was 6.8 (SD=12.0; range=0-75).  Table 1 provides more detail on demographic and clinical characteristics of the sample.

\newpage
<!--************************************************************************-->
<!-- Table 1: Demographics and clinical characteristics-->
<!--KW: fix rendering - footnote runs off page and table doesn't go to end of paper like other table-->

<!--UPDATE -JJC handle description of outliers for quit attempts-->
```{r dem_table}
#| output: true


footnote_table_dem_a <- "N = 151"

footnote_table_dem_b <- "Two participants reported 100 or more quit attempts. We removed these outliers prior to calculating the mean (M), standard deviation (SD), and range."

dem  |>  
  bind_rows(auh |>  
              mutate(across(mean:max, ~round(.x, 1))) |>  
              mutate(across(mean:max, ~as.character(.x)))) |>  
  bind_rows(lapse_info |>  
              mutate(across(mean:max, ~round(.x, 1))) |>  
              mutate(across(mean:max, ~as.character(.x)))) |>  
  mutate(range = str_c(min, "-", max)) |> 
  select(-c(min, max)) |>  
  kbl(longtable = TRUE,
      booktabs = TRUE,
      col.names = c("", "N", "%", "M", "SD", "Range"),
      align = c("l", "c", "c", "c", "c", "c"),
      digits = 1,
      caption = "Demographics and clinical characteristics") |> 
  kable_styling(position = "l", latex_options = c("HOLD_position")) |>  
  row_spec(row = 0, align = "c", italic = TRUE) |>  
  column_spec(column = 1, width = "18em") |> 
  pack_rows("Sex", 2, 3, bold = FALSE) |>  
  pack_rows("Race", 4, 8, bold = FALSE) |>  
  pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) |>  
  pack_rows("Education", 11, 16, bold = FALSE) |>  
  pack_rows("Employment", 17, 25, bold = FALSE) |>  
  pack_rows("Marital Status", 27, 31, bold = FALSE) |>  
  pack_rows("Alcohol Use Disorder Milestones", 32, 35, bold = FALSE) |>  
  pack_rows("Lifetime History of Treatment (Can choose more than 1)", 37, 43, bold = FALSE) |>  
  pack_rows("Received Medication for Alcohol Use Disorder", 44, 45, bold = FALSE) |>  
  pack_rows("Current (Past 3 Month) Drug Use", 47, 54, bold = FALSE) |>  
  pack_rows("Reported 1 or More Lapse During Study Period", 55, 56, bold = FALSE) |> 
  kableExtra::footnote(general = c(footnote_table_dem_a, footnote_table_dem_b))
```


\newpage


## EMA compliance, features (predictors), and prediction window labels (outcome)

EMA compliance over the study was generally good, with participants completing an average of 3.1 (SD=0.6) of the 4 daily EMAs per day or 78% compliance overall.  Participants completed at least 1 EMA on 95% of days.  Across individual weeks in the study, EMA compliance percentages ranged from 75% - 87% completion for all of the 4x daily EMAs and from 92% - 99% for at least 1 daily EMA completed.  Figure S3 displays completion percentages over time (by week) across the study period for both 4x daily and at least one daily report.  

Using these EMA reports, we created datasets with 270,081, 274,179, and 267,287 future prediction windows for the outcome that were labeled as "lapse" or "no lapse" for the week, day, and hour window widths, respectively.  Each of these datasets also contained 286 features that were derived from 1) EMAs that were completed prior to the start of the associated prediction window, 2) baseline demographic characteristics, and 3) prediction window start dates/times as described earlier in the method. Each of these datasets was unbalanced with respect to the outcome such that lapses were observed in 68,467 (25.3%) week windows, 21,107 (7.7%) day windows, and 1,017 (0.3%) hour windows. 

 
## Model Performance

### auROC

Best model configurations were selected via validation set performance. The median auROCs from the validation sets for the best configurations were high for the week (median=`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> median())`, IQR=`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> IQR())`, range=`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> min())`-`r sprintf("%1.2f", auc_week |> pull(roc_auc_in) |> max())`), day (median=`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> median())`, IQR=`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> IQR())`, range=`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> min())`-`r sprintf("%1.2f", auc_day |> pull(roc_auc_in) |> max())`), and hour (median=`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> median())`, IQR=`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> IQR())`, range=`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> min())`-`r sprintf("%1.2f", auc_hour |> pull(roc_auc_in) |> max())`) prediction windows. 

Best model configurations were evaluated via test set performance. The median auROC across the 30 test sets remained high for the week (median=`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> median())`, IQR=`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> IQR())`, range=`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> min())`-`r sprintf("%1.2f", auc_week |> pull(roc_auc) |> max())`), day (median=`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> median())`, IQR=`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> IQR())`, range=`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> min())`-`r sprintf("%1.2f", auc_day |> pull(roc_auc) |> max())`), and hour (median=`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> median())`, IQR=`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> IQR())`, range=`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> min())`-`r sprintf("%1.2f", auc_hour |> pull(roc_auc) |> max())`) prediction windows.  The left panel of Figure 1 displays the ROC curves by model (i.e., window width) derived by aggregating predicted lapse probabilities across these same 30 test sets.  Figure S4 presents the individual ROC curves from each of the 30 test sets.  

Posterior probability distributions (and 95% CIs) for the auROC are displayed separately for each model in the right panel of Figure 1.  The median auROCs from these posterior distributions were `r sprintf("%1.2f", ci |> filter(model=="Week") |> pull(median) |> as.numeric())`,  `r sprintf("%1.2f", ci |> filter(model=="Day") |> pull(median) |> as.numeric())`, and `r sprintf("%1.2f", ci |> filter(model=="Hour") |> pull(median) |> as.numeric())` for the week, day, and hour models, respectively.  These median values from the Bayesian posterior distributions represent our best estimates for magnitude of the auROC parameter for the models for these three prediction windows.  The 95% Bayesian confidence intervals (95% CI) for the auROCs for these models were relatively narrow and did not contain 0.5 for any of the three window widths: week [`r sprintf("%1.2f", ci |> filter(model=="Week") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci |> filter(model=="Week") |> pull(upper) |> as.numeric())`], day [`r sprintf("%1.2f", ci |> filter(model=="Day") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci |> filter(model=="Day") |> pull(upper) |> as.numeric())`], hour [`r sprintf("%1.2f", ci |> filter(model=="Hour") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci |> filter(model=="Hour") |> pull(upper) |> as.numeric())`].  

We also used these posterior probability distributions for the auROCs of the three models to formally compare the differences in performance of these models.  The median increase in auROC for the hour vs. the day model was `r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. day") |> pull(median) |> as.numeric())` (95% CI=[`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. day") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. day") |> pull(upper) |> as.numeric())`], yielding a probability of `r sprintf("%1.3f", pp_diffs |> filter(contrast=="hour vs. day") |> pull(difference) |> (\(x) mean(x>0))())` that the hour model had superior performance relative to the day model.  The median increase in auROC for the hour vs. the week model was `r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. week") |> pull(median) |> as.numeric())` (95% CI=[`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. week") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci_diffs |> filter(contrast == "hour vs. week") |> pull(upper) |> as.numeric())`], yielding a probability of `r sprintf("%1.3f", pp_diffs |> filter(contrast=="hour vs. week") |> pull(difference) |> (\(x) mean(x>0))())` that the hour model had superior performance relative to the week model.  The median increase in auROC for the day vs. the week model was `r sprintf("%1.2f", ci_diffs |> filter(contrast == "day vs. week") |> pull(median) |> as.numeric())` (95% CI=[`r sprintf("%1.2f", ci_diffs |> filter(contrast == "day vs. week") |> pull(lower) |> as.numeric())`-`r sprintf("%1.2f", ci_diffs |> filter(contrast == "day vs. week") |> pull(upper) |> as.numeric())`], yielding a probability of `r sprintf("%1.3f", pp_diffs |> filter(contrast=="day vs. week") |> pull(difference) |> (\(x) mean(x>0))())` that the day model had superior performance relative to the week model.  We present histograms (and 95% CIs) of the posterior probability distributions for these model contrasts on auROC in Figure S5.

\newpage
<!--************************************************************************-->
<!-- Figure 1: ROC and Posterior probability histograms for auROC by model-->  
<!--JC: check fig caption and fig note-->
<!--KW: needs to figure out why figure won't render when fig note is included. Commented out for now-->

```{r fig_1_roc}
#| output: true
#| apa-cap: ROC curves and posterior probabilities for auROCs

# apa-note: The left panel depicts the aggregate receiver operating characteristic (ROC) curve for each model, derived by concatenating predicted lapse probabilities across all test. The dotted line represents the expected ROC curve for a random classifier. The histograms on the right depict the posterior probabilities for the areas under the receiver operating characteristic curves (auROCs) for each model. The vertical lines represents the median posterior probability and the horizontal line represents the 95% credible interval.
roc_plot <- roc_all |>  
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"), 
                        labels = c("Week", "Day", "Hour"))) |>  
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path(linewidth = 1.25, show.legend = FALSE) +
  geom_abline(lty = 3) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Specificity",
       y = "Sensitivity") +
  scale_x_continuous(breaks = seq(0,1,.25),
                     labels = sprintf("%.2f", seq(1,0,-.25))) 

pp_plot <- pp_tidy |>  
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour"))) |> 
  ggplot() + 
  geom_histogram(aes(x = posterior, fill = model), color = "black", alpha = .4, 
                 bins = 30) +
  geom_segment(mapping = aes(y = y+100, yend = y-100, x = median, xend = median,
                           color = model), show.legend = FALSE, data = ci) +
  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, color = model),
              show.legend = FALSE, data = ci) +
  # geom_text(data = ci, x = c(.93, .907, .92), y = 1000, 
  #           label = str_c(round(ci$median, 2), " [", round(ci$lower, 2), ", ", round(ci$upper, 2), "]")) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous("Count", breaks = c(0, 500, 1000)) +
  xlab("Posterior probability for auROC") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank())

roc_plot + pp_plot +
  plot_layout (guides = "collect") &
  theme(legend.position = "bottom")
```


\newpage

### Other Performance Metrics

To rigorously evaluate the performance of these three prediction models, we calculated additional performance metrics beyond the auROC in the 30 test sets.  Figure S6 displays histograms for the predicted probabilities of lapse for all observations in these test sets separately for each model.  We evaluated the sensitivity, specificity, balanced accuracy (i.e., mean of sensitivity and specificity), PPV, and NPV when these predicted lapse probabilities were used for binary classification (lapse vs. no lapse) with decision thresholds (i.e., the probability cut-point above which an observation is classified as a lapse) identified by Youden's Index <!--ref--> to balance the trade-off between sensitivity and specificity for each of the three models.  Table 2 presents these additional performance metrics by model.    

We also created Precision-Recall curves by aggregating predicted lapse probabilities across the 30 test sets to evaluate the trade-off between positive predictive value (i.e., precision) and sensitivity (i.e., recall) across decision thresholds. As seen in Figure 2, the PPV of any model can be increased by increasing the decision threshold for classifying an observation as a lapse; however, increasing the decision threshold will also lower the model's sensitivity. Decision thresholds can be selected from anywhere on the curve to maximize specific performance estimates (e.g., Youden's Index as displayed in Table 2) or to fit an intended application context. These Precision-Recall curves (Figure 2) depict the trade-off between positive predictive value and sensitivity across decision thresholds.   For example, the dotted lines in Figure 2 depict the sensitivities (`r sprintf("%1.2f", ppv_70 |> filter(model == "Week") |> pull(recall))`, `r sprintf("%1.2f", ppv_70 |> filter(model == "Day") |> pull(recall))`, and `r sprintf("%1.2f", ppv_70 |> filter(model == "Hour") |> pull(recall))` for week, day, and hour models, respectively) associated with decision thresholds that yield 0.70 positive predictive value for each of those models.  

\newpage
<!--************************************************************************-->
<!-- Table 2: Performance metrics-->
<!--John: check footnote-->

```{r table_2}
#| output: true

footnote_table_metrics <- "Areas under the receiver operating characteristic curves (auROCs) summarize the model's sensitivity and specificity over all possible decision thresholds. Sensitivity, specificity, balanced accuracy, positive predictive value, and negative predictive value are performance metrics calculated at a single decision threshold for each model. Our decision thresholds (.24, .06, and .003 for week, day, and hour level model’s respectively) was determined with Youden’s index, a statistical formula to maximize sensitivity and sensitivity."

metrics |> 
 mutate(metric = case_when(metric == "auROC" ~ "auROC",
                           metric == "sens" ~ "sensitivity",
                           metric == "spec" ~ "specificity",
                           metric == "bal_accuracy" ~ "balanced accuracy",
                           metric == "ppv" ~ "positive predictive value",
                           metric == "npv" ~ "negative predictive value")) |> 
 kbl(col.names = c("Metric", "Week", "Day", "Hour"),
     booktabs = TRUE,
     digits = 2,
     align = c("l", "l", "l", "l"),
     linesep = "",
     caption = "Performance Metrics by Model") |>  
  kable_styling(position = "left", latex_options = c("HOLD_position")) |>  
  column_spec(column = 1, width = "25em") |> 
  kableExtra::footnote(general = c(footnote_table_metrics), threeparttable = TRUE)
```

\newpage
<!--************************************************************************-->
<!-- Figure 2: PR curves by model-->
<!--JOHN: check fig caption-->
```{r fig_2_pr}
#| output: true
#| apa-cap: Precision-recall curves by model 
#| apa-note: The plot depicts the aggregate precision-recall curves for each model, derived by concatenating predicted lapse probabilities across all test sets. The dotted lines depict the sensitivities (0.72, 0.47, and 0.33 for week, day, and hour models, respectively) associated with decision thresholds that yield 0.70 positive predictive value for each of those models.


pr_all |>  
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) |> 
  ggplot(aes(x = recall, y = precision, color = model)) +
  geom_path(linewidth = 1.25) +
  geom_segment(mapping = aes(y = .7, yend = .7, x = -.5, xend = recall,
                           color = model),
               linetype = "dashed",
               alpha = .8,
               show.legend = FALSE,
               data = ppv_70) +
  geom_segment(mapping = aes(y = -.5, yend = .7, x = recall, xend = recall,
                           color = model),
               linetype = "dashed",
               alpha = .8,
               show.legend = FALSE,
               data = ppv_70) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Sensitivity (Recall)",
       y = "Positive Predictive Value (Precision)")
```

\newpage


## Feature Importance

We display the global importance (mean |Shapley value|) for feature categories for each of the three models in Panel A of Figure 3.  These feature categories are ordered by their aggregate global importance (i.e., total bar length) across the three models.  The importance of each feature category for specific models is displayed separately by color.  <!--KW: just noting that the content of the rest of this paragraph is also in the discussion-->Past use is a strong global predictor of future use consistently for all three models.  Self-reported future abstinence efficacy also emerges as a strong and consistent global predictor of future use.  The start time of the prediction window affects lapse predictions for next hour prediction but loses its value when the prediction windows encompass a full day or week.  Most of the more punctuate, time varying constructs (e.g., craving, stressful events, arousal) appear to have greater impact within the next hour prediction model. <!--JJC: not sure how much of this is starting to sounds like discussion vs. results--> <!-- GEF: i took out the final clause about what that might mean-->

We display local Shapley values that quantify the influence of feature categories on individual observations (i.e., a single prediction window for a specific participant) for each model in Panels B-D of Figure 3.  Critically, these Sina plots confirm that some feature categories (e.g., past pleasant events, future stressful events) impact lapse probability for specific individuals at specific times even if they are not globally important across all observations.  <!--KW: this last sentence is also brought into discussion. Not sure if we want to keep it here too for emphasis-->


<!--************************************************************************-->
<!-- Figure 3: SHAP Importance figure-->
<!--NEED TO ADD HEAT COLORS FOR DIRECTION TO LOCAL PLOTS - KENDRA?-->
<!--CONSIDER PANEL TITLES?-->
<!--CONSIDER WHERE LEGEND GOES-->
<!--JOHN: check fig caption. Also, I add panel titles and moved legend. The code is commented out for knitting the paper though right now because it takes so long. Fig is instead saved out and read in.-->
```{r make_global_shap}
# panel_shap_global <- shap_global_all |>  
#   mutate(variable_grp = reorder(variable_grp, mean_value, sum)) |>  
#   ggplot() +
#   geom_bar(aes(x = variable_grp, y = mean_value, fill = model), stat = "identity", alpha = .4) +
#   ylab("Mean(|Shapley Value|)") +
#   xlab("") +
#   coord_flip()
```

```{r make_shap_local}
# # get colors for models to match other figures that use faceting or grouping
# colors_hex <- scales::hue_pal()(3) # week = 1, day = 2, hour = 3
# 
# # order features to match global plot
# shap_levels <- shap_global_all |>
#   mutate(variable_grp = reorder(variable_grp, mean_value, sum)) |>
#   pull(variable_grp) |>
#   levels()
# 
# # downsample to 10% of observations for each plot
# downsample_ratio <- .10
# ids_week <- shap_local_week |>
#   pull(id_obs) |>
#   unique()
# ids_week <- ids_week |> sample(size = round(length(ids_week)/(1/downsample_ratio)))
# ids_day <- shap_local_day |>
#   pull(id_obs) |>
#   unique()
# ids_day <- ids_day |> sample(size = round(length(ids_day)/(1/downsample_ratio)))
# ids_hour <- shap_local_hour |>
#   pull(id_obs) |>
#   unique()
# ids_hour <- ids_hour |> sample(size = round(length(ids_hour)/(1/downsample_ratio)))
# 
# # week panel
# panel_shap_local_week <- shap_local_week |>
#   filter(id_obs %in% ids_week) |>
#   mutate(variable_grp = factor(variable_grp, levels = shap_levels)) |>
#   ggplot(mapping = aes(x = variable_grp, y = value)) +
#   ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4,
#                      color = colors_hex[1]) +
#   geom_hline(yintercept = 0) +
#   scale_y_continuous(limits = c(-2, 5), breaks = seq(-2, 5)) +
#   ylab("Shapley Value") +
#   xlab("") +
#   coord_flip()
# 
# # day panel
# panel_shap_local_day <- shap_local_day |>
#   filter(id_obs %in% ids_day) |>
#   mutate(variable_grp = factor(variable_grp, levels = shap_levels)) |>
#   ggplot(mapping = aes(x = variable_grp, y = value)) +
#   ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4,
#                      color = colors_hex[2]) +
#   geom_hline(yintercept = 0) +
#   scale_y_continuous(limits = c(-2, 5), breaks = seq(-2, 5)) +
#   ylab("Shapley Value") +
#   xlab("") +
#   coord_flip()
# 
# # hour panel
# panel_shap_local_hour <- shap_local_hour |>
#   filter(id_obs %in% ids_hour) |>
#   mutate(variable_grp = factor(variable_grp, levels = shap_levels)) |>
#   ggplot(mapping = aes(x = variable_grp, y = value)) +
#   ggforce::geom_sina(method = "counts", maxwidth = 0.7, alpha = 0.4,
#                      color = colors_hex[3]) +
#   geom_hline(yintercept = 0) +
#   scale_y_continuous(limits = c(-2, 5), breaks = seq(-2, 5)) +
#   ylab("Shapley Value") +
#   xlab("") +
#   coord_flip()
```


```{r fig_3_shap}
#| output: true
#| apa-cap: Variable Importance (Shapley Values) by Model
#| apa-note: Panel A displays the global importance (mean |Shapley value|) for feature categories for each model. Raw EMA features are grouped into categories by the original item from the EMA. Features from demographics and the day and hour for the start of the prediction window are also included. Feature categories are ordered by their aggregate global importance (i.e., total bar length) across the three models.  The importance of each feature category for specific models is displayed separately by color. Panels B-D display local Shapley values that quantify the influence of feature categories on individual observations (i.e., a single prediction window for a specific participant) for each model.

knitr::include_graphics(path = "figures/fig_3.jpeg", dpi = 130)

# panel_shap_global + panel_shap_local_week + panel_shap_local_day + panel_shap_local_hour +
#    plot_annotation(tag_levels = 'A') +
#    plot_layout (ncol = 2, guides = "collect", width = c(1,1)) 
```



# Discussion

## Model Performance

Models across all three window widths performed exceptionally well, yielding auROCs of .90, .91, and .94 for week, day, and hour level models, respectively. auROCs summarize the model's sensitivity and specificity over all possible decision thresholds (i.e., the probability cut points at which an observation is classified as positive). auROCs above .9 are generally described as having "excellent" performance, meaning that the model will correctly classify a positive case as positive and negative case as negative 90% of the time [@mandrekarReceiverOperatingCharacteristic2010]. All three of our models performed as well as or better than existing alcohol lapse prediction models to date [@chihPredictiveModelingAddiction2014<!--KW: Sarah, is this the only true "alcohol lapse" prediction paper to cite? You had Berenholtz et al., 2020 listed but can't figure out what article you are referring to? Also noting this statement would not be true if we include alcohol use prediction (i.e., Bae et al)-->]. This indicates EMA data can be used to predict alcohol lapses in the next week, next day, and next hour with high levels of sensitivity and specificity. 

An additional, critical component of model performance is PPV, which describes the proportion of a model's predicted positive cases relative to the true amount positive cases in the data at a given decision threshold.  We used Youden's index to determine an optimal decision threshold to maximize sensitivity and specificity. At this threshold, PPV across our models varied considerably with values of .60, .27, and .02 for week, day, and hour level models, respectively. PPV, unlike auROC, is highly influenced by unbalanced outcomes (i.e., low numbers of positive cases). Therefore, as the prediction window width (and number of positive cases) increased, PPV naturally improved. PPV can also be improved by increasing the decision threshold -- as the threshold increases, the model needs to be "more confident" that a positive prediction represents a true positive, increasing PPV but decreasing sensitivity (see the Precision-Recall curves in Figure 2 for a depiction of this trade off). 

High performance of prediction models is only beneficial if the models are generalizable. Our study was the first to employ grouped, nested, k-fold cross-validation for generating the reported model performances. This design provided an unbiased estimate of model performance by separating hyperparameter tuning and model selection processes from model performance estimation. Furthermore, it grouped an individual's data so that it was only used in model selection or evaluation for a given fold. Thus, our methods provide the most rigorous test of any EMA lapse prediction model to date.

We built upon additional gaps in existing EMA lapse<!--KW: these gaps are related to alcohol use prediction not necessarily alcohol lapses. Should we clarify this somehow? Especially since the 2nd gap we point to is we are predicting lapses.--> prediction literature to further increase the generalizability of our models. First, our models were trained on a large, treatment-seeking sample of adults in early recovery from AUD that more closely matches the individuals most likely to benefit from a risk prediction model (vs non-treatment-seeking samples). Second, we specifically predicted episodes of goal-inconsistent alcohol use (i.e., *lapses*) because factors predicting goal-inconsistent use likely differ substantially from factors that predict other types of alcohol use (e.g., episodes of binge drinking among college students, intentional instances of drinking among people pursuing harm reduction goals). This relatively homogeneous prediction outcome likely increased the predictive ability of our model. Finally, we collected data from participants with high frequency (4x per day) over a clinically meaningful period (up to three months). This sampling density allowed us not only to capture changes in dynamic risk factors with high precision but also to build models whose prediction windows updated frequently. Altogether, these specifications help to maximize both the predictive ability and eventual clinical utility of our models.

## Understanding & Contextualizing Model Performance
Remaining analyses explored the feature categories driving our models' performance. Some feature categories consistently emerged as being globally important (i.e., mean |Shapley value| across all observations) across our week, day, and hour level models. Unsurprisingly, the largest contribution to prediction of a lapse was past use (i.e., lapses). An individual who reported lapsing frequently was more likely to lapse at any given observation in the future. This is consistent with decades of research on relapse precipitants and our understanding of human behavior more generally (i.e., past behavior predicts future behavior) [@marlattRelapsePreventionMaintenance1985; @hogstrombrandtPredictionSingleEpisodes1999].  Additionally, congruent with the relapse prevention literature, we also saw an increased likelihood of lapse when participants reported lower ratings of confidence in their ability to maintain their goal of abstinence (i.e., lower future efficacy).

Some differences emerged in the global importance of time-varying feature categories across models due to the varying temporal precision of our prediction window widths. For example, our hour level model ranked the time and day of week of the start of the prediction window as important features contributing to lapse. However, in our day level models only the day of week emerged as being important, and week level models were unable to capture enough variance in either feature. Additionally, most of the more punctuate, time varying constructs (e.g., craving, past stressful events, arousal) appear to have greater impact within the next hour prediction model. These temporal differences may have contributed to our hour level model outperforming our day and week level models.

Importantly, non-aggregated Shapley values for individual observations (i.e., local importance) shed light on the multidimensional and heterogeneous nature of lapse and relapse in our sample. Sina plots in Figure 3 displayed much variance in what feature categories were locally important for a specific observation. For example, some feature categories with low global importance (e.g., past pleasant events, future stressful events) still impacted lapse probability for specific individuals at specific times.

The demographic variables included in our models did not yield high global or local importance. Despite our data having wide representation with respect to socioeconomic status, gender, and age, these features did not significantly contribute to the lapse prediction. While this does not rule out these features' predictive utility, it does suggest that other EMA feature categories (e.g., past use, future efficacy, craving) may be more relevant for lapse prediction than these characteristics. Race and ethnicity also did not emerge as globally or locally important features. However, the limited representation of Black and Brown participants in our sample warrants caution in drawing conclusions about the predictive utility of race and ethnicity. Data collection is underway for a related project in our laboratory to build a lapse risk prediction model for individuals with opioid use disorder. Participants are being recruited nationally with the explicit goal of improving geographic, racial, and ethnic diversity to match national population data.

## Considerations for Clinical Implementation

The aim of the current project was to build models for alcohol lapse risk prediction using EMA data. Although this is a necessary first step, the ultimate goal of this line of work is to use these models clinically. We believe these models may be most effective when embedded in a digital therapeutic platform for reasons of access, availability, and affordability described previously. Below we describe potential considerations for clinical implementation of our model predictions.

The primary output of our model is the likelihood of lapsing for an individual within the next week, day, or hour. This probability could in turn be used to recommend actions that an individual could take to reduce their risk of a lapse. In this situation, a model like ours could communicate an actionable treatment recommendation to the individual based on their lapse risk and the locally important feature categories contributing to that risk (e.g., recommending an urge surfing activity in response to reported high cravings). Importantly, many of our features with high local and/or global importance align well with the risk factors and associated intervention strategies delineated in Marlatt's Relapse Prevention model. This means recommendations could be mapped onto existing therapeutic frameworks already shown to be effective for AUD (e.g., CBT, mindfulness-based relapse). Thus, prediction models can optimize digital therapeutics such that these personalized treatment goals are realized while capitalizing on the benefits of existing self-guided digital therapeutics (e.g., reaching people not connected with a treatment provider, around-the-clock availability). 

Prediction models can also be tailored to an individual user or their context with respect to decision thresholds. Some contexts support raising the decision threshold to improve PPV while reducing sensitivity (i.e., increasing confidence in lapse predictions while also increasing the chance of a "missed" lapse). For example, an individual using the app may wish to know that if they receive an alert, they can trust it. Conversely, some contexts support lowering the decision threshold to avoid missing true lapses. Modules recommended in a digital therapeutic platform in response to a lapse risk alert are likely to benefit an individual whether they are truly at risk or not. For example, completing a mindfulness meditation activity in response to a risk alert is likely to benefit the individual whether this was a true prediction or a "false positive." There is a high personal, health, and economic cost when an individual returns to harmful alcohol use; thus, avoiding misses may be safest in some contexts. Overall, a recommendation-guided digital therapeutic could adjust the decision threshold based on overall costs of a recommendation (e.g., sending a reminder to check in on their recovery goals vs. reaching out to a supportive friend or family member), the availability of resources, and the user's own preference.

## Future directions

A primary future direction for this research is the eventual clinical implementation of a lapse risk prediction model into a digital therapeutic platform. Future research is needed to determine the optimal way to integrate output from these types of algorithms into treatment. Although our project served to increase substantially the generalizability of existing EMA-guided lapse prediction, several limitations continue to hinder generalizability including low representation of Black and Brown individuals and unknown assessment of engagement after three months. To address these limitations, data collection efforts are currently underway in our laboratory for a nationally recruited sample of individuals with opioid use disorder. These individuals will participate for a full year. We will compare completion and attrition rates across these projects to explore how burden may change as duration increases (though with the confound of different substance-using populations). 

Our prediction models were trained using a clinical sample with a goal of abstinence. This sample set our study apart from previous work that has examined other drinking outcomes among non-clinical samples (e.g., binge drinking among college students). Many individuals in recovery do pursue abstinence goals; however, others prefer moderation or harm reduction goals. The factors that predict lapses among individuals pursuing abstinence may differ from the factors that predict alcohol use among individuals pursuing moderation goals. It is likely that this model-building approach can be adapted for other recovery goals (e.g., moderation); however, alcohol use would need to be differentiated as goal-consistent use (e.g., having one drink) and goal-inconsistent use (e.g., exceeding intended frequency or amount of use).

In the current study, we built features from EMA surveys, which are considered *active* sensing because they require action on the part of the individual. In a previous paper from our laboratory using these data, we examined the burden of providing data for this project [@wyantAcceptabilityPersonalSensing2023]. Overall, individuals found the burden to be acceptable and reported being (hypothetically) willing to continue providing data for a full year. Despite these promising results, uncertainty remains as to how long this response rate could be maintained. The chronic, relapsing nature of AUD requires lifelong monitoring. A risk prediction model embedded in a digital therapeutic could shoulder this burden, but only if an individual could provide data long-term - and potentially indefinitely. 

Several options exist to explore reducing assessment burden. First, the current study uses all four daily EMA surveys for risk prediction. All four EMAs contain the same seven questions, but the morning EMA includes three additional questions asked only once daily. A future project could examine how well models perform using only the morning EMA survey, providing an estimate of how well we might predict using lower-burden data collection of only 1X daily EMA. Second, although the current study uses only active EMA and demographic characteristics as feature categories, the broader parent project collected many other *passive* sensing data streams such as geolocation, cellular communication logs, and text message content. Future projects will determine the predictive utility of these passive data streams, both on their own and in conjunction with reduced actively sensed feature categories, to understand whether burden could be lowered in these ways. Third, although it cannot be addressed with the current data, future projects could explore adaptations for long-term data collection. The sampling density of actively sensed signals could be reduced as individuals enter sustained recovery/remission. Active sensing could be adapted on an individual basis to focus on features that emerge as locally important for that individual, reducing the number of items assessed regularly. These and other opportunities for adaptation may improve long-term engagement, but future research will be needed to test these ideas and explore any impact on prediction accuracy or other outcomes. 

A final future direction concerns model lag time. Our models are currently zero lag models, which means a predicted lapse might occur anytime in the prediction window. This method uses all data collected up until the prediction window onset. Zero lag is practical for an hour level model where we might consider just-in-time interventions (i.e., interventions deployed in response to very recent changes in proximal risk factors). However, zero lag is less well-suited to longer prediction window widths like the week level model. Our week level model indicates whether an individual will lapse anytime in the next week beginning right now; it does not indicate an individual will lapse a week from now. Recommendations that take time to implement, such as making an appointment with a therapist, would not be helpful for next-hour risk; however, it could be helpful if they were at risk of lapsing a week into the future. We plan to explore how models with different combinations of prediction window widths and lag times differ with respect to performance and potential clinical utility.

  
    
\newpage

# References
