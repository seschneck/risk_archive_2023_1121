---
title: "Untitled"
format: pdf
csl: https://raw.githubusercontent.com/jjcurtin/lab_support/main/rmd_templates/csl/elsevier-vancouver.csl
geometry: margin=.5in
fontsize: 11pt
bibliography: paper_ema.bib
editor_options: 
  chunk_output_type: console
---



<!--General notes
Considering American Journal of Psychiatry
https://ajp.psychiatryonline.org/ajp_ifora

ARTICLES
Articles are reports of original work that embodies scientific excellence in psychiatric medicine and advances in clinical research. Typically, articles will contain new data derived from a sizable series of patients or subjects. The text is usually within 3,500 words, which does not include an abstract of no more than 250 words, a maximum of 5 tables and figures (total), and up to 40 references. Word count includes only the main body of text (i.e., not tables, figures, abstracts or references). Additional tables can be submitted in a separate file as supplemental data for posting online. (See Supplemental Data for what types of data and formats are acceptable for posting online.)
-->

```{r knitr_settings, include = FALSE}
# settings
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = FALSE, 
                      message = FALSE)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(fig.pos = "ht", out.extra = "")
```

```{r setup, include = FALSE}
library(knitr)
# library(yardstick) # for roc_curve
library(kableExtra)
library(janitor)
# library(corx)
library(patchwork)
library(ggtext)
library(consort)
library(tidyverse)
library(tidymodels)
library(tidyposterior)
library(cowplot)

theme_set(theme_classic()) 
```


```{r paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- "P:/studydata/risk/chtc/ema"
          path_processed <- "P:/studydata/risk/data_processed/ema"
          path_models <- "P:/studydata/risk/models/ema"
          path_shared <- "P:/studydata/risk/data_processed/shared"},

        # IOS paths
        Darwin = {
          path_input <- "/Volumes/private/studydata/risk/chtc/ema"
          path_processed <- "/Volumes/private/studydata/risk/data_processed/ema"
          path_models <- "/Volumes/private/studydata/risk/models/ema"
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"},
        
        # Linux paths
        Linux = {
          path_input <- "~/mnt/private/studydata/risk/chtc/ema"
          path_processed <- "~/mnt/private/studydata/risk/data_processed/ema"
          path_models <- "~/mnt/private/studydata/risk/models/ema"
          path_shared <- "~/mnt/private/studydata/risk/data_processed/shared"}
        )
```

```{r load_data}
# Table data
disposition <- read_csv(file.path(path_processed, "disposition.csv"), 
                        col_types = "ccDDcccccccccc")
screen <- read_csv(file.path(path_shared, "screen.csv"), 
                   col_types = vroom::cols()) |>
  filter(subid %in% subset(disposition, analysis == "yes")$subid)

lapses <- read_csv(file.path(path_shared, "lapses.csv"), col_types = cols()) |>
  filter(exclude == FALSE)

# Predictions data
preds_week<- read_rds(file.path(path_models, "outer_preds_1week_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)
preds_day<- read_rds(file.path(path_models, "outer_preds_1day_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)
preds_hour<- read_rds(file.path(path_models, "outer_preds_1hour_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)

auc_week <- read_rds(file.path(path_models, "outer_metrics_1week_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))
auc_day <- read_rds(file.path(path_models, "outer_metrics_1day_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))
auc_hour <- read_rds(file.path(path_models, "outer_metrics_1hour_0_v5_nested.rds")) |> 
  select(outer_split_num, contains("auc"))

# posterior probabilities
pp <- read_rds(file.path(path_models, "posteriors_all_0_v5_nested.rds"))

# ROC curves
roc_week <- preds_week %>% 
  roc_curve(prob, truth = label) %>% 
  mutate(model = "1week")

roc_day <- preds_day %>% 
  roc_curve(prob, truth = label) %>% 
  mutate(model = "1day")

roc_hour <- preds_hour%>% 
  roc_curve(prob, truth = label) %>% 
  mutate(model = "1hour")

roc_all <- roc_week %>% 
  bind_rows(roc_day) %>% 
  bind_rows(roc_hour)

# PR curves
pr_week <- preds_week %>% 
  pr_curve(prob, truth = label) %>% 
  mutate(model = "1week")

pr_day <- preds_day %>% 
  pr_curve(prob, truth = label) %>% 
  mutate(model = "1day")

pr_hour <- preds_hour%>% 
  pr_curve(prob, truth = label) %>% 
  mutate(model = "1hour")

pr_all <- pr_week %>% 
  bind_rows(pr_day) %>% 
  bind_rows(pr_hour)

# UPDATE - JJC
# Local and Global Grouped SHAPS
shap_local_week <- read_rds(file.path(path_models, "outer_shapsgrp_1week_0_v5_nested.rds")) 
shap_local_day <- read_rds(file.path(path_models, "outer_shapsgrp_1day_0_v5_nested.rds"))
shap_local_hour <- read_rds(file.path(path_models, "outer_shapsgrp_1hour_0_v5_nested.rds")) 

shap_global_week <- shap_local_week %>% 
  group_by(variable_grp) %>% 
  summarize(mean_value = mean(abs(value)), .groups = "drop") %>% 
  arrange(mean_value) |> 
  mutate(model = "Week")
shap_global_day <- shap_local_day %>% 
  group_by(variable_grp) %>% 
  summarize(mean_value = mean(abs(value)), .groups = "drop") %>% 
  arrange(mean_value) |> 
  mutate(model = "Day")
shap_global_hour <- shap_local_hour %>% 
  group_by(variable_grp) %>% 
  summarize(mean_value = mean(abs(value)), .groups = "drop") %>% 
  arrange(mean_value) |> 
  mutate(model = "Hour")

shap_global_all <- shap_global_week |> 
  bind_rows(shap_global_day) |> 
  bind_rows(shap_global_hour) |> 
  mutate(model = factor(model, levels = c("Week", "Day", "Hour"))) %>% 
  mutate(variable_grp = factor(variable_grp, levels = c("past use (EMA item)", 
                                          "craving (EMA item)", 
                                          "past risky situation (EMA item)", 
                                          "past stressful event (EMA item)", 
                                          "past pleasant event (EMA item)", 
                                          "valence (EMA item)", 
                                          "arousal (EMA item)", 
                                          "future risky situation (EMA item)", 
                                          "future stressful event (EMA item)", 
                                          "future efficacy (EMA item)",
                                          "lapse day (other)",
                                          "lapse hour (other)",
                                          "missing surveys (other)",
                                          "age (demographic)",
                                          "sex (demographic)",
                                          "race (demographic)",
                                          "marital (demographic)",
                                          "education (demographic)")))

```

# Introduction

<!--GEF overarching notes

i have added definitions of relapse (a goal-inconsistent return to substance use) and lapse (a single instance of goal-inconsistent substance use) the first time these terms are used per our group discussion. i will say that we seem to be using the terms somewhat interchangeably, which is perhaps ok until we start talking about our study, which should be specifically and exclusively about LAPSE risk. something to monitor in a final read.

i included potential subheadings in comments within the intro if we want to use them-->
 
Alcohol and other substance use disorders (SUDs) are highly prevalent and costly.  In 2019, the National Survey on Drug Use and Health estimated that over 20 million adults in the United States had some form of active SUD within that year [@substanceabuseandmentalhealthservicesadministrationKeySubstanceUse2020]. Nearly 15 million of those adults had an active alcohol use disorder (AUD) [@samhsa2019NationalSurvey; @samhsa2019NationalSurveyb].<!--KW: 2021 numbers are quite different when I compare the tables side by side. Past year SUD jumps to over 40 million, with nearly 30 million having AUD. There is a note in the table that says "Estimates from years prior to 2021 are not shown because of methodological changes in 2021" which likely explains this discrepancy. Should we use these more recent numbers?--> Furthermore, a substantial 25.8% of U.S. adults reported engaging in hazardous alcohol misuse within the past month <!--KW: 23.3% is 2021 estimate. Also note for context both of these estimates are reports of past month binge drinking--> [@samhsa2019NationalSurveya]. Alcohol ranks as the third leading preventable cause of death, accounting for approximately 140,000 fatalities per year [@centersfordiseasecontrolandpreventionAlcoholPublicHealth; @esserEstimatedDeathsAttributable2022]. In economic terms, the US Surgeon General disclosed that alcohol misuse cost the United States $249 billion in 2016 alone [@administrationusFacingAddictionAmerica2016].

Existing clinician-delivered treatments for AUD such as cognitive-behavioral therapy [@mchughCognitiveBehavioralTherapySubstance2010; @lieseCognitiveBehavioralTherapyAddictive2022], mindfulness-based relapse prevention [@bowenMindfulnessBasedRelapsePrevention2021; @goldbergMindfulnessbasedInterventionsPsychiatric2018], motivational interviewing [@millerMotivationalInterviewingHelping2012], and contingency management [@bigelowTheoreticalEmpiricalFoundations1999; @dutraMetaAnalyticReviewPsychosocial2008<!--can cut some refs eventually-->] are effective when provided to patients.  Unfortunately, fewer than 1 in 13 adults with an active AUD in 2019 received any treatment[@samhsa2019NationalSurveyc]<!--KW: in 2021 went from 7.5 to 4.6% (fewer than 1 in 20)-->.  Even more concerning, the failure to access treatment is associated with demographic factors including race, ethnicity, geographic region, and socioeconomic status, which further increase mental health disparities [@wangFailureDelayInitial2005; @MentalHealthReport1999; @generalusMentalHealthCulture2001; @mauraMentalHealthDisparities2017; @novakChangesHealthInsurance2018]. This treatment gap and associated disparities stem from well-known barriers to receiving clinician-delivered mental healthcare related to their affordability, accessibility, availability, and acceptability[@jacobsonUsingDigitalTherapeutics2023].

<!-- ## Digital Therapeutics Can Address Treatment Barriers -->

In recent years, digital therapeutics have emerged as an alternative method to deliver evidence-based treatments and other support to patients either independently or in conjunction with medications or clinician-administered mental healthcare[@jacobsonDigitalTherapeuticsMental2022 <!--Can re-use this reference later rather than chapters if needed to get cites <= 40-->]. Digital therapeutics are web-based or smartphone "apps" that are used to prevent, treat, or manage a medical disorder including AUD or other mental illnesses.  Several large randomized controlled trials have confirmed that digital therapeutics for alcohol or other SUDs improve clinical outcomes (e.g., abstinence, heavy drinking days[@gustafsonSmartphoneApplicationSupport2014; @campbellInternetdeliveredTreatmentSubstance2014]; see Campbell et al. [@campbellFirstWaveScalable2023] for review). They also may enhance treatment retention, reduce readmissions, and support engagement with medication-assisted treatments [@bottsMPOWERProjectResults2017; @japuntichSmokingCessationInternet2006; @pattenRandomizedClinicalTrial2006; @campbellInternetdeliveredTreatmentSubstance2014].

Digital therapeutics can mitigate or eliminate many of the affordability, accessibility, availability, and acceptability barriers associated with in-person, clinician-delivered mental healthcare because they are typically provided to patients on their smartphones[@jacobsonUsingDigitalTherapeutics2023].  Recent Pew Research Center survey data indicate high rates of smartphone ownership among US adults (approximately 85% in April 2021), with minimal variation across race, ethnicity, socioeconomic status, and geographic settings (e.g., urban, suburban, rural) [@pewresearchcenterMobileFactSheet2021]. Moreover, individuals with SUDs generally exhibit high rates of mobile technology use [@collinsFactorsAssociatedPatterns2016]. Consequently, digital therapeutics can offer highly scalable, on-demand therapeutic support that is accessible whenever and wherever it is needed most. 

<!--KW: Just noting the back and forth of focus on SUD, AUD, and mental health broadly in the intro. Since this was an issue with the burden reviewers we might want to consider whether we want to constrain our focus to SUD/AUD or even just AUD. -->


<!-- ## Improving Digital Therapeutics -->

Despite the documented clinical and other benefits of digital therapeutics, their full potential has not yet been realized because patients do not use them optimally.  Patients often don't engage with them as developers intended, and long-term engagement may not be sustained or matched to patients' needs [@hatchExpertConsensusSurvey2018; @lattieDigitalMentalHealth2019; @ngUserEngagementMental2019; @yeagerIfWeBuild2018].  The substantial benefits of digital therapeutics come from easy, 24/7 access to their many modules - their treatments, tools, and other support services.  However, this benefit also presents patients with a challenge.  They may be uncertain about when to use the app, which of the numerous modules within the app best suit their needs, and more specifically, which modules are most appropriate for their current situation.

Identifying these most appropriate modules can be difficult because of the dynamic nature of recovery from AUD over time. AUD is a chronic, relapsing disorder [@brandonRelapseRelapsePrevention2007; @witkiewitzModelingComplexityPosttreatment2007; @mclellanDrugDependenceChronic2000]<!--GEF: i moved the final clause of the previous paragraph into the beginning of this paragraph and split the topic sentence into two. didn't move any refs, so may need to move some refs to first sentence if they are about dynamic nature of recovery from AUD rather than AUD being a chronic relapsing disorder -->. Numerous risk and protective factors interact in complex, non-linear ways to influence the probability, timing, and severity of relapse (i.e., a goal-inconsistent return to substance use <!--KW: should we make this definition specific to alcohol use since this paragraph is now focussed on AUD?-->) [@hendershotRelapsePreventionAddictive2011; @witkiewitzRelapsePreventionAlcohol2004; @huffordRelapseNonlinearDynamic2003; @witkiewitzNonnormalityDivergencePosttreatment2007; @witkiewitzModelingComplexityPosttreatment2007]. Many of these factors are transient, leading to fluctuating relapse risk.  Factors such as urges, mood, lifestyle imbalances, self-efficacy, and motivation can all vary over time.  Social networks may evolve to be more protective or risky, and high risk situations can arise unexpectedly.

Clinical observations and research indicate that successful recovery necessitates life-long monitoring [@hendershotRelapsePreventionAddictive2011; @brandonRelapseRelapsePrevention2007; @huffordRelapseNonlinearDynamic2003; @witkiewitzTherapistGuideEvidenceBased2007; @witkiewitzModelingComplexityPosttreatment2007]. Continuous monitoring of risk for lapse (i.e., a single instance of goal-inconsistent substance use <!--KW: alcohol use? drinking?-->) and its contributing factors, if feasible, would enable patients to adapt their lifestyle, behaviors, and supports to their changing needs. In the context of digital therapeutic use, successful monitoring could direct patients to engage with the most appropriate specific modules in an app, addressing the unique risks present at any given moment throughout their recovery. This guided, adaptive engagement could potentially enhance the app's effectiveness. Thus far, however, the continuous, multi-factor monitoring required for this guidance has proved challenging due to the dynamic and complex interplay of various factors over time. <!--GEF: this final sentence was originally part of the first sentence; i felt it flowed better to talk about benefits and then end with not being able to do it yet since we then move into "but now we can!". didn't move any refs though so check if some from first sentence need to go here -->

<!--relevant review: mohrPersonalSensingUnderstanding2017-->
<!--KW: I think this paragraph below feels a little abrupt here. It might flow better if a subheading separates the above paragraph and this one? Also noting we are zooming out to mental health broadly again.--> 
Moment-by-moment personal sensing of intra- and interpersonal risk factors to support both long-term monitoring and forecasting of mental health functioning is now feasible[@epsteinPredictionStressDrug2020; @suchtingUsingElasticNet2019; @hebertPredictingFirstSmoking2021a; @engelhardPredictingSmokingEvents2018; @businelleUsingIntensiveLongitudinal2016; @soysterPooledPersonspecificMachine2022; @hebertEcologicalMomentaryIntervention2018; @moshontzProspectivePredictionLapses2021; @wyantAcceptabilityPersonalSensing2022; @chihPredictiveModelingAddiction2014; @baeMobilePhoneSensors2018]. Mohr et al. [@mohrPersonalSensingUnderstanding2017] define personal sensing as "collecting and analyzing raw data from sensors embedded in the context of daily life with the aim of identifying human behaviors, thoughts, feelings, and traits" <!--GEF: need a page number with a direct quotation-->. The widespread proliferation of smartphones has made personal sensing both powerful and practical.  Smartphones can be used for active sensing such as ecological momentary assessment (EMA; i.e., repeated, brief self-reports) and also passive, continuous sensing of geolocation, cellular communications (e.g., phone calls and text messages), activity level, sleep, and other raw signals that can predict meaningful clinical outcomes. 

<!-- ## EMA: A Powerful Sensing Tool -->

The current project focuses explicitly on the use of EMA to monitor lapse risk for both practical and strategic reasons. EMA can be easily implemented with only a smartphone; no additional hardware is necessary.  Moreover, comparable raw data (items and responses) can be used consistently across different hardware, operating systems, and sensing apps.  For these reasons, EMA can be incorporated essentially identically into any existing or future smartphone-based digital therapeutic.  EMA can support frequent, in situ longitudinal measurement that will likely be necessary for monitoring episodic or otherwise fluctuating relapse risk.  Long-term monitoring with EMA has been well-tolerated by individuals with SUD [@wyantAcceptabilityPersonalSensing2022; @moshontzProspectivePredictionLapses2021].  Previous research has validated the use of EMA to measure theoretically implicated risk and protective factors for relapse including craving [@dulinSmartphoneBasedMomentaryInterventionCraving2017], mood [@russellAffectRelativeDayLevelDrinking2020], stressors[@wemmDayByDayStressAlcohol], positive life events[@dvorakTensionReductionAffectReg2018], and motivation/efficacy [@dvorakEMAAcuteAUDSymptoms2014].  Furthermore, EMA provides privileged access to many of these more subjective constructs that may be difficult to quantify reliably through other sensing methods.  

Preliminary research is now emerging that uses EMA as features in machine learning models to predict the probability of future alcohol lapses or related outcomes [<!--Sarah - insert  refs-->].  This research is potentially important because it rigorously establishes temporal ordering between the predictors (features engineered from EMAs) and the outcomes. Additionally, the models are evaluated on data from participants that were not used to train the models, thus prioritizing model generalizability. Both these criteria are necessary to develop models that can be clinically useful to truly predict future lapses when implemented with new patients rather than simply confirm associations between putative risk factors and alcohol lapses among existing research participants.  

Despite this initial promise, several important gaps in this preliminary research restrict its generalizability and utility.  First, there are limitations regarding the samples used to train and evaluate these models.  Prediction models developed with samples such as college students or non-treatment-seeking hazardous drinking populations [e.g., <!--Sarah - insert college student or other convenience samples-->] provide important proof of concept; however, they may not generalize well to clinical samples of patients with AUD.  Similarly, models that have been developed to predict alcohol use in non-treatment seeking patients with AUD may be less clinically useful.  These individuals are unlikely to use digital therapeutics to support their recovery until they begin to contemplate and/or commit to behavior change regarding their alcohol use [<!--JJC to insert stage of change reference-->].  Moreover, features that predict planned or otherwise intentional alcohol use among individuals that are not motivated to change their behavior may not generalize to predict goal-inconsistent alcohol use (i.e., lapses) among patients in recovery.  

<!--Sarah - what about models predicting outcomes other than lapse (e.g., craving, stress)?  What can you tell us about those studies?

Added: Limited to studies that use EMA as a predictor, target SUD related outcomes (some smoking related...), utilized machine learning methods-->

<!--JOHN: review following paragraph re: including non-alcohol studies -->

A second limitation concerns the types of outcomes models are trained to predict. A handful of studies have trained models to predict precursors of substance use, such as craving [@burgess-hullTrajectoriesCravingMedicationassisted2022; @dumortierClassifyingSmokingUrges2016] and stress (Sarker et al., 2016). Although craving and stress are strongly associated with substance use, they do not reliably predict instances of substance use, limiting their utility for identifying specific instances for substance use treatment <!--GEF: probably need citations to make this claim, or make clear that the citations in the previous sentence are somehow supporting this-->. Moreover, due to the increased frequency of craving and stress events relative to substance use episodes, higher model performance can be achieved with less effort [@epsteinPredictionStressDrug2020] <!--GEF: "with less effort"?... is this about outcome class distribution? increased overall sample size? not quite sure what this means-->. Other studies have predicted temporally coarse clinical outcomes such as binary SUD treatment success [@acionUseMachineLearning2017; @coughlinMachineLearningApproachPredicting2020] or probability of treatment dropout for individuals in a medication-assisted treatment program for opioid use disorder [@gottliebMachineLearningPredicting2022]. These prediction models may be useful for identifying individuals who would benefit from increased SUD treatment; however, they do not provide enough temporal specificity to assist in identifying when resources should be directed to these individuals. <!--GEF: Sarah - I did my best to find the references you were citing & add them to Zotero; please double check that I found the studies you were thinking of! I was too uncertain about the Sarker 2016 paper to make a guess on that one.-->

<!--GEF: removed these two sentences from previous & just added citations to the second sentence - maintaining in case this level of detail is desired
For example, EMA has been demonstrated to predict levels of craving in individuals attempting to quit smoking (Durmortier et al 2016) as well as individuals in medication-assisted treatment programs for opioid use disorder (Burguess-Hall et al 2022). EMA combined with physiological data yielded prospective prediction of stressful event episodes in a small sample of individuals endorsing polysubstance use (Sarker et al 2016). 
-->


<!--JJC will justify lapse after we have above paragraph out other outcomes drafted.  - temporally precise, clearly defined, necessary for relapse, often early warning, can produce AVEs which can further undermine recovery. Abstinence is the FDA indicated outcome for clinical trials.  Describe here or in the current study below

GEF: took a stab at this below. My vote is for the paragraph to go here as I think it provides a good go-between from the previous paragraph about less than ideal outcomes to the next paragraph about the "exemplary" study that used lapses as outcome among other benefits.
-->

Predicting alcohol lapses (i.e., single instances of goal-inconsistent alcohol use) may be preferred for several reasons. First, a lapse is a drinking behavior rather than an associated factor or related behavior (e.g., craving, stress). Second, lapses are clearly defined and temporally precise. They have a specific onset and offset whose exact timing can be recorded; the lapse begins when drinking starts, and it ends when drinking stops. Lapses contrast with an outcome like relapse (i.e., returning to goal-inconsistent alcohol use), which cannot be clearly defined - it is unclear when a lapse becomes a relapse. However, the tight connection between lapses and relapses offers a third advantage in that a lapse is necessary for a relapse. Thus, lapses represent outcomes that precede relapses and may serve as early warnings for relapse. By building models that predict lapses, we position ourselves to assess risk for an event on which we can intervene before relapse. Fourth, lapses can produce adverse effects that could undermine recovery, making them clinically meaningful events that individuals pursuing recovery may wish to avoid. Finally, lapses represent events that are inconsistent with an abstinence goal. Abstinence not only remains a common goal among individuals in recovery from AUD but also is an FDA-indicated outcome for clinical trials. 

An early but exemplary prediction model developed by Gustafson et al. [@chihPredictiveModelingAddiction2014] provided the foundation on which our current project builds.  Participants enrolled in their study as they completed a residential treatment program for AUD.  Alcohol lapses were recorded for 8 months following program discharge while they received continuing care through the use of a digital therapeutic for AUD (A-CHESS). As such, the model was developed to predict a clinically meaningful outcome using individuals targeted for digital therapeutic use (i.e., clinical sample committed to AUD recovery-related behavior change).  However, the temporal precision for both the machine learning features and alcohol lapses was coarse because they were measured through a weekly survey.  Therefore, model predictions updated only once per week at best, and lapse onsets could occur anytime within the next two weeks.  This coarseness limits the utility of the model for "just-in-time" micro-interventions (e.g., guided mindfulness or other stress reduction techniques, urge surfing) that are well-suited to digital therapeutics but should be used in moments of peak risk. Furthermore, evaluation of this model should be considered both preliminary and optimistically biased because periods with missing surveys were excluded.  

<!-- ## Current Study -->

We designed the current study to address these gaps and limitations of previous machine learning prediction models from Gustafson [@chihPredictiveModelingAddiction2014] and others [<!--insert refs from above-->].  We developed our models using participants in early recovery (1-8 weeks since last alcohol use at study enrollment) from moderate to severe AUD who reported a goal of alcohol abstinence.  Consistent with this goal, these models predict future lapses back to alcohol use (i.e., instances of goal-inconsistent alcohol use).  We developed three separate models that provide hour-by-hour probabilities of a future lapse with increasing temporal precision (i.e., lapses in the next week, next day, and next hour, respectively).  Model features were based on raw and longitudinal change in theoretically-implicated risk factors[<!--insert relapse prevention model cites-->] derived from 4x daily EMAs. This research represents an important step toward the development of a "smart" (machine learning guided) sensing system that can both identify periods of peak lapse risk and recommend specific supports to address factors contributing to this risk.

# Method

## Research Transparency
We value the principles of research transparency that are fundamental to the robustness and replicability of science and took several steps to follow open science guidelines. We reported how we determined our sample size <!--did we?-->, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]<!--this is 21 word solution, do we use that statement?-->. We completed a transparency checklist (see Supplement; [@aczelConsensusbasedTransparencyChecklist2019]<!--have we?-->). Finally, we made the data, analysis scripts, annotated results, questionnaires, and other study materials associated with this report publicly available [<!--insert project OSF link-->]. 

Throughout this project, we iteratively improved the machine learning methods that are rapidly evolving in the social sciences and used in this study. These factors made our analyses inherently exploratory and thus inappropriate for preregistration. However, we restricted many researcher degrees of freedom via cross-validation procedures that can robustly guide decision-making. Replication is built into cross-validation: models are fit using held-in training sets, decisions are made using held-out validation sets, and final model performance is evaluated in a confirmatory manner using held-out test sets.

## Participants
We recruited 151 participants in early recovery (1-8 weeks of abstinence) from AUD in Madison, Wisconsin, USA, to participate in a 3-month longitudinal study. Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required that participants:

1.  were 18 years of age or older,
2.  were able to write and read in English,
3.  had at least moderate AUD (\>= 4 DSM-5 symptoms^[We measured DSM-5 symptoms with a self-report survey administered to participants during the in-person screening visit.]),
4.  were abstinent from alcohol for at least 1 week but no longer than 2 months,
5.  were willing to use a single smartphone (their personal phone or one provided by us) while enrolled in the study.

We also excluded participants if they exhibited severe symptoms of psychosis or paranoia^[Psychosis and paranoia were defined as scores greater than 2.2 or 2.8, respectively, on the psychosis or paranoia scales of the on the Symptom Checklist â€“ 90 (SCL-90) [@derogatisSCL90OutpatientPsychiatric1973].]. 

## Procedure
Participants completed five study visits over approximately three months. After an initial phone screen, participants attended an in-person screening visit where we determined eligibility, obtained informed consent, and collected self-report measures of individual differences (e.g., demographics, mental health, and alcohol use history; see Measures below). Eligible and consented participants returned approximately one week later to enroll in the study at an intake visit. Three additional follow-up visits occurred about every 30 days participants were on study. At each follow-up visit, we collected additional self-report and interview measures. 

For the entire duration on study, participants were expected to complete EMAs four times each day. Other personal sensing data streams (geolocation, cellular communications, sleep quality, and audio check-ins) were collected as part of the larger grant's aims (R01 AA024391). A full description of the procedure and data collected at each visit can be found at the study's Open Science Framework (OSF) page [<!--Insert link here-->]. All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

## Measures
### EMA
<!--citation for validity of self-reported alcohol use: https://pubmed.ncbi.nlm.nih.gov/26160523/-->
Participants completed a brief (7-10 questions) EMA four times each day following pushed text message reminders. These text messages included a link to a Qualtrics survey, optimized for completion on their smartphone. 

All four EMAs included seven items that asked about alcohol use not yet reported, current affective state (pleasantness and arousal), greatest urge to drink alcohol since the last EMA, any pleasant or positive events, any hassles or stressful events, and any exposure to risky situations (i.e., people, places, or things) that occurred since the last EMA. The first EMA each day asked three additional questions about how likely participants were to encounter a risky situation, encounter a stressful event, and drink alcohol in the upcoming week. 

The first and last EMAs of the day were scheduled within one hour of participants' typical wake and sleep times. The other two EMAs were each scheduled randomly within the first and second halves of the participants' typical day. All EMAs were separated from each other by at least one hour.

### Individual Differences
At the screening visit we collected self-report information about demographics (age, sex, race, ethnicity, education, employment, personal income, and marital status) and clinical characteristics (AUD milestones, number of quit attempts, lifetime history of treatment for AUD, lifetime receipt of medication for AUD, DSM-5 AUD symptom count, and current drug use [@whoassistworkinggroupAlcoholSmokingSubstance2002]) to describe our sample. Only age, sex, race, education, and marital status are used as features for our analyses. A full description of all measures collected as part the larger grant are available on the study's OSF page [<!--Insert link here-->].

## Data Analytic Strategy
Data preprocessing and modeling were done in RStudio [@rstudioteamRStudioIntegratedDevelopment2020] using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020].  All models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computer [<!--Center for High Throughput Computing. (2006). Center for High Throughput Computing. doi:10.21231/GNT1-HW21-->]

### Lapse Labels
We created future prediction windows that varied in their temporal precision by using three distinct window widths (i.e., duration between window start and end time): 1 week, 1 day, and 1 hour.  For each participant, the start of the first prediction window for all three widths began at midnight on their second day of participation and ended 1 week, 1 day, or 1 hour later, respectively. By beginning at the end of the second day, we were assured that there would be at least 24 hours of past EMAs to use for future lapse prediction in these first windows.  Subsequent windows were created for all three widths by repeatedly rolling the window start/end forward 1 hour until the end their study participation was reached (i.e., each participant's last prediction window for each width ended at the date and hour of their last recorded EMA). By using windows that rolled by 1 hour for all three window widths, we were able to develop classification models that provided hour-by-hour predictions of future lapse probability for all three window widths.

We labeled each prediction window as "lapse" or "no lapse" using participants' reports from question 1 of the EMA ("Have you drank any alcohol that you have not yet reported?"). If participants answered yes to this question, they were prompted to enter the hour and date of the start and end of the drinking episode.  These reports were also validated by study staff during the monthly visits, and staff probed for any unreported lapses at that time using a timeline follow-back format. <!-- GEF: cite TLFB? -->  

A prediction window was labeled "lapse" if the start date/hour of any drinking episode fell within that window.  A window was labeled as "no lapse" if no alcohol use occurred within 24 hours of the window start/end.  We used this conservative 24 hour fence for labeling windows as "no lapse" to increase the fidelity of these labels.  Thus, if no alcohol use occurred within the window but did occur within the 24 hour fence, the window was excluded. Given that most windows were labeled "no lapse" (i.e., the outcome was highly unbalanced), it was not problematic to exclude some windows to further increase confidence in those labels.  <!-- GEF: it was unclear the connection between the fence and exclusions; i tried to clarify with an additional sentence-->

### Feature Engineering
Features were calculated using only data that were collected prior to the start of each prediction window. This assured that our models were true "prediction" models, making future predictions rather than simply identifying concurrent associations between features and lapses.  

Features were derived from three sources.   The first source of features included demographic characteristics (i.e., age, sex, race, marital status, education) measured at baseline.  The second source of features used previous EMA responses that were collected prior to the start of the associated lapse window.  Using these EMA responses,  we created features using both raw (e.g., min., max., median, most recent response, and total counts) and change (e.g., within-subject baseline comparisons) scores. We scored raw min, max, median, and count features within a small set of periods prior to the start of the prediction window (6, 12, 24, 48, 72, and 168 hours prior to start of window).  We scored change features by subtracting the mean response for each feature over all data prior to the start of the lapse window from the associated raw feature.  The third source of features was based on the day of the week and the time of day (daytime vs. evening/night) of the start of the lapse window. 

Other generic feature engineering steps included 1) imputation for missing data for features (median imputation for numeric features, mode imputation for nominal features); 2) dummy coding for nominal features; and 3) removal of any zero variance features.  Medians/mode for missing data imputation and identification of zero variance features were derived from training (held-in) data and applied to held out (validation and test) data to prevent issues associated with data leakage (see Model Training procedures below). <!--KW: possibly reference supplemental recipe code here-->.  


### Model Training and Evaluation

#### Statistical Algorithm and Hyperparameters
We initially considered four candidate classification statistical algorithms (XGBoost, Random Forest, K-Nearest Neighbors, and Elastic Net) that differed across various characteristics expected to affect model performance (e.g., flexibility, ability to handle higher-order interactions natively, complexity, linear vs. non-linear). These algorithms are well-established with documented good "out of box" performance, and they vary with respect to the degree of feature selection performed automatically during model fitting [@kuhnAppliedPredictiveModeling2018]. However, preliminary exploratory analyses suggested that XGBoost consistently outperformed the other three algorithms.  Furthermore, the calculation of Shapley Additive Explanations (SHAP) values, which we planned to use for explanatory analyses of feature importance, are optimized for XGBoost.  For these reasons, we focused our primary model training and evaluation on the XGBoost algorithm only.  
  
We trained candidate XGBoost model configurations that differed across sensible values for the hyper-parameters mtry, tree depth, and learning rate using grid search.  All configurations used 500 trees combined with early stopping to prevent over-fitting.  All other hyper-parameters were also set to defaults established by tidymodels and xgboost packages in R <!-- GEF: cite packages again here-->.  Candidate model configurations also differed with respect to the outcome resampling method.  Specifically, we used up-sampling and down-sampling of the outcome using majority (no lapse) to minority (lapse) ratios that ranged from 1:1 to 5:1 to address class imbalance (i.e., windows that contained lapses were much less frequent than no lapse windows for all three window widths).  In addition, we calibrated the predicted probabilities from these XGBoost models using the beta distribution to support optimal decision-making under variable outcome class distributions [@kullSigmoidsHowObtain2017]

#### Performance Metric
Our primary performance metric for selecting and evaluating best model configurations was the area under the Receiver Operating Characteristic Curve (auROC)[@kuhnAppliedPredictiveModeling2018]. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case relative to a randomly selected negative case.  An auROC of 0.5 indicates chance performance; an AUC of 1.0 perfectly discriminates between positive and negative outcome classes.  This metric was selected because it 1) combines sensitivity and specificity, which are both important characteristics to consider for clinical implementation; 2) is an aggregate metric across all possible decision thresholds, which is important because the optimal decision threshold may differ depending on setting and goals; and 3) is not affected by class imbalance, which is important when comparing models across the three different prediction window widths that have different levels of imbalance.  

#### Cross-validation 
We used grouped (by participant), nested cross-validation to train models, select best models, and evaluate those best models on auROC.  With grouped cross-validation, all of a participant's data is either held-in or held-out to avoid bias that is introduced when predicting a participant's data from their own data[<!--ref?-->].  

In nested cross-validation, there are two nested loops for dividing and holding out folds: an outer loop with k<sub>outer</sub> folds, where held-out folds serve as test sets for model evaluation; and inner loops with k<sub>inner</sub> folds, where held-out folds serve as validation sets for model selection. The full dataset is divided into k<sub>outer</sub> approximately equal-sized, non-overlapping folds. Each k<sub>outer</sub> fold serves once as an independent test set to evaluate the model configuration trained and selected using the remaining data (i.e., the k<sub>outer</sub> - 1 outer folds). The model configurations are trained and selected by dividing that remaining data into k<sub>inner</sub> folds and performing simple k-fold cross validation (i.e., each k<sub>inner<sub> fold is used once as a held-out validation set for models trained on k<sub>inner<sub> - 1 inner folds). Nested cross-validation maintains separation between training sets (k<sub>inner</sub> - 1 folds), validation sets (k<sub>inner</sub>th held-out fold), and test sets (k<sub>outer</sub>th held-out fold). This separation removes optimization bias from the evaluation of model performance and can also yield lower variance performance estimates than when a single independent test set is used [@jonathanUseCrossvalidationAssess2000].  

In this study, we used 1 repeat of 10-fold cross-validation for the inner loop and 3 repeats of 10-fold cross-validation for the outer loop.  Therefore, best model configurations were selected based on the median auROC across the 10 held-out folds from the inner loop.  Final evaluation of the performance of those best model configurations was based on the median <!--consider footnote for why median--> auROC across the 30 held-out folds in the outer loop.  For completeness, we report both inner loop and outer loop median auROC for our best model configurations for each of the three window widths (week, day, and hour).  In addition, we report key additional performance metrics for the best model configurations from the outer loop including sensitivity, specificity, accuracy, positive predictive value, and negative predictive value. We also display the Receiver Operating Characteristic curves and Precision-Recall Curves by window width.
<!--KW: will cite source for these metrics - tidymodels reference or IAML;  JC says we need only one source - likely ISL because we have to keep our citations to <= 30-->.

### Bayesian Estimation of auROC and Model Comparisons 

We used a Bayesian hierarchical generalized linear model [<!--McElreath, R. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC press. 
-->] to estimate the posterior probability distributions for auROC for the best models for each prediction window width.  These analyses allowed us to calculate 95% credible intervals for the auROC for the three best models and to determine the probability that these models' performance differed systematically from each other with respect to their auROCs.  These analyses were accomplished using the tidyposterior [@kuhnTidyposteriorBayesianAnalysis2022] and rstanarm [@goodrichBayesianAppliedRegression2023] packages in R.  We regressed the auROCs (logit transformed to address bounded, skewed distribution) from the 30 test sets (held-out outer folds) for each model as a function of window width.  Following recommendations from the tidymodels team[@kuhnTidyposteriorBayesianAnalysis2022; @kuhnBayesianAnalysisResampling], we set two random intercepts: one for the repeat, and another for the fold within repeat (folds are nested with repeats for auROCs collected with 3x 10-fold cross-validation).  Following recommendations from the rstanarm team and others [@rstudioteamRStudioIntegratedDevelopment2020; @gabryPriorDistributionsRstanarm2023], we used the rstanarm default autoscaled, weakly informative, data-dependent priors that take into account the order of magnitude of the variables to provide some regularization to stabilize computation and avoid over-fitting.  Specifically, the priors were set as follows: residual standard deviation ~ normal(location=0, scale=exp(2)), intercept (after centering predictors) ~ normal(location=2.3, scale=1.3), the two coefficients for window width contrasts ~ normal (location=0, scale=2.69), and covariance ~ decov(regularization=1, concentration=1, shape=1, scale=1).  We report the 95% (equal-tailed) credible intervals from the posterior probability distributions for auROC for models for each window width.  We also report 95% (equal-tailed) credible intervals for the differences in performance among the three models.  


### Shapley Additive Explanations for Feature Importance

We used the Shapley Additive Explanations (SHAP) method [@lundbergUnifiedApproachInterpreting2017] to provide a consistent and objective explanation of the importance for model predictions associated with categories of features [<!--ref-->] from models for each prediction window width.  SHAP computes Shapley values, which have a solid theoretical foundation in game theory.  The use of SHAP for model interpretation is attractive because Shapley values are model-agnostic (i.e., can be computed for any statistical algorithm) and possess several useful properties including: Additivity (Shapley values for each feature can be computed independently and then summed); Efficiency (the sum of Shapley values across features must add up to the difference between the predicted and observed outcome for each observation); Symmetry (Shapley values for two features should be equal if the two features contribute equally to all possible coalitions); and Dummy (a feature that does not change the predicted value in any coalition will have a Shapley value of 0).  Finally, SHAP has a fast implementation for tree-based models that makes its use computationally feasible for xgboost, even with large sample sizes.  We calculated Shapley values using the held-out folds from 3 repeats of 10-fold cross-validation in the outer loop of our nested cross-validation, using the SHAPforxgboost package that provides Shapley values in log-odds units for binary classification models.  We averaged the three Shapley values for each observation for each feature across the three repeats to increase their stability.  To calculate global importance for categories of features (e.g., all features associated with the EMA urge item), we averaged the absolute value of the Shapley values of all features in the category across all observations.  We report these global importance scores for feature categories described earlier, separated by prediction window width. 



\newpage


# Results

## Demographic and Clinical Characteristics

One hundred ninety-two participants were eligible for enrollment. Of these participants, 191 consented to participate in the study at the screening session and 169 subsequently enrolled in the study at the enrollment visit which occurred approximately one week later. Fifteen participants discontinued prior to the first monthly follow-up visit. 

We excluded data from one participant who did not maintain a goal of abstinence during their participation (i.e., they reported they were uncertain if their goal was abstinence on the daily EMA and monthly follow-up surveys). We also excluded data from two participants who showed evidence of careless responding and unusually low compliance, rendering their lapse labels unusable. Our final sample consisted of 151 participants. These participants provided study measures for one (N = 14), two (N = 6) or three (N = 131) months. Figure S1 presents a CONSORT diagram that displays more detail on enrollment and disposition for all eligible participants. 

The final sample of 151 participants included approximately equal numbers of men (N=77; 49%) and women (N=74; 51%) who ranged in age from 21 - 72 years old.  The sample was majority white (N=131; 87%) and non-Hispanic (N=147; 97%).  Participants self-reported a mean of 8.9 DSM-5 symptoms of alcohol use disorder (SD=5.8; range=4-11), a mean of 5.5 previous quit attempts (SD=5.8, range=0-30), and many reported using medications for AUD (N=59; 39%).  The majority of participants (N=84; 56%) reported one or more alcohol lapses during the study period.  The mean number of lapses per participant during the study period was XX (SD; range). <!--JJC will update previous with table data-->  Table 1 provides more detail on demographic and clinical characteristics of the sample.


<!-- Demographics table-->
```{r}
footnote_table_dem_a <- "N = 151"
footnote_table_dem_b <- "Two participants reported 100 or more quit attempts. We removed these outliers prior to calculating the mean (M), standard deviation (SD), and range."
```

```{r table_demo}
options(knitr.kable.NA = "")

n_total <- 151

dem <- screen %>% 
  summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
            SD = as.character(round(sd(dem_1, na.rm = TRUE), 1)),
            min = as.character(min(dem_1, na.rm = TRUE)),
            max = as.character(max(dem_1, na.rm = TRUE))) %>% 
  mutate(var = "Age",
         n = as.numeric(""),
         perc = as.numeric("")) %>% 
  select(var, n, perc, everything()) %>% 
  full_join(screen %>% 
  select(var = dem_2) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_3) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("American Indian/Alaska Native", "Asian", "Black/African American",
                           "White/Caucasian", "Other/Multiracial")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_4) %>% 
  mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
                         TRUE ~ "Yes"),
         var = fct_relevel(factor(var, c("Yes", "No")))) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_5) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Less than high school or GED degree", "High school or GED", 
                           "Some college", "2-Year degree", "College degree", "Advanced degree")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_6, dem_6_1) %>% 
  mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
                         dem_6_1 == "Part-time" ~ "Employed part-time",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Employed full-time", "Employed part-time", "Full-time student",
                           "Homemaker", "Disabled", "Retired", "Unemployed", 
                           "Temporarily laid off, sick leave, or maternity leave",
                           "Other, not otherwise specified")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  summarise(mean = format(round(mean(dem_7, na.rm = TRUE), 0), big.mark = ","),
            SD = format(round(sd(dem_7, na.rm = TRUE), 0), big.mark = ","),
            min =format(round(min(dem_7, na.rm = TRUE), 0), big.mark = ","),
            max = format(round(max(dem_7, na.rm = TRUE), 0), scientific = FALSE, big.mark = ",")) %>% 
  mutate(var = "Personal Income",
        n = as.numeric(""),
        perc = as.numeric(""),
        mean = str_c("$", as.character(mean)),
        SD = str_c("$", as.character(SD)),
        min = str_c("$", as.character(min)),
        max = as.character(max)) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", "min", "max")) %>% 
  full_join(screen %>% 
  select(var = dem_8) %>% 
  mutate(var = case_when(var == "Never Married" ~ "Never married",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Never married", "Married", "Divorced", "Separated",
                           "Widowed")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))

auh <- screen %>% 
  summarise(mean = mean(auh_1, na.rm = TRUE),
            SD = sd(auh_1, na.rm = TRUE),
            min = min(auh_1, na.rm = TRUE),
            max = max(auh_1, na.rm = TRUE)) %>% 
  mutate(var = "Age of first drink",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()) %>% 
  full_join(screen %>% 
  summarise(mean = mean(auh_2, na.rm = TRUE),
            SD = sd(auh_2, na.rm = TRUE),
            min = min(auh_2, na.rm = TRUE),
            max = max(auh_2, na.rm = TRUE)) %>% 
  mutate(var = "Age of regular drinking",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", 
                                             "min", "max")) %>% 
  full_join(screen %>% 
  summarise(mean = mean(auh_3, na.rm = TRUE),
            SD = sd(auh_3, na.rm = TRUE),
            min = min(auh_3, na.rm = TRUE),
            max = max(auh_3, na.rm = TRUE)) %>% 
  mutate(var = "Age at which drinking became problematic",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  summarise(mean = mean(auh_4, na.rm = TRUE),
            SD = sd(auh_4, na.rm = TRUE),
            min = min(auh_4, na.rm = TRUE),
            max = max(auh_4, na.rm = TRUE)) %>% 
  mutate(var = "Age of first quit attempt",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  # filter out 2 people with 100 and 365 reported quit attempts - will make footnote in table
  filter(auh_5 < 100) %>% 
  summarise(mean = mean(auh_5, na.rm = TRUE),
            SD = sd(auh_5, na.rm = TRUE),
            min = min(auh_5, na.rm = TRUE),
            max = max(auh_5, na.rm = TRUE)) %>% 
  mutate(var = "Number of Quit Attempts*",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  select(var = auh_6_1) %>%
  mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ months)",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_2) %>%
  mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 months)",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_3) %>%
  mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_4) %>%
  mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_5) %>%
  mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_6) %>%
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_7) %>%
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_7) %>% 
  mutate(var = fct_relevel(factor(var, c("Yes", "No")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) %>% 
  rowwise() %>% 
  # calculate dsm5 score by adding up dsm5_1 through dsm5_11
  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7, 
                            dsm5_8, dsm5_9, dsm5_10, dsm5_11))) %>% 
  ungroup() %>% 
  summarise(mean = mean(dsm5_total),
            SD = sd(dsm5_total),
            min = min(dsm5_total, na.rm = TRUE),
            max = max(dsm5_total, na.rm = TRUE)) %>% 
  mutate(var = "DSM-5 Alcohol Use Disorder Symptom Count",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  select(var = assist_2_1) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_2) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Cannabis (marijuana, pot, grass, hash, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_3) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Cocaine (coke, crack, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_4) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_5) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Inhalants (nitrous, glue, petrol, paint thinner, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_6) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_7) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_8) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Opioids (heroin, morphine, methadone, codeine, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100), by = c("var", "n", "perc")) 

lapses_per_subid <- screen %>% 
  select(subid) %>% 
  left_join(lapses %>% 
  tabyl(subid) %>% 
  select(-percent), by = "subid") %>% 
  mutate(n = if_else(is.na(n), 0, n),
         lapse = if_else(n > 0, "yes", "no")) 

lapse_info <- lapses_per_subid %>% 
  group_by(lapse) %>% 
  rename(var = lapse) %>% 
  mutate(var = factor(var, levels = c("yes", "no"), labels = c("Yes", "No"))) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / n_total) * 100,
         mean = NA_real_,
         SD = NA_real_,
         min = NA_real_,
         max = NA_real_) %>% 
  full_join(lapses_per_subid %>% 
  summarise(mean = mean(n),
            SD = sd(n),
            min = min(n),
            max = max(n)) %>% 
  mutate(var = "Number of reported lapses"), 
  by = c("var", "mean", "SD", "min", "max"))
```

<!--UPDATE -JJC handle description of outliers for quit attempts-->
```{r}
# display and format table
dem %>% 
  bind_rows(auh %>% 
              mutate(across(mean:max, ~round(.x, 1))) %>% 
              mutate(across(mean:max, ~as.character(.x)))) %>% 
  bind_rows(lapse_info %>% 
              mutate(across(mean:max, ~round(.x, 1))) %>% 
              mutate(across(mean:max, ~as.character(.x)))) %>% 
  mutate(range = str_c(min, "-", max)) %>%
  select(-c(min, max)) %>% 
  kbl(longtable = TRUE,
      booktabs = TRUE,
      col.names = c("", "N", "%", "M", "SD", "Range"),
      align = c("l", "c", "c", "c", "c", "c"),
      digits = 1,
      caption = "Demographics and Clinical Characteristics for the Sample") %>%
  kable_styling() %>% 
  row_spec(row = 0, align = "c", italic = TRUE) %>% 
  pack_rows("Sex", 2, 3, bold = FALSE) %>% 
  pack_rows("Race", 4, 8, bold = FALSE) %>% 
  pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) %>% 
  pack_rows("Education", 11, 16, bold = FALSE) %>% 
  pack_rows("Employment", 17, 25, bold = FALSE) %>% 
  pack_rows("Marital Status", 27, 31, bold = FALSE) %>% 
  pack_rows("Alcohol Use Disorder Milestones", 32, 35, bold = FALSE) %>% 
  pack_rows("Lifetime History of Treatment (Can choose more than 1)", 37, 43, bold = FALSE) %>% 
  pack_rows("Received Medication for Alcohol Use Disorder", 44, 45, bold = FALSE) %>% 
  pack_rows("Current (Past 3 Month) Drug Use", 47, 54, bold = FALSE) %>% 
  pack_rows("Reported 1 or More Lapse During Study Period", 55, 56, bold = FALSE) %>% 
  footnote(general=footnote_table_dem_a, symbol = c(footnote_table_dem_b), 
           threeparttable = TRUE)
```


\newpage


## EMA compliance, features (predictors), and prediction window labels (outcome)

We used responses from participants' 4x daily EMAs to create datasets of features and labeled outcomes to develop and evaluate three separate classification models to predict future lapses in the next week, the next day, and the next hour, respectively.  EMA compliance over the study was generally good, with participants completing an average of 3.1 (SD=0.6) of the 4 daily EMAs per day or 78% compliance overall.  Participants completed at least 1 EMA on 95% of days.  Across weeks in the study, EMA compliance percentages ranged from 75% - 87% for 4x daily and from 92% - 99% for at least 1 daily EMA.  Figure <!--insert number--> displays compliance percentages over time (by week) across the study period for both 4x daily and at least one daily report.  

Using these EMA reports, we created datasets with 270,081, 274,179, and 267,287 future prediction windows for the outcome that were labeled as lapse or no lapse for the week, day, and hour window widths, respectively.  Each of these datasets also contained 286 features that were derived from EMAs that were completed prior to the start of the associated prediction window, baseline demographic characteristics, and prediction window start dates/times as described earlier in the method. Each of these datasets was unbalanced with respect to the outcome such that lapses were observed in 68467 (25.3%) week windows, 21107 (7.7%) day windows, and 1017 (0.3%) hour windows.  As noted previously, we addressed this outcome class imbalance using up-sampling and down-sampling of the outcome during model training.  

 
## Model Performance

We developed and evaluated separate classification models to predict future lapses in three windows: the next week, the next day, and the next hour.  We used grouped (by participant), nested cross-validation for each of these prediction windows to train models, select best models, and evaluate those best models  This nested cross-validation resampling strategy used 1 repeat of 10-fold cross validation for the inner loop, and 3 repeats of 10-fold cross validation for the outer loop.  Our primary performance metric for model selection and evaluation was the area on the Receiver Operating Characteristic Curve (auROC).  Given this, best models were selected based on the median <!--insert note about why median--> auROC across 10 held-out validation folds in the inner loop.  These best models were evaluated using the median auROC across 30 held-out test folds in the outer loop. Of course, consistent with the nested cross-validation resampling method, validation and test sets were completely independent to remove optimization bias from the performance metrics.


### auROC

Our primary performance metric for model selection and evaluation was the area on the Receiver Operating Characteristic Curve (auROC). Best model configurations were selected using median auROCs from the validation folds in the inner loop of the nested cross-validation.  The median auROCs for the best configurations were `round(auc_week |> pull(roc_auc_in) |> median(), 3)`, `round(auc_day |> pull(roc_auc_in) |> median(), 3)`, and `round(auc_hour |> pull(roc_auc_in) |> median(), 3)`

The median auROC across the 30 test folds was high for the week (<!--AUC; range-->), day (<!--AUC; range-->), and hour (<!--AUC; range-->) prediction windows.  The left panel of figure 1 displays the Receiver Operating Characteristic curves by prediction window derived by aggregating predictions across the 30 held-out test folds.  Figure S2 presents the individual ROC curves for each of the 30 test folds.  

We used the auROCs for the week, day and hour models across the 30 test folds to estimate the posterior probability distribution for the auROC for each model.  These posterior probability distributions are displayed by prediction window in the right panels of figure 1.  

<!--Citation for ROC effect size conversion - https://journals.copmadrid.org/ejpalc/art/ejpalc2018a5 (.58 = small effect size, .69 = medium effect size, .79 = large effect size, corresponding to Cohen's d of .2, .5, .8 respectively).-->

### Other Performance Metrics


### Feature Importance







```{r}
footnote_table_metrics <- "Insert footnote"
```

```{r table_metrics}
j_thres_week <- roc_week |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)

j_thres_day <- roc_day |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)

j_thres_hour <- roc_hour |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)


metrics_week <- preds_week |> 
  mutate(estimate = if_else(prob > j_thres_week, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(week = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics_day <- preds_day |> 
  mutate(estimate = if_else(prob > j_thres_day, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(day = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics_hour <- preds_hour |> 
  mutate(estimate = if_else(prob > j_thres_hour, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(hour = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics <- metrics_week %>% 
  full_join(metrics_day, by = "metric") %>% 
  full_join(metrics_hour, by = "metric") %>% 
  filter(metric %in% c("accuracy", "sens", "spec", "ppv", "npv"))

auc <- tibble(metric = "auc", 
              week = preds_week %>% roc_auc(prob, truth = label) %>%  
                pull(.estimate) %>% round(3), 
              day = preds_day %>% roc_auc(prob, truth = label) %>%  
                pull(.estimate) %>% round(3),
              hour = preds_hour %>% roc_auc(prob, truth = label) %>%  
                pull(.estimate) %>% round(3))

metrics <- metrics %>% 
  bind_rows(auc)

metrics <- metrics[c(6,1,2,3,4,5),]

metrics %>%
 kbl(col.names = c("Metric", "Week", "Day", "Hour"),
      booktabs = TRUE,
      digits = 2,
      align = c("r"),
      caption = "Performance Metrics by Model",
     linesep = "") %>% 
  row_spec(row = 0, align ="r") %>% 
  kable_styling(position = "left") %>% 
  footnote(general=footnote_table_metrics)
```

\clearpage




<!-- AUC figure by model w/posterior probabilities-->  
```{r caption_roc_pp}
fig_caption_roc_pp <- "Insert note here"
```

```{r fig_roc_pp, fig.cap = fig_caption_roc_pp, fig.height = 4.5, fig.width = 7}

roc_plot <- roc_all %>% 
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"), 
                        labels = c("Week", "Day", "Hour"))) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path(linewidth = 1.25, show.legend = FALSE) +
  geom_abline(lty = 3) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Specificity",
       y = "Sensitivity") +
  scale_x_continuous(breaks = seq(0,1,.25),
                     labels = sprintf("%.2f", seq(1,0,-.25))) 

pp_tidy <- pp %>% 
  tidy(seed = 123)

q = c(.025, .5, .975)
ci <- pp_tidy %>% 
  group_by(model) %>% 
  summarize(median = quantile(posterior, probs = q[2]),
            lower = quantile(posterior, probs = q[1]), 
            upper = quantile(posterior, probs = q[3])) %>% 
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour")),
         y = 1000) |> 
  arrange(model)

pp_plot <- pp_tidy %>% 
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour"))) %>%
  ggplot() + 
  geom_histogram(aes(x = posterior, fill = model), color = "black", alpha = .4, 
                 bins = 30) +
  geom_segment(mapping = aes(y = y+100, yend = y-100, x = median, xend = median,
                           color = model), show.legend = FALSE, data = ci) +
  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, color = model),
              show.legend = FALSE, data = ci) +
  # geom_text(data = ci, x = c(.93, .907, .92), y = 1000, 
  #           label = str_c(round(ci$median, 2), " [", round(ci$lower, 2), ", ", round(ci$upper, 2), "]")) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous("Posterior Probability", breaks = c(0, 500, 1000)) +
  xlab("Area Under ROC Curve") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank())

roc_plot + pp_plot +
  plot_layout (guides = "collect") &
  theme(legend.position = "bottom")
```




<!-- PR-->
<!-- get exact sensitivity at .75 PPV -->
<!-- no output-->
```{r}

#| output: false

ppv_70 <- pr_all %>% 
  mutate(recall = round(recall, 3),
         precision = round(precision, 3),
         .threshold = round(.threshold, 3),
         model = factor(model,
                        levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) %>% 
  filter(precision == .70) %>% 
  group_by(model, precision) %>% 
  summarise(recall = mean(recall),
            threshold = mean(.threshold),
            .groups = "drop")
```

<!-- Week sensitivity at .70 PPV =  `r round(pull(filter(ppv_70, model == "Week"), recall), 3)` -->

<!-- Day sensitivity at .70 PPV =  `r round(pull(filter(ppv_70, model == "Day"), recall), 3)` -->

<!-- Hour sensitivity at .70 PPV =  `r round(pull(filter(ppv_70, model == "Hour"), recall), 3)` -->



<!-- PR curves-->

```{r caption_pr}
fig_caption_pr <- "Precision-Recall Curves for models."
```

```{r fig_pr, fig.cap = fig_caption_pr, fig.height=7}
pr_all %>% 
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) %>%
  ggplot(aes(x = recall, y = precision, color = model)) +
  geom_path(linewidth = 1.25) +
  geom_segment(mapping = aes(y = .75, yend = .75, x = -.5, xend = recall,
                           color = model),
               linetype = "dashed",
               alpha = .8,
               show.legend = FALSE,
               data = ppv_70) +
  geom_segment(mapping = aes(y = -.5, yend = .75, x = recall, xend = recall,
                           color = model),
               linetype = "dashed",
               alpha = .8,
               show.legend = FALSE,
               data = ppv_70) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Sensitivity (Recall)",
       y = "Positive Predictive Value (Precision)")
```



<!-- SHAP Importance figure-->
```{r caption_shap}
fig_caption_shapgrouped <- "Variable Importance (SHAP Values) for each Model.  Raw EMA features are grouped by the original item from the EMA. Features from demographics and the day and hour for the start of the  prediction window are also included."
```

```{r fig_shap, fig.cap = fig_caption_shapgrouped, fig.height=7}
shap_global_all %>% 
  mutate(variable_grp = reorder(variable_grp, mean_value, sum)) %>% 
  ggplot() +
  geom_bar(aes(x = variable_grp, y = mean_value, fill = model), stat = "identity", alpha = .4) +
  ylab("Mean |SHAP| value") +
  xlab("") +
  coord_flip()
```

# Discussion

## 1. Model Performance

<!-- Overall model performance -->

Models across all three time windows performed exceptionally well, yielding AUC ROCs of [], [], and [] for hour level, day level, and week level models respectively. AUC ROCs summarize the model's ability to distinguish positive and negative classes over all possible decision thresholds. AUCs above .9 are generally described as having "excellent" performance, meaning that the model will correctly assign a larger probability to a positive case than a negative case 90% of the time (<!--Jayawant, 2010-->). All three of our models perform as well as or better than existing alcohol lapse prediction models to date (<!--Berenholtz et al,  2020-->). This indicates EMA data can be used to predict alcohol lapse in the next hour, next day, and next week with high levels of sensitivity and specificity. <!--[reference graph with specific sensitivity specificity values at certain cut-points?]-->

Aspects of the analytic design also likely contributed to overall high model performance. These include: using nonlinear algorithms to capture nonlinear relationship and higher-order interactions among predictors; including change scores in predictors over time within participants to capture within-subject variability; and using a high dimensional feature feature space to capture the complex interplay among proximal and distal risk factors. Moreover, using machine learning and cross-validation allowed us to benefit from these design features while feeling confident that our models were generalizing to new data. It is important to note that our method of model selection did not consider other types of algorithms or lower dimensional feature sets. Therefore, we cannot conclude that these specific algorithms and features are required for the reported performance estimates; rather, they are sufficient for producing the reported results.

Our study was also unique in its use of grouped, nested, k-fold cross-validation for generating our reported model performances. This is the most rigorous test of generalizability of EMA lapse prediction models in the literature to date. Nested cross-validation provides an unbiased estimate of model performance due to its separation of hyperparameter tuning and model selection processes from model performance estimation. Additionally, our approach grouped all observations from each participant within folds, meaning that observations from one participant were not used for both model building and evaluation. This method results in a more realistic assessment of how our models are expected to perform in unseen data from new participants, increasing our confidence that our model will generalize beyond this specific study.

In addition to improved performance, this project builds upon previous EMA lapse prediction work in several ways. First, our models were trained on a large, treatment-seeking sample of adults in early recovery from AUD. Although prediction work in other samples (e.g., college heavy drinkers) is valuable for risk detection within those groups, it is unlikely to port to clinical samples. This sample comprises individuals who would benefit from a risk prediction model, particularly if situated within a digital therapeutic to support ongoing recovery. Second, we specifically predicted episodes of unintentional alcohol use (i.e., "lapses"). The factors that predict goal-inconsistent use may differ substantially from factors that predict other types of alcohol use (e.g., episodes of binge drinking among college students, instances of drinking among people pursuing harm reduction goals). This relatively homogeneous prediction outcome likely increased the predictive ability of our model. Finally, we collected data from participants with high frequency (4x per day) over a clinically meaningful period of time. This sampling density allowed us not only to capture changes in dynamic risk factors with high precision but also to build models whose prediction windows updated frequently. Altogether, these specifications help to maximize both the predictive ability of our model as well as the eventual clinical utility of our predictions. 

## 2. Understanding & Contextualizing Model Performance

<!--SHAPs-->

We further examined the performance of our models via analysis of SHAP values. SHAP values are a method for interpreting the predictions of machine learning models. They provide insights into the contribution of each feature (i.e., predictor variable) in a prediction, allowing us to understand the importance of individual features in the model's decision-making process. Global importance provides information about which features are the most useful to the model as a whole (i.e., across all predictions), while local importance evaluates the contribution of each feature within a single context or prediction. The direction and magnitude of SHAP values correspond to the influence of that feature in generating a positive prediction.

Figure [X] displays the global importance values for features across our best performing week, day, and hour level models. Unsurprisingly, the largest contribution to prediction of a lapse is frequency of previously reported lapses. An individual who reports lapsing frequently is more likely to lapse at any given observation in the future. Additionally, the likelihood of a lapse increases when participants report lower ratings of confidence in their ability to maintain their goal of abstinence. Congruent with relapse prevention literature, we also see increased likelihood of lapse when participants report higher levels of craving, increased magnitude of stressful events experienced in the past 24 hours, and increased exposure to situations described as "risky for [your] recovery."

However, these features alone are not enough to obtain high predictive performance. As displayed in Figure [X], over 15 features contribute to any individual prediction of lapse. Additionally, we see in Figure [local shaps] that some features with low global importance yield high importance scores at the local level. [Explain some shaps with high local importance on graph...etc]. Local importance is particularly relevant for making risk predictions actionable. The locally important features that contribute to a specific prediction might represent targets for intervention. For example, if increased craving has high local importance for a given prediction, this might suggest that intervening in craving (e.g., with an urge surfing activity) could be effective to reduce lapse risk in the moment. Importantly, many of our features with high local and/or global importance align well with the risk factors and associated intervention strategies delineated in Marlatt's Relapse Prevention model. Thus, when thinking about situating our risk prediction model in a digital therapeutic, locally important features could provide a mapping to intervention targets and recommended treatment modules. 

Our included demographic variables did not yield high global or local importance scores across models. However, these conclusions are dependent on the representatives of the sample. Despite our data having wide representation with respect to SES, gender, and age characteristics, these features did not typically emerge as significantly contributing to the lapse prediction (as measured by SHAP values). While this does not rule out these features' predictive utility, it does suggest that other EMA variables (craving, past use) are more relevant for lapse prediction than these characteristics. Race and ethnicity also did not emerge in global or local SHAPs. However, the limited representation of Black and Brown populations in the current sample warrants caution in drawing conclusions about the predictive utility of race and ethnicity. Data collection is underway for a related project in our laboratory to build a lapse risk prediction model for individuals with opioid use disorder. Participants are being recruited nationally with the explicit goal of improving geographic, racial, and ethnic diversity to match national population data.

<!-- Model comparisons -->

SHAP values also assist in contextualizing the comparative performance of our week, day, and hour level models. Predicting lapse in the next week may seem less difficult than predicting lapse in the next hour because of the wider prediction window (i.e., less temporal precision required). However, Bayesian model comparisons demonstrate all three models have comparable predictive utility. In fact, prediction of lapse within the next hour yields slightly better performance than predicting lapse in the next day or week. Differences in global SHAP values across models help to understand why this might be the case. First, week-level models cannot capture time-relevant components of the lapse window such as temporally-based features (e.g., hour of day, day of week). Individuals were more likely to report lapsing in evening hours and on weekends, which resulted in day- and hour-related features having greater importance and predictive utility in more fine-grained prediction windows. Additionally, hour-level models can take greater advantage of time-varying predictors, specifically features occurring closer in time to the lapse event.

## 3. Clinical Implementation

The goal of the current project was to build models for lapse risk prediction using EMA data. Although this is a necessary first step, the ultimate goal of this line of work is to use this model clinically. Consequently, we conducted the current project and built these models with clinical implementation in mind. We believe these models may be most effective when embedded in a digital therapeutic context for reasons of access, availability, and affordability described previously. However, even within a digital therapeutic, there are several ways a model like ours might be clinically implemented. 

First, it might be used to communicate a patient's lapse risk to their treatment provider (e.g., therapist, primary care provider). For example, a treatment provider could receive notifications about which patients are at a high risk of lapsing at the start of each shift. They could then decide if they wanted to connect with a patient or use this information to triage their availability (i.e., which patients should be seen first). Although this information would likely be helpful to the treatment provider, it comes with several limitations. It requires that the treatment provider be willing to use the risk information and that they have the bandwidth to take on this additional load (e.g., monitoring, evaluating, and taking action based on lapse risk information for a high number of patients). Indeed, this seems unlikely, as it is well documented that treatment providers are over-extended and over-whelmed with patient loads [@thephysiciansfoundation2014; @nationalacademiesofsciencesFactorsContributingClinician2019 <!--81% of physicians report being over-extended or at capacity. High job demands cited as primary contributor to burnout.-->]. It also limits provider-initiated action to broader prediction windows: it is not feasible to expect an already-overburdened treatment provider to act on lapse risk information about the next hour or even next day. Instead, an application that communicates information to treatment providers might be constrained to the next week. This offers some clinical benefit, but it removes the possibility of just-in-time interventions to act on steep, rapidly-fluctuating increases in lapse risk. Finally, it requires that patients 1) have an established treatment provider, and 2) are willing to share information related to their lapse risk with their provider. For those without health insurance, gaining access to a treatment provider could be difficult if not impossible. For those with health insurance, willingness to share lapse risk with a provider might be lower if there is a chance of that information affecting insurance coverage or cost.

Second, a risk prediction model like ours might be used to communicate lapse risk directly to the individual. This removes the treatment provider as a gatekeeper of the risk information. As a result, individuals can be alerted to their lapse risk at anytime and at any desired threshold (i.e., not just at a threshold high enough to warrant clinician intervention). This may allow individuals to feel they have more control over their data and alleviate potential concerns about unauthorized use of their lapse risk information. Unfortunately, this information on its own may not be helpful to individuals monitoring their lapse risk and could even have unintended harmful consequences. This method requires an individual to know what action to take, or what module to use in a digital therapeutic, in response to the lapse risk information. Moreover, being alerted to a high risk of lapsing, without any accompanying intervention, could result in a self-fulfilling prophecy due to feelings of loss of control, failure, or associating their lapse risk with internal, stable, and global factors (e.g., abstinence violation effect).   <!-- GEF: probably good to have a citation related to this effect -->   

Third, this type of model might be used to recommend an action that an individual could take to reduce their risk of a lapse. In this situation, it could communicate an actionable treatment recommendation to the individual based on their lapse risk and the top features contributing to that risk (e.g., recommending an urge surfing activity in response to reported high cravings). Recommendations could be mapped onto existing therapeutic frameworks shown to be effective for alcohol use disorder (e.g., CBT, mindfulness-based relapse). A recommendation-guided digital therapeutic would reduce the risk of iatrogenic effects because suggested actions based on predicted risk would likely be helpful and positive regardless of one's actual risk. For example, if an individual receives a recommendation to complete a mindfulness meditation activity in response to high stress, this activity is likely to benefit the individual whether this was a true prediction or a "false positive." This approach would also capitalize on the benefits of existing self-guided digital therapeutics (e.g., reaching people not connected with a treatment provider, around-the-clock availability). Thus, this third approach would provide the benefits associated with therapeutics while optimizing their use to guide individuals towards specific recommendations based on personalized risk factors at the right times.

A critical piece affecting all three forms of clinical implementation is PPV. Whereas sensitivity describes how many of the true positives (actual lapses) are predicted as lapses by our model, PPV describes how many of our positive predictions are in fact true positives. In other words, how accurate are our lapse predictions, and how much confidence can an individual have that a lapse risk warning from our model confers real risk of lapsing? We found that when we set our decision threshold to .5 (i.e., all probabilities  > .5 are predicted to be a lapse; default threshold), PPV was low across models. Low PPV can be problematic in that it could involve mobilizing resources and/or alerting an individual that they may be at a risk of lapsing when they actually are not. PPV can be improved in two ways: increasing the prediction window, or adjusting the decision threshold. PPV is highly influenced by an unbalanced outcome variable (e.g., fewer lapses compared to no lapses). Therefore, we saw a natural increase in PPV as our prediction windows broadened, with one week prediction windows having the highest PPV <!-- insert value here -->. Additionally, increasing the decision threshold can improve PPV - as the threshold increases, the model needs to be "more confident" that a positive prediction represents a true positive. However, increasing the threshold comes at a cost to sensitivity. This means that we may miss some lapses, but the lapses we do predict are more likely to be true lapses. <!--give specific example from our data?-->     

Whether sensitivity or PPV matters more depends on the context and the user. For example, an individual using the app may wish to know that if they receive an alert, they can trust it; similarly, a provider may have limited resources to allocate and therefore wants to be near-certain that an alert confers true lapse risk. These contexts would support raising the decision threshold to improve PPV. Conversely, modules recommended in a digital therapeutic platform in response to a lapse risk alert are likely to benefit an individual whether they are truly at risk or not. Completing these modules may even serve as a protective factor or promote continued engagement with the digital therapeutic. There is also a high personal, health, and economic cost when an individual returns to heavy substance use. Thus, these perspectives would suggest keeping the decision threshold low to avoid missing true lapses. A recommendation-guided digital therapeutic could adjust the decision threshold based on overall costs of a recommendation (e.g., sending a reminder to check in on their recovery goals vs. reaching out to a supportive friend or family member), the availability of resources, and the user's own preference.

## 4. Additional Future Directions & Limitations

A primary future direction for this line of work is the eventual clinical implementation of a lapse risk prediction model, likely into a digital therapeutic platform. However, there are other future directions that may be pursued, particularly with the aim of addressing limitations of the current study.

Although we varied the duration of the outcome windows (i.e., one hour, one day, one week), each model currently has a lag time of 0. This means that a predicted lapse might occur anytime from the moment the prediction is updated through the end of the prediction window. This method takes advantage of the most and most up-to-date information with regard to risk factors - all data collected until that point can be used, and the most recent data occur immediately prior to the onset of the prediction window. This is likely quite practical for our hour model when we consider just-in-time interventions - we want to intervene immediately in response to changes in lapse risk driven by very recent changes in proximal risk factors. However, a lag time of 0 is less well-suited to a longer outcome window - in particular, our week model. A high likelihood of a lapse predicted by our week model means that the lapse could occur in the next hour or 6 days later. This model is not saying that an individual is likely to lapse in a week (i.e., a week from now); rather, it says an individual is likely to lapse anytime in the next week beginning right now. No lag creates potential problems for recommendations that take longer to implement. Suggesting someone make an appointment with their therapist would not be helpful if someone was going to lapse in the next hour; however, it could be helpful if they were at risk of lapsing a week into the future. Therefore, it is important to consider how lag time and prediction windows interact when making intervention recommendations. Narrow windows (e.g., next hour) with no lag could be particularly useful for making immediate intervention recommendations (e.g., urge surfing, calling a supportive contact, stimulus control techniques). Lagged broad windows (e.g., next week starting one week from now) could also be especially useful for incorporating interventions that may take longer. We plan to explore models with not only different outcome windows but also different lag times. We will investigate how these models differ with respect to performance and what this might suggest for potential clinical utility.

Our prediction algorithm was trained using participants with a goal of abstinence. We view this as both a strength and a limitation. As described above, a homogeneous outcome of goal-inconsistent use likely improved model performance. This outcome also set our study apart from previous work that has examined other types of drinking outcomes (e.g., binge drinking among college students) that is less closely connected with the type of clinical sample who might eventually use our models embedded within a digital therapeutic. Additionally, many individuals in recovery from alcohol and other substance use disorders do pursue abstinence goals, and this approach is still strongly recommended by some treatments (e.g., Alcoholics Anonymous). However, some individuals prefer to pursue other recovery goals more in line with moderation or harm reduction. It is likely that the factors that predict alcohol use among individuals pursuing abstinence as in the current study differ from the factors that predict alcohol use among individuals pursuing moderation goals. Moreover, within that population, alcohol use instances would need to be differentiated as goal-consistent use (e.g., having one drink) and goal-inconsistent use (e.g., having two drinks when your goal is one; having one drink for the second time that week when your goal is to drink once per week). It is likely that this model-building approach can be adapted for other recovery goals (e.g., moderation); however, it needs to be first tested, and it may require a completely new sample for model development and evaluation.

A final consideration is the assessment burden required of individuals to provide the data used to build and maintain lapse risk prediction models. In the current study, we used 4X daily EMA surveys, where each survey ranged in length from 7 to 10 items. These EMA surveys are considered "active" sensing, in that they require action on the part of the individual to provide data. In a previous paper from our laboratory using these data, we examined the burden of providing data for this project [@wyantAcceptabilityPersonalSensing2022]. Overall, individuals found the burden to be acceptable and reported being (hypothetically) willing to continue providing data for a full year. Despite these promising results, uncertainty remains as to how long this response rate could be maintained. AUD is a chronic, relapsing disorder that requires lifelong monitoring. Currently, this burden currently falls on the individual or (intermittently) their treatment provider. A risk prediction model embedded in a digital therapeutic could shoulder this burden, but only if an individual could provide data long-term - and potentially indefinitely. 

Several future directions for research may help investigate and address this concern. First, as mentioned previously, data collection efforts are underway for a nationally-recruited sample of individuals with opioid use disorder. These individuals will participate for a full year compared to the current study's duration of 3 months. We will be able to compare completion and attrition rates across these projects to explore how burden may change as duration increases (though with the confound of different substance-using populations). Second, the current study uses all four daily EMA surveys for risk prediction. All four EMAs contain the same 7 questions, but the morning EMA includes 3 additional questions asked only once daily. A future project could examine how well models perform using only the morning EMA survey, providing an estimate of how well we might predict using lower-burden data collection of only 1X daily EMA. Third, although the current study uses only active EMA and demographic characteristics as features, the broader parent project collected many other passive signals such as GPS location, cellular communications, and cellular metadata. Future projects will determine the predictive utility of these passive signals, both on their own and perhaps in conjunction with reduced actively sensed features, to understand whether burden could be lowered in these ways. Fourth, although it cannot be addressed with the current data, future research could explore adaptations for long-term data collection. The sampling density of actively sensed signals could be reduced as individuals enter sustained recovery/remission. Active sensing could be adapted on an individual basis to focus on features that emerge as primary global features or frequent locally important for that individual, reducing the number of items assessed regularly. These and other opportunities for adaptation may improve long-term engagement, but future research will be needed to test these ideas and explore any impact on prediction accuracy or other outcomes.
  
    





\clearpage


=======
---
title: "Untitled"
format: pdf
csl: https://raw.githubusercontent.com/jjcurtin/lab_support/main/rmd_templates/csl/elsevier-vancouver.csl
geometry: margin=.5in
fontsize: 11pt
bibliography: paper_ema.bib
editor_options: 
  chunk_output_type: console
---



<!--General notes
Considering American Journal of Psychiatry
https://ajp.psychiatryonline.org/ajp_ifora

ARTICLES
Articles are reports of original work that embodies scientific excellence in psychiatric medicine and advances in clinical research. Typically, articles will contain new data derived from a sizable series of patients or subjects. The text is usually within 3,500 words, which does not include an abstract of no more than 250 words, a maximum of 5 tables and figures (total), and up to 40 references. Word count includes only the main body of text (i.e., not tables, figures, abstracts or references). Additional tables can be submitted in a separate file as supplemental data for posting online. (See Supplemental Data for what types of data and formats are acceptable for posting online.)
-->

```{r knitr_settings, include = FALSE}
# settings
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = FALSE, 
                      message = FALSE)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(fig.pos = "ht", out.extra = "")
```

```{r setup, include = FALSE}
library(knitr)
# library(yardstick) # for roc_curve
library(kableExtra)
library(janitor)
# library(corx)
library(patchwork)
library(ggtext)
library(consort)
library(tidyverse)
library(tidymodels)
library(tidyposterior)
library(cowplot)

theme_set(theme_classic()) 
```


```{r paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- "P:/studydata/risk/chtc/ema"
          path_processed <- "P:/studydata/risk/data_processed/ema"
          path_models <- "P:/studydata/risk/models/ema"
          path_shared <- "P:/studydata/risk/data_processed/shared"},

        # IOS paths
        Darwin = {
          path_input <- "/Volumes/private/studydata/risk/chtc/ema"
          path_processed <- "/Volumes/private/studydata/risk/data_processed/ema"
          path_models <- "/Volumes/private/studydata/risk/models/ema"
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"},
        
        # Linux paths
        Linux = {
          path_input <- "~/mnt/private/studydata/risk/chtc/ema"
          path_processed <- "~/mnt/private/studydata/risk/data_processed/ema"
          path_models <- "~/mnt/private/studydata/risk/models/ema"
          path_shared <- "~/mnt/private/studydata/risk/data_processed/shared"}
        )
```

```{r load_data}
# Table data
disposition <- read_csv(file.path(path_processed, "disposition.csv"), 
                        col_types = "ccDDcccccccccc")
screen <- read_csv(file.path(path_shared, "screen.csv"), 
                   col_types = vroom::cols()) |>
  filter(subid %in% subset(disposition, analysis == "yes")$subid)

lapses <- read_csv(file.path(path_shared, "lapses.csv"), col_types = cols()) |>
  filter(exclude == FALSE)

# Predictions data
preds_week<- read_rds(file.path(path_models, "outer_preds_1week_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)
preds_day<- read_rds(file.path(path_models, "outer_preds_1day_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)
preds_hour<- read_rds(file.path(path_models, "outer_preds_1hour_0_v5_nested.rds")) |> 
  select(outer_split_num, label, prob = prob_beta)

# posterior probabilities
pp <- read_rds(file.path(path_models, "posteriors_all_0_v5_nested.rds"))

# ROC curves
roc_week <- preds_week %>% 
  roc_curve(prob, truth = label) %>% 
  mutate(model = "1week")

roc_day <- preds_day %>% 
  roc_curve(prob, truth = label) %>% 
  mutate(model = "1day")

roc_hour <- preds_hour%>% 
  roc_curve(prob, truth = label) %>% 
  mutate(model = "1hour")

roc_all <- roc_week %>% 
  bind_rows(roc_day) %>% 
  bind_rows(roc_hour)

# PR curves
pr_week <- preds_week %>% 
  pr_curve(prob, truth = label) %>% 
  mutate(model = "1week")

pr_day <- preds_day %>% 
  pr_curve(prob, truth = label) %>% 
  mutate(model = "1day")

pr_hour <- preds_hour%>% 
  pr_curve(prob, truth = label) %>% 
  mutate(model = "1hour")

pr_all <- pr_week %>% 
  bind_rows(pr_day) %>% 
  bind_rows(pr_hour)

# UPDATE - JJC
# Local and Global Grouped SHAPS
shap_local_week <- read_rds(file.path(path_models, "outer_shapsgrp_1week_0_v5_nested.rds")) 
shap_local_day <- read_rds(file.path(path_models, "outer_shapsgrp_1day_0_v5_nested.rds"))
shap_local_hour <- read_rds(file.path(path_models, "outer_shapsgrp_1hour_0_v5_nested.rds")) 

shap_global_week <- shap_local_week %>% 
  group_by(variable_grp) %>% 
  summarize(mean_value = mean(abs(value)), .groups = "drop") %>% 
  arrange(mean_value) |> 
  mutate(model = "Week")
shap_global_day <- shap_local_day %>% 
  group_by(variable_grp) %>% 
  summarize(mean_value = mean(abs(value)), .groups = "drop") %>% 
  arrange(mean_value) |> 
  mutate(model = "Day")
shap_global_hour <- shap_local_hour %>% 
  group_by(variable_grp) %>% 
  summarize(mean_value = mean(abs(value)), .groups = "drop") %>% 
  arrange(mean_value) |> 
  mutate(model = "Hour")

shap_global_all <- shap_global_week |> 
  bind_rows(shap_global_day) |> 
  bind_rows(shap_global_hour) |> 
  mutate(model = factor(model, levels = c("Week", "Day", "Hour"))) %>% 
  mutate(variable_grp = factor(variable_grp, levels = c("past use (EMA item)", 
                                          "craving (EMA item)", 
                                          "past risky situation (EMA item)", 
                                          "past stressful event (EMA item)", 
                                          "past pleasant event (EMA item)", 
                                          "valence (EMA item)", 
                                          "arousal (EMA item)", 
                                          "future risky situation (EMA item)", 
                                          "future stressful event (EMA item)", 
                                          "future efficacy (EMA item)",
                                          "lapse day (other)",
                                          "lapse hour (other)",
                                          "missing surveys (other)",
                                          "age (demographic)",
                                          "sex (demographic)",
                                          "race (demographic)",
                                          "marital (demographic)",
                                          "education (demographic)")))

```

# Introduction

<!--GEF overarching notes

i have added definitions of relapse (a goal-inconsistent return to substance use) and lapse (a single instance of goal-inconsistent substance use) the first time these terms are used per our group discussion. i will say that we seem to be using the terms somewhat interchangeably, which is perhaps ok until we start talking about our study, which should be specifically and exclusively about LAPSE risk. something to monitor in a final read.

i included potential subheadings in comments within the intro if we want to use them-->
 
Alcohol and other substance use disorders (SUDs) are highly prevalent and costly.  In 2019, the National Survey on Drug Use and Health estimated that over 20 million adults in the United States had some form of active SUD within that year [@substanceabuseandmentalhealthservicesadministrationKeySubstanceUse2020]. Nearly 15 million of those adults had an active alcohol use disorder (AUD) [@samhsa2019NationalSurvey; @samhsa2019NationalSurveyb].<!--KW: 2021 numbers are quite different when I compare the tables side by side. Past year SUD jumps to over 40 million, with nearly 30 million having AUD. There is a note in the table that says "Estimates from years prior to 2021 are not shown because of methodological changes in 2021" which likely explains this discrepancy. Should we use these more recent numbers?--> Furthermore, a substantial 25.8% of U.S. adults reported engaging in hazardous alcohol misuse within the past month <!--KW: 23.3% is 2021 estimate. Also note for context both of these estimates are reports of past month binge drinking--> [@samhsa2019NationalSurveya]. Alcohol ranks as the third leading preventable cause of death, accounting for approximately 140,000 fatalities per year [@centersfordiseasecontrolandpreventionAlcoholPublicHealth; @esserEstimatedDeathsAttributable2022]. In economic terms, the US Surgeon General disclosed that alcohol misuse cost the United States $249 billion in 2016 alone [@administrationusFacingAddictionAmerica2016].

Existing clinician-delivered treatments for AUD such as cognitive-behavioral therapy [@mchughCognitiveBehavioralTherapySubstance2010; @lieseCognitiveBehavioralTherapyAddictive2022], mindfulness-based relapse prevention [@bowenMindfulnessBasedRelapsePrevention2021; @goldbergMindfulnessbasedInterventionsPsychiatric2018], motivational interviewing [@millerMotivationalInterviewingHelping2012], and contingency management [@bigelowTheoreticalEmpiricalFoundations1999; @dutraMetaAnalyticReviewPsychosocial2008<!--can cut some refs eventually-->] are effective when provided to patients.  Unfortunately, fewer than 1 in 13 adults with an active AUD in 2019 received any treatment[@samhsa2019NationalSurveyc]<!--KW: in 2021 went from 7.5 to 4.6% (fewer than 1 in 20)-->.  Even more concerning, the failure to access treatment is associated with demographic factors including race, ethnicity, geographic region, and socioeconomic status, which further increase mental health disparities [@wangFailureDelayInitial2005; @MentalHealthReport1999; @generalusMentalHealthCulture2001; @mauraMentalHealthDisparities2017; @novakChangesHealthInsurance2018]. This treatment gap and associated disparities stem from well-known barriers to receiving clinician-delivered mental healthcare related to their affordability, accessibility, availability, and acceptability[@jacobsonUsingDigitalTherapeutics2023].

<!-- ## Digital Therapeutics Can Address Treatment Barriers -->

In recent years, digital therapeutics have emerged as an alternative method to deliver evidence-based treatments and other support to patients either independently or in conjunction with medications or clinician-administered mental healthcare[@jacobsonDigitalTherapeuticsMental2022 <!--Can re-use this reference later rather than chapters if needed to get cites <= 40-->]. Digital therapeutics are web-based or smartphone "apps" that are used to prevent, treat, or manage a medical disorder including AUD or other mental illnesses.  Several large randomized controlled trials have confirmed that digital therapeutics for alcohol or other SUDs improve clinical outcomes (e.g., abstinence, heavy drinking days[@gustafsonSmartphoneApplicationSupport2014; @campbellInternetdeliveredTreatmentSubstance2014]; see Campbell et al. [@campbellFirstWaveScalable2023] for review). They also may enhance treatment retention, reduce readmissions, and support engagement with medication-assisted treatments [@bottsMPOWERProjectResults2017; @japuntichSmokingCessationInternet2006; @pattenRandomizedClinicalTrial2006; @campbellInternetdeliveredTreatmentSubstance2014].

Digital therapeutics can mitigate or eliminate many of the affordability, accessibility, availability, and acceptability barriers associated with in-person, clinician-delivered mental healthcare because they are typically provided to patients on their smartphones[@jacobsonUsingDigitalTherapeutics2023].  Recent Pew Research Center survey data indicate high rates of smartphone ownership among US adults (approximately 85% in April 2021), with minimal variation across race, ethnicity, socioeconomic status, and geographic settings (e.g., urban, suburban, rural) [@pewresearchcenterMobileFactSheet2021]. Moreover, individuals with SUDs generally exhibit high rates of mobile technology use [@collinsFactorsAssociatedPatterns2016]. Consequently, digital therapeutics can offer highly scalable, on-demand therapeutic support that is accessible whenever and wherever it is needed most. 

<!--KW: Just noting the back and forth of focus on SUD, AUD, and mental health broadly in the intro. Since this was an issue with the burden reviewers we might want to consider whether we want to constrain our focus to SUD/AUD or even just AUD. -->


<!-- ## Improving Digital Therapeutics -->

Despite the documented clinical and other benefits of digital therapeutics, their full potential has not yet been realized because patients do not use them optimally.  Patients often don't engage with them as developers intended, and long-term engagement may not be sustained or matched to patients' needs [@hatchExpertConsensusSurvey2018; @lattieDigitalMentalHealth2019; @ngUserEngagementMental2019; @yeagerIfWeBuild2018].  The substantial benefits of digital therapeutics come from easy, 24/7 access to their many modules - their treatments, tools, and other support services.  However, this benefit also presents patients with a challenge.  They may be uncertain about when to use the app, which of the numerous modules within the app best suit their needs, and more specifically, which modules are most appropriate for their current situation.

Identifying these most appropriate modules can be difficult because of the dynamic nature of recovery from AUD over time. AUD is a chronic, relapsing disorder [@brandonRelapseRelapsePrevention2007; @witkiewitzModelingComplexityPosttreatment2007; @mclellanDrugDependenceChronic2000]<!--GEF: i moved the final clause of the previous paragraph into the beginning of this paragraph and split the topic sentence into two. didn't move any refs, so may need to move some refs to first sentence if they are about dynamic nature of recovery from AUD rather than AUD being a chronic relapsing disorder -->. Numerous risk and protective factors interact in complex, non-linear ways to influence the probability, timing, and severity of relapse (i.e., a goal-inconsistent return to substance use <!--KW: should we make this definition specific to alcohol use since this paragraph is now focussed on AUD?-->) [@hendershotRelapsePreventionAddictive2011; @witkiewitzRelapsePreventionAlcohol2004; @huffordRelapseNonlinearDynamic2003; @witkiewitzNonnormalityDivergencePosttreatment2007; @witkiewitzModelingComplexityPosttreatment2007]. Many of these factors are transient, leading to fluctuating relapse risk.  Factors such as urges, mood, lifestyle imbalances, self-efficacy, and motivation can all vary over time.  Social networks may evolve to be more protective or risky, and high risk situations can arise unexpectedly.

Clinical observations and research indicate that successful recovery necessitates life-long monitoring [@hendershotRelapsePreventionAddictive2011; @brandonRelapseRelapsePrevention2007; @huffordRelapseNonlinearDynamic2003; @witkiewitzTherapistGuideEvidenceBased2007; @witkiewitzModelingComplexityPosttreatment2007]. Continuous monitoring of risk for lapse (i.e., a single instance of goal-inconsistent substance use <!--KW: alcohol use? drinking?-->) and its contributing factors, if feasible, would enable patients to adapt their lifestyle, behaviors, and supports to their changing needs. In the context of digital therapeutic use, successful monitoring could direct patients to engage with the most appropriate specific modules in an app, addressing the unique risks present at any given moment throughout their recovery. This guided, adaptive engagement could potentially enhance the app's effectiveness. Thus far, however, the continuous, multi-factor monitoring required for this guidance has proved challenging due to the dynamic and complex interplay of various factors over time. <!--GEF: this final sentence was originally part of the first sentence; i felt it flowed better to talk about benefits and then end with not being able to do it yet since we then move into "but now we can!". didn't move any refs though so check if some from first sentence need to go here -->

<!--relevant review: mohrPersonalSensingUnderstanding2017-->
<!--KW: I think this paragraph below feels a little abrupt here. It might flow better if a subheading separates the above paragraph and this one? Also noting we are zooming out to mental health broadly again.--> 
Moment-by-moment personal sensing of intra- and interpersonal risk factors to support both long-term monitoring and forecasting of mental health functioning is now feasible[@epsteinPredictionStressDrug2020; @suchtingUsingElasticNet2019; @hebertPredictingFirstSmoking2021a; @engelhardPredictingSmokingEvents2018; @businelleUsingIntensiveLongitudinal2016; @soysterPooledPersonspecificMachine2022; @hebertEcologicalMomentaryIntervention2018; @moshontzProspectivePredictionLapses2021; @wyantAcceptabilityPersonalSensing2022; @chihPredictiveModelingAddiction2014; @baeMobilePhoneSensors2018]. Mohr et al. [@mohrPersonalSensingUnderstanding2017] define personal sensing as "collecting and analyzing raw data from sensors embedded in the context of daily life with the aim of identifying human behaviors, thoughts, feelings, and traits" <!--GEF: need a page number with a direct quotation-->. The widespread proliferation of smartphones has made personal sensing both powerful and practical.  Smartphones can be used for active sensing such as ecological momentary assessment (EMA; i.e., repeated, brief self-reports) and also passive, continuous sensing of geolocation, cellular communications (e.g., phone calls and text messages), activity level, sleep, and other raw signals that can predict meaningful clinical outcomes. 

<!-- ## EMA: A Powerful Sensing Tool -->

The current project focuses explicitly on the use of EMA to monitor lapse risk for both practical and strategic reasons. EMA can be easily implemented with only a smartphone; no additional hardware is necessary.  Moreover, comparable raw data (items and responses) can be used consistently across different hardware, operating systems, and sensing apps.  For these reasons, EMA can be incorporated essentially identically into any existing or future smartphone-based digital therapeutic.  EMA can support frequent, in situ longitudinal measurement that will likely be necessary for monitoring episodic or otherwise fluctuating relapse risk.  Long-term monitoring with EMA has been well-tolerated by individuals with SUD [@wyantAcceptabilityPersonalSensing2022; @moshontzProspectivePredictionLapses2021].  Previous research has validated the use of EMA to measure theoretically implicated risk and protective factors for relapse including craving [@dulinSmartphoneBasedMomentaryInterventionCraving2017], mood [@russellAffectRelativeDayLevelDrinking2020], stressors[@wemmDayByDayStressAlcohol], positive life events[@dvorakTensionReductionAffectReg2018], and motivation/efficacy [@dvorakEMAAcuteAUDSymptoms2014].  Furthermore, EMA provides privileged access to many of these more subjective constructs that may be difficult to quantify reliably through other sensing methods.  

Preliminary research is now emerging that uses EMA as features in machine learning models to predict the probability of future alcohol lapses or related outcomes [<!--Sarah - insert  refs-->].  This research is potentially important because it rigorously establishes temporal ordering between the predictors (features engineered from EMAs) and the outcomes. Additionally, the models are evaluated on data from participants that were not used to train the models, thus prioritizing model generalizability. Both these criteria are necessary to develop models that can be clinically useful to truly predict future lapses when implemented with new patients rather than simply confirm associations between putative risk factors and alcohol lapses among existing research participants.  

Despite this initial promise, several important gaps in this preliminary research restrict its generalizability and utility.  First, there are limitations regarding the samples used to train and evaluate these models.  Prediction models developed with samples such as college students or non-treatment-seeking hazardous drinking populations [e.g., <!--Sarah - insert college student or other convenience samples-->] provide important proof of concept; however, they may not generalize well to clinical samples of patients with AUD.  Similarly, models that have been developed to predict alcohol use in non-treatment seeking patients with AUD may be less clinically useful.  These individuals are unlikely to use digital therapeutics to support their recovery until they begin to contemplate and/or commit to behavior change regarding their alcohol use [<!--JJC to insert stage of change reference-->].  Moreover, features that predict planned or otherwise intentional alcohol use among individuals that are not motivated to change their behavior may not generalize to predict goal-inconsistent alcohol use (i.e., lapses) among patients in recovery.  

<!--Sarah - what about models predicting outcomes other than lapse (e.g., craving, stress)?  What can you tell us about those studies?

Added: Limited to studies that use EMA as a predictor, target SUD related outcomes (some smoking related...), utilized machine learning methods-->

<!--JOHN: review following paragraph re: including non-alcohol studies -->

A second limitation concerns the types of outcomes models are trained to predict. A handful of studies have trained models to predict precursors of substance use, such as craving [@burgess-hullTrajectoriesCravingMedicationassisted2022; @dumortierClassifyingSmokingUrges2016] and stress (Sarker et al., 2016). Although craving and stress are strongly associated with substance use, they do not reliably predict instances of substance use, limiting their utility for identifying specific instances for substance use treatment <!--GEF: probably need citations to make this claim, or make clear that the citations in the previous sentence are somehow supporting this-->. Moreover, due to the increased frequency of craving and stress events relative to substance use episodes, higher model performance can be achieved with less effort [@epsteinPredictionStressDrug2020] <!--GEF: "with less effort"?... is this about outcome class distribution? increased overall sample size? not quite sure what this means-->. Other studies have predicted temporally coarse clinical outcomes such as binary SUD treatment success [@acionUseMachineLearning2017; @coughlinMachineLearningApproachPredicting2020] or probability of treatment dropout for individuals in a medication-assisted treatment program for opioid use disorder [@gottliebMachineLearningPredicting2022]. These prediction models may be useful for identifying individuals who would benefit from increased SUD treatment; however, they do not provide enough temporal specificity to assist in identifying when resources should be directed to these individuals. <!--GEF: Sarah - I did my best to find the references you were citing & add them to Zotero; please double check that I found the studies you were thinking of! I was too uncertain about the Sarker 2016 paper to make a guess on that one.-->

<!--GEF: removed these two sentences from previous & just added citations to the second sentence - maintaining in case this level of detail is desired
For example, EMA has been demonstrated to predict levels of craving in individuals attempting to quit smoking (Durmortier et al 2016) as well as individuals in medication-assisted treatment programs for opioid use disorder (Burguess-Hall et al 2022). EMA combined with physiological data yielded prospective prediction of stressful event episodes in a small sample of individuals endorsing polysubstance use (Sarker et al 2016). 
-->


<!--JJC will justify lapse after we have above paragraph out other outcomes drafted.  - temporally precise, clearly defined, necessary for relapse, often early warning, can produce AVEs which can further undermine recovery. Abstinence is the FDA indicated outcome for clinical trials.  Describe here or in the current study below

GEF: took a stab at this below. My vote is for the paragraph to go here as I think it provides a good go-between from the previous paragraph about less than ideal outcomes to the next paragraph about the "exemplary" study that used lapses as outcome among other benefits.
-->

Predicting alcohol lapses (i.e., single instances of goal-inconsistent alcohol use) may be preferred for several reasons. First, a lapse is a drinking behavior rather than an associated factor or related behavior (e.g., craving, stress). Second, lapses are clearly defined and temporally precise. They have a specific onset and offset whose exact timing can be recorded; the lapse begins when drinking starts, and it ends when drinking stops. Lapses contrast with an outcome like relapse (i.e., returning to goal-inconsistent alcohol use), which cannot be clearly defined - it is unclear when a lapse becomes a relapse. However, the tight connection between lapses and relapses offers a third advantage in that a lapse is necessary for a relapse. Thus, lapses represent outcomes that precede relapses and may serve as early warnings for relapse. By building models that predict lapses, we position ourselves to assess risk for an event on which we can intervene before relapse. Fourth, lapses can produce adverse effects that could undermine recovery, making them clinically meaningful events that individuals pursuing recovery may wish to avoid. Finally, lapses represent events that are inconsistent with an abstinence goal. Abstinence not only remains a common goal among individuals in recovery from AUD but also is an FDA-indicated outcome for clinical trials. 

An early but exemplary prediction model developed by Gustafson et al. [@chihPredictiveModelingAddiction2014] provided the foundation on which our current project builds.  Participants enrolled in their study as they completed a residential treatment program for AUD.  Alcohol lapses were recorded for 8 months following program discharge while they received continuing care through the use of a digital therapeutic for AUD (A-CHESS). As such, the model was developed to predict a clinically meaningful outcome using individuals targeted for digital therapeutic use (i.e., clinical sample committed to AUD recovery-related behavior change).  However, the temporal precision for both the machine learning features and alcohol lapses was coarse because they were measured through a weekly survey.  Therefore, model predictions updated only once per week at best, and lapse onsets could occur anytime within the next two weeks.  This coarseness limits the utility of the model for "just-in-time" micro-interventions (e.g., guided mindfulness or other stress reduction techniques, urge surfing) that are well-suited to digital therapeutics but should be used in moments of peak risk. Furthermore, evaluation of this model should be considered both preliminary and optimistically biased because periods with missing surveys were excluded.  

<!-- ## Current Study -->

We designed the current study to address these gaps and limitations of previous machine learning prediction models from Gustafson [@chihPredictiveModelingAddiction2014] and others [<!--insert refs from above-->].  We developed our models using participants in early recovery (1-8 weeks since last alcohol use at study enrollment) from moderate to severe AUD who reported a goal of alcohol abstinence.  Consistent with this goal, these models predict future lapses back to alcohol use (i.e., instances of goal-inconsistent alcohol use).  We developed three separate models that provide hour-by-hour probabilities of a future lapse with increasing temporal precision (i.e., lapses in the next week, next day, and next hour, respectively).  Model features were based on raw and longitudinal change in theoretically-implicated risk factors[<!--insert relapse prevention model cites-->] derived from 4x daily EMAs. This research represents an important step toward the development of a "smart" (machine learning guided) sensing system that can both identify periods of peak lapse risk and recommend specific supports to address factors contributing to this risk.

# Method

## Research Transparency
We value the principles of research transparency that are fundamental to the robustness and replicability of science and took several steps to follow open science guidelines. We reported how we determined our sample size <!--did we?-->, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]<!--this is 21 word solution, do we use that statement?-->. We completed a transparency checklist (see Supplement; [@aczelConsensusbasedTransparencyChecklist2019]<!--have we?-->). Finally, we made the data, analysis scripts, annotated results, questionnaires, and other study materials associated with this report publicly available [<!--insert project OSF link-->]. 

Throughout this project, we iteratively improved the machine learning methods that are rapidly evolving in the social sciences and used in this study. These factors made our analyses inherently exploratory and thus inappropriate for preregistration. However, we restricted many researcher degrees of freedom via cross-validation procedures that can robustly guide decision-making. Replication is built into cross-validation: models are fit using held-in training sets, decisions are made using held-out validation sets, and final model performance is evaluated in a confirmatory manner using held-out test sets.

## Participants
We recruited 151 participants in early recovery (1-8 weeks of abstinence) from AUD in Madison, Wisconsin, USA, to participate in a 3-month longitudinal study. Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required that participants:

1.  were 18 years of age or older,
2.  were able to write and read in English,
3.  had at least moderate AUD (\>= 4 DSM-5 symptoms^[We measured DSM-5 symptoms with a self-report survey administered to participants during the in-person screening visit.]),
4.  were abstinent from alcohol for at least 1 week but no longer than 2 months,
5.  were willing to use a single smartphone (their personal phone or one provided by us) while enrolled in the study.

We also excluded participants if they exhibited severe symptoms of psychosis or paranoia^[Psychosis and paranoia were defined as scores greater than 2.2 or 2.8, respectively, on the psychosis or paranoia scales of the on the Symptom Checklist â€“ 90 (SCL-90) [@derogatisSCL90OutpatientPsychiatric1973].]. 

## Procedure
Participants completed five study visits over approximately three months. After an initial phone screen, participants attended an in-person screening visit where we determined eligibility, obtained informed consent, and collected self-report measures of individual differences (e.g., demographics, mental health, and alcohol use history; see Measures below). Eligible and consented participants returned approximately one week later to enroll in the study at an intake visit. Three additional follow-up visits occurred about every 30 days participants were on study. At each follow-up visit, we collected additional self-report and interview measures. 

For the entire duration on study, participants were expected to complete EMAs four times each day. Other personal sensing data streams (geolocation, cellular communications, sleep quality, and audio check-ins) were collected as part of the larger grant's aims (R01 AA024391). A full description of the procedure and data collected at each visit can be found at the study's Open Science Framework (OSF) page [<!--Insert link here-->]. All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

## Measures
### EMA
<!--citation for validity of self-reported alcohol use: https://pubmed.ncbi.nlm.nih.gov/26160523/-->
Participants completed a brief (7-10 questions) EMA four times each day following pushed text message reminders. These text messages included a link to a Qualtrics survey, optimized for completion on their smartphone. 

All four EMAs included seven items that asked about alcohol use not yet reported, current affective state (pleasantness and arousal), greatest urge to drink alcohol since the last EMA, any pleasant or positive events, any hassles or stressful events, and any exposure to risky situations (i.e., people, places, or things) that occurred since the last EMA. The first EMA each day asked an additional three questions about how likely participants were to encounter a risky situation, encounter a stressful event, and drink alcohol in the upcoming week. 

The first and last EMAs of the day were scheduled within one hour of participants' typical wake and sleep times. The other two EMAs were each scheduled randomly within the first and second halves of the participants' typical day. All EMAs were separated from each other by at least one hour.

### Individual Differences
At the screening visit we collected self-report information about demographics (age, sex, race, ethnicity, education, employment, personal income, and marital status) and clinical characteristics (AUD milestones, number of quit attempts, lifetime history of treatment for AUD, lifetime receipt of medication for AUD, DSM-5 AUD symptom count, and current drug use [@whoassistworkinggroupAlcoholSmokingSubstance2002]) to describe our sample. Only age, sex, race, education, and marital status are used as features for our analyses. A full description of all measures collected as part the larger grant are available on the study's OSF page [<!--Insert link here-->].

## Data Analytic Strategy
Data preprocessing and modeling were done in RStudio [@rstudioteamRStudioIntegratedDevelopment2020] using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020].

### Lapse Labels
We created future prediction windows that varied in their temporal precision by using three distinct window widths (i.e., duration between window start and end time): 1 week, 1 day, and 1 hour.  For each participant, the start of the first prediction window for all three widths began at midnight on their second day of participation and ended 1 week, 1 day, or 1 hour later, respectively.  By beginning at the end of the second day, we were assured that there would be at least 24 hours of past EMAs to use for future lapse prediction in these first windows.  Subsequent windows were created for all three widths by repeatedly rolling the window start/end forward 1 hour until the end their study participation was reached (i.e., each participant's last prediction window for each width ended at the date and hour of their last recorded EMA).  By using windows that rolled by 1 hour for all three window widths, we were able to develop classification models that provided hour-by-hour predictions of future lapse probability for all three window widths.

We labeled each prediction window as "lapse" or "no lapse" using participants reports from question 1 of the EMA ("Have you drank any alcohol that you have not yet reported?").   If participants answered yes to this question, they were prompted to enter the hour and date of the start and end of the drinking episode  These reports were also validated by study staff during the monthly visits and staff probed for any unreported lapses at that time using a timeline follow-back format.  

A prediction window was labeled "lapse" if the start date/hour of any drinking episode fell within that window.  A window was labeled as "no lapse" if no alcohol use occurred within 24 hours of the window start/end.  We used this conservative 24 hour fence for labeling windows as "no lapse" to increase the fidelity of these labels.  Given that most windows were labeled "no lapse" (i.e., the outcome was highly unbalanced), it was not problematic to exclude some to further increase confidence in those labels.  

### Feature Engineering
Features were calculated using only data that were collected prior to the start of each prediction window to assure that our models were true "prediction" models, making future predictions rather than simply identifying concurrent associations between features and lapses.  Features were derived from three sources.   The first source of features included demographic characteristics (i.e., age, sex, race, marital status, education) measured at baseline.  The second source of features used previous EMA responses that were collected prior to the start of the associated lapse window.  Using these EMA responses,  we created features using both raw (e.g., min., max., median, most recent response, and total counts) and change (e.g., within-subject baseline comparisons) scores. We scored raw min, max, median, and count features within a small set of periods prior to the start of the prediction window (6, 12, 24, 48, 72, and 168 hours prior start of window).  We scored change features by subtracting the mean response for each feature over all data prior to the start of the lapse window from the associated raw feature.  The third source of features was based on the day of the week and the time of day (daytime vs. evening/night) of the start of the lapse window. 

Other generic feature engineering steps included 1) imputation for missing data for features (median imputation for numeric features, mode imputation for nominal features), 2) dummy coding for nominal features, and 3) removal of any zero variance features.  Medians/mode for missing data imputation and identification of zero variance features were derived from training (held-in) data and applied to held out (validation and test) data to prevent issues associated with data leakage (see Model Training procedures below). <!--KW: possibly reference supplemental recipe code here-->.  


### Model Training and Evaluation
We initially considered four candidate classification statistical algorithms (XGBoost, Random Forest, K-Nearest Neighbors, and Elastic Net) that differed across various characteristics expected to affect model performance (e.g., flexibility, ability to handle higher-order interactions natively, complexity, linear vs. non-linear). These algorithms are well-established with documented good "out of box" performance, and they vary with respect to the degree of feature selection performed automatically during model fitting [@kuhnAppliedPredictiveModeling2018]. However, preliminary exploratory analyses suggested that XGBoost outperformed the other three algorithms.  Furthermore, the calculation of Shapely Additive Explanations (SHAP) values, which we planned to use for explanatory analyses of feature importance, are optimized for XGBoost.  For these reasons, we focused our primary model training and evaluation on the XGBoost algorithm only.  
  
We trained candidate XGBoost model configurations that differed across sensible values for the hyper-parameters mtry, tree depth, and learning rate using grid search.  All configurations used 500 trees combined with early stopping to prevent over-fitting.  All other hyper-parameters were also set to defaults established by tidymodels and xgboost packages in R.  Candidate model configurations also differed with respect to the outcome resampling method.  Specifically, we used up-sampling and down-sampling of the outcome using majority (no lapse) to minority (lapse) ratios that ranged from 1:1 to 5:1 to address class imbalance (i.e., windows that contained lapses were much less frequent than no lapse windows for all three window widths).  In addition, we calibrated the predicted probabilities from these XGBoost models using the beta distribution to support optimal decision-making under variable outcome class  distributions [@kullSigmoidsHowObtain2017]

Our primary performance metric for selecting and evaluating best model configurations was the area under the Receiver Operating Characteristic Curve (auROC)[@kuhnAppliedPredictiveModeling2018]. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case relative to a randomly selected negative case.  An auROC of 0.5 indicates chance performance; an AUC of 1.0 perfectly discriminates between positive and negative outcome classes.  This metric was selected because it 1) combines both sensitivity and specificity, which are both important characteristics to consider for clinical implementation, 2) it is an aggregate metric across all possible decision thresholds, which is important because the optimal decision threshold may differ depending on setting and goals, 3) it is not affected by class imbalance, which is important when comparing models across the three different prediction window widths that have difference levels of imbalance.  

We used grouped (by participant), nested cross-validation to train models, select best models, and evaluate those best models on auROC.  With grouped cross-validation, all of a participant's data is either held-in or held-out to avoid bias that is introduced when predicting a participants data from their own data[<!--ref?-->].  In nested cross validation, there are two nested loops for dividing and holding out folds: an outer loop with k<sub>outer</sub> folds, where held-out folds serve as test sets for model evaluation; and inner loops with k<sub>inner</sub> folds, where held-out folds serve as validation sets for model selection. The full dataset is divided into k<sub>outer</sub> approximately equal-sized, non-overlapping folds. Each k<sub>outer</sub> fold serves once as an independent test set to evaluate the model configuration trained and selected using the remaining data (i.e., the k<sub>outer</sub> - 1 outer folds). The model configurations are trained and selected by dividing that remaining data into k<sub>inner</sub> folds and performing simple k-fold cross validation. Nested cross validation maintains separation between training sets (k<sub>inner</sub> - 1 folds), validation sets (k<sub>inner</sub>th held-out fold), and test sets (k<sub>outer</sub>th held-out fold). This separation removes optimization bias from the evaluation of model performance and can also yield lower variance performance estimates than when a single independent test set is used [@jonathanUseCrossvalidationAssess2000].  In this study, we used 1 repeat of 10-fold cross validation for the inner loop and 3 repeats of 10-fold cross validation.  Therefore, best model configurations were selected based on the median auROC across the 10 held-out folds from the inner loop.  Final evaluation of the performance of those best model configurations was based on the median <!--consider footnote for why median--> auROC across the 30 held-out folders in the outer loop.  For completeness, we report both inner loop and outer loop median auROC for each of the three window widths (week, day, and hour).  In addition, we report key additional performance metrics for each window width from the outer loop including sensitivity, specificity, accuracy, positive predictive value and negative predictive value. We also display the Receiver Operating Characteristic curves and Precision-Recall Curves by window width.
<!--KW: will cite source for these metrics - tidymodels reference or IAML;  JC says we need only one source - likely ISL because we have to keep our citations to <= 30-->.

### Bayesian Estimation of auROC and Model Comparisons 

We used a Bayesian hierarchical generalized linear model [<!--McElreath, R. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC press. 
-->] to estimate the posterior probability distributions for auROC for the best models for each prediction window width.  These analyses allowed us to calculate 95% credible intervals for the auROC for the three best models and to determine the probability that these models' performance differed systematically from each other with respect to their auROCs.  These analyses were accomplished using the tidyposterior [@kuhnTidyposteriorBayesianAnalysis2022] and rstanarm [@goodrichBayesianAppliedRegression2023] packages in R.  We regressed the auROCs (logit transformed to address bounded, skewed distribution) from the 30 test sets for each model as a function of window width.  Following recommendations from the tidymodels team[@kuhnTidyposteriorBayesianAnalysis2022; @kuhnBayesianAnalysisResampling], we set two random intercepts; one for the repeat and another for the fold within repeat (folds are nested with repeats for auROCs collected with 3x 10-fold CV).  Following recommendations from the rstanarm team and others [@rstudioteamRStudioIntegratedDevelopment2020; @gabryPriorDistributionsRstanarm2023], we used the rstanarm default autoscaled, weakly informative, data-dependent priors that take into account the order of magnitude of the variables to provide some regularization to stabilize computation and avoid overfitting.  Specifically, the priors were set as follows: residual standard deviation ~ normal(location=0, scale=exp(2)), intercept (after centering predictors) ~ normal(location=2.3, scale=1.3), the two coefficients for window width contrasts ~ normal (location=0, scale=2.69), and covariance ~ decov(regularization=1, concentration=1, shape=1, scale=1).  We report the 95% (equal-tailed) credible intervals from the posterior probability distributions for auROC for models for each window width.  We also report 95% (equal-tailed) credible intervals for the differences in performance among the three models.  


### Shapley Additive Explanations for Feature Importance

We used Shapley Additive Explanations (SHAP) method [@lundbergUnifiedApproachInterpreting2017] to provide a consistent and objective explanation of the importance for model predictions associated with categories of features [<!--ref-->] from models for each prediction window width.  SHAP computes Shapley values, which have a solid theoretical foundation in game theory.  The use of SHAP for model interpretation is attractive because Shapley values are model-agnostic (i.e., can be computed for any statistical algorithm) and possess several useful properties including: Additivity (Shapley values for each feature can be computed independently and then summed), Efficiency (i.e., the sum of Shapley values across features must add up to the difference between the predicted and observed outcome for each observation), Symmetry (Shapley values for two features should be equal if the two features contribute equally to all possible coalitions), and Dummy (a feature that does not change the predicted value in any coalition will have a Shapley value of 0).  Finally, SHAP has a fast implementation for tree-based models that make its use computationally feasible for xgboost, even with large sample sizes.  We calculated Shapley values using the held-out folds from 3 repeats of 10-fold CV in the outer loop of our nested cross-validation, using the SHAPforxgboost package that provides Shapley values in log-odds units for binary classification models.  We averaged the three Shapley values for each observation for each feature across the three repeats to increase their stability.  To calculate global importance for categories of features (e.g., all features associated with the EMA urge item), we averaged the absolute value of the Shapley values of all features in the category across all observations.  We report these global importance scores for feature categories described earlier, separated by prediction window width. 



\newpage


# Results
<!--Information for results: Participants were on study for an average of 85 days out of the possible 90 days. Participants had endorsed using on average 4 other types of drugs (not including alcohol) over their lifetime. Additionally, participants on average scored a 9 on a self-report version of the DSM-5 symptom criteria for alcohol use disorder. Generally, scores of 2-3 are considered mild, 4-5 are considered moderate, and 6+ considered severe alcohol use disorder.-->

<!--Move to results: Across participants there were a total of 1029 unique lapses. There was variation in the frequency of lapses, ranging from 0-75 lapses per participant (M = 6.8, SD = 12.0). Only 56% of participants (N = 84) reported a lapse. However, this was expected since our participants all had a goal of abstinence from alcohol.-->

<!--Citation for ROC cutoffs - https://journals.copmadrid.org/ejpalc/art/ejpalc2018a5 (.58 = small effect size, .69 = medium effect size, .79 = large effect size, corresponding to Cohen's d of .2, .5, .8 respectively).-->

<!--From methods: Participants were mostly white (87%), roughly half were male (51%), and the mean age was 41 years (SD = 12). -->

<!--
This process resulted in a total of 267,283 features for 1-hour lapse windows, 274,175 features for 24-hour lapse windows, and 270,077 features for 168-hour lapse windows.-->


Hour - 267287 observations, 286 features
Day - 274179 observations, 286 features
Week - 270081 observations, 286 features

## Demographic and Clinical Characteristics

One hundred ninety-two participants were eligible for enrollment. Of these participants, 191 consented to participate in the study at the screening session and 169 subsequently enrolled in the study at the enrollment visit which occurred approximately one week later. Fifteen participants discontinued prior to the first monthly follow-up visit. Figure S1 presents a CONSORT diagram that displays more detail on enrollment and disposition for all eligible participants.  <!--GEF: i'm left wondering why these people didn't enroll - though not sure if we have that information. would maybe be answered in consort diagram--> 
<!--KW: Consort diagram has reasons for not continuing!-->

We excluded data from one participant who appeared not to have a goal of abstinence during their participation (i.e., they had lapses every day on study except for one day and reported they were uncertain if their goal was abstinence on the daily EMA and monthly follow-up surveys). We also excluded data from two participants who showed evidence of careless responding (e.g., completing 2-4 EMAs within an hour and providing different responses) and unusually low compliance (e.g., only 5 EMAs completed over one month), rendering their lapse labels unusable. 

Our final sample consisted of 151 participants. Participants provided study measures for one (N = 14), two (N = 6) or three (N = 131) months. 





The final sample of 151 participants included approximately equal numbers of men (N=77; 49%) and women (N=74; 51%) who ranged in age from 21 - 72 years old.  The sample was majority white (N=131; 87%) and non-Hispanic (N=147; 97%).  Participants self-reported a mean of 8.9 DSM-5 symptoms of alcohol use disorder (SD=5.8; range=4-11), a mean of 5.5 previous quit attempts (SD=5.8, range=0-30), and many reported using medications for AUD (N=59; 39%).  The majority of participants (N=85; 56%) reported one or more alcohol lapses during the study period.  The mean number of lapses per participant during the study period was XX (SD; range). <!--JJC will update previous with table data-->  Table 1 provides more detail on demographic and clinical characteristics of the sample.


<!-- Demographics table-->
```{r}
footnote_table_dem_a <- "N = 151"
footnote_table_dem_b <- "Two participants reported 100 or more quit attempts. We removed these outliers prior to calculating the mean (M), standard deviation (SD), and range."
```

```{r table_demo}
options(knitr.kable.NA = "")

dem <- screen %>% 
  summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
            SD = as.character(round(sd(dem_1, na.rm = TRUE), 1)),
            min = as.character(min(dem_1, na.rm = TRUE)),
            max = as.character(max(dem_1, na.rm = TRUE))) %>% 
  mutate(var = "Age",
         n = as.numeric(""),
         perc = as.numeric("")) %>% 
  select(var, n, perc, everything()) %>% 
  full_join(screen %>% 
  select(var = dem_2) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_3) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("American Indian/Alaska Native", "Asian", "Black/African American",
                           "White/Caucasian", "Other/Multiracial")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_4) %>% 
  mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
                         TRUE ~ "Yes"),
         var = fct_relevel(factor(var, c("Yes", "No")))) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_5) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Less than high school or GED degree", "High school or GED", 
                           "Some college", "2-Year degree", "College degree", "Advanced degree")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_6, dem_6_1) %>% 
  mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
                         dem_6_1 == "Part-time" ~ "Employed part-time",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Employed full-time", "Employed part-time", "Full-time student",
                           "Homemaker", "Disabled", "Retired", "Unemployed", 
                           "Temporarily laid off, sick leave, or maternity leave",
                           "Other, not otherwise specified")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  summarise(mean = format(round(mean(dem_7, na.rm = TRUE), 0), big.mark = ","),
            SD = format(round(sd(dem_7, na.rm = TRUE), 0), big.mark = ","),
            min =format(round(min(dem_7, na.rm = TRUE), 0), big.mark = ","),
            max = format(round(max(dem_7, na.rm = TRUE), 0), scientific = FALSE, big.mark = ",")) %>% 
  mutate(var = "Personal Income",
        n = as.numeric(""),
        perc = as.numeric(""),
        mean = str_c("$", as.character(mean)),
        SD = str_c("$", as.character(SD)),
        min = str_c("$", as.character(min)),
        max = as.character(max)) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", "min", "max")) %>% 
  full_join(screen %>% 
  select(var = dem_8) %>% 
  mutate(var = case_when(var == "Never Married" ~ "Never married",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Never married", "Married", "Divorced", "Separated",
                           "Widowed")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))

auh <- screen %>% 
  summarise(mean = mean(auh_1, na.rm = TRUE),
            SD = sd(auh_1, na.rm = TRUE),
            min = min(auh_1, na.rm = TRUE),
            max = max(auh_1, na.rm = TRUE)) %>% 
  mutate(var = "Age of first drink",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()) %>% 
  full_join(screen %>% 
  summarise(mean = mean(auh_2, na.rm = TRUE),
            SD = sd(auh_2, na.rm = TRUE),
            min = min(auh_2, na.rm = TRUE),
            max = max(auh_2, na.rm = TRUE)) %>% 
  mutate(var = "Age of regular drinking",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD", 
                                             "min", "max")) %>% 
  full_join(screen %>% 
  summarise(mean = mean(auh_3, na.rm = TRUE),
            SD = sd(auh_3, na.rm = TRUE),
            min = min(auh_3, na.rm = TRUE),
            max = max(auh_3, na.rm = TRUE)) %>% 
  mutate(var = "Age at which drinking became problematic",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  summarise(mean = mean(auh_4, na.rm = TRUE),
            SD = sd(auh_4, na.rm = TRUE),
            min = min(auh_4, na.rm = TRUE),
            max = max(auh_4, na.rm = TRUE)) %>% 
  mutate(var = "Age of first quit attempt",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  # filter out 2 people with 100 and 365 reported quit attempts - will make footnote in table
  filter(auh_5 < 100) %>% 
  summarise(mean = mean(auh_5, na.rm = TRUE),
            SD = sd(auh_5, na.rm = TRUE),
            min = min(auh_5, na.rm = TRUE),
            max = max(auh_5, na.rm = TRUE)) %>% 
  mutate(var = "Number of Quit Attempts*",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  select(var = auh_6_1) %>%
  mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ months)",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_2) %>%
  mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 months)",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_3) %>%
  mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_4) %>%
  mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_5) %>%
  mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
                         TRUE ~ var)) %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_6) %>%
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_6_7) %>%
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = auh_7) %>% 
  mutate(var = fct_relevel(factor(var, c("Yes", "No")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) %>% 
  rowwise() %>% 
  # calculate dsm5 score by adding up dsm5_1 through dsm5_11
  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7, 
                            dsm5_8, dsm5_9, dsm5_10, dsm5_11))) %>% 
  ungroup() %>% 
  summarise(mean = mean(dsm5_total),
            SD = sd(dsm5_total),
            min = min(dsm5_total, na.rm = TRUE),
            max = max(dsm5_total, na.rm = TRUE)) %>% 
  mutate(var = "DSM-5 Alcohol Use Disorder Symptom Count",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD",
                                             "min", "max")) %>% 
  full_join(screen %>% 
  select(var = assist_2_1) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_2) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Cannabis (marijuana, pot, grass, hash, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_3) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Cocaine (coke, crack, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_4) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_5) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Inhalants (nitrous, glue, petrol, paint thinner, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_6) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_7) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = assist_2_8) %>%
  filter(var != "Never" & !is.na(var)) %>% 
  mutate(var = "Opioids (heroin, morphine, methadone, codeine, etc.)") %>% 
  group_by(var) %>% 
  drop_na() %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100), by = c("var", "n", "perc")) 

lapses_per_subid <- screen %>% 
  select(subid) %>% 
  left_join(lapses %>% 
  tabyl(subid) %>% 
  select(-percent), by = "subid") %>% 
  mutate(n = if_else(is.na(n), 0, n),
         lapse = if_else(n > 0, "yes", "no")) 

lapse_info <- lapses_per_subid %>% 
  group_by(lapse) %>% 
  rename(var = lapse) %>% 
  mutate(var = factor(var, levels = c("yes", "no"), labels = c("Yes", "No"))) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / 151) * 100,
         mean = NA_real_,
         SD = NA_real_,
         min = NA_real_,
         max = NA_real_) %>% 
  full_join(lapses_per_subid %>% 
  summarise(mean = mean(n),
            SD = sd(n),
            min = min(n),
            max = max(n)) %>% 
  mutate(var = "Number of reported lapses"), 
  by = c("var", "mean", "SD", "min", "max"))
```

<!--UPDATE -JJC handle description of outliers for quit attempts-->
```{r}
# display and format table
dem %>% 
  bind_rows(auh %>% 
              mutate(across(mean:max, ~round(.x, 1))) %>% 
              mutate(across(mean:max, ~as.character(.x)))) %>% 
  bind_rows(lapse_info %>% 
              mutate(across(mean:max, ~round(.x, 1))) %>% 
              mutate(across(mean:max, ~as.character(.x)))) %>% 
  mutate(range = str_c(min, "-", max)) %>%
  select(-c(min, max)) %>% 
  kbl(longtable = TRUE,
      booktabs = TRUE,
      col.names = c("", "N", "%", "M", "SD", "Range"),
      align = c("l", "c", "c", "c", "c", "c"),
      digits = 1,
      caption = "Demographics and Clinical Characteristics for the Sample") %>%
  kable_styling() %>% 
  row_spec(row = 0, align = "c", italic = TRUE) %>% 
  pack_rows("Sex", 2, 3, bold = FALSE) %>% 
  pack_rows("Race", 4, 8, bold = FALSE) %>% 
  pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) %>% 
  pack_rows("Education", 11, 16, bold = FALSE) %>% 
  pack_rows("Employment", 17, 25, bold = FALSE) %>% 
  pack_rows("Marital Status", 27, 31, bold = FALSE) %>% 
  pack_rows("Alcohol Use Disorder Milestones", 32, 35, bold = FALSE) %>% 
  pack_rows("Lifetime History of Treatment (Can choose more than 1)", 37, 43, bold = FALSE) %>% 
  pack_rows("Received Medication for Alcohol Use Disorder", 44, 45, bold = FALSE) %>% 
  pack_rows("Current (Past 3 Month) Drug Use", 47, 54, bold = FALSE) %>% 
  pack_rows("Reported 1 or More Lapse During Study Period", 55, 56, bold = FALSE) %>% 
  footnote(general=footnote_table_dem_a, symbol = c(footnote_table_dem_b), 
           threeparttable = TRUE)
```


\newpage


## Model Performance

We developed and evaluated separate classification models to predict future lapses in three windows: the next week, the next day, and the next hour.  We used grouped (by participant), nested cross-validation for each of these prediction windows to train models, select best models, and evaluate those best models  This nested cross-validation resampling strategy used 1 repeat of 10-fold cross validation for the inner loop, and 3 repeats of 10-fold cross validation for the outer loop.  Our primary performance metric for model selection and evaluation was the area on the Receiver Operating Characteristic Curve (auROC).  Given this, best models were selected based on the median <!--insert note about why median--> auROC across 10 held-out validation folds in the inner loop.  These best models were evaluated using the median auROC across 30 held-out test folds in the outer loop. Of course, consistent with the nested cross-validation resampling method, validation and test sets were completely independent to remove optimization bias from the performance metrics.

We developed classification models for these three prediction windows using separate datasets that contained XX, XX, and XX <!--get numbers--> labeled lapse observations (lapse vs. no lapse) and associated features for the week, day, and hour prediction windows, respectively.  Each of these datasets was unbalanced with respect to the lapse outcome such that lapses were observed in XX (%) week windows, XX (%) day windows, and XX (%) hour windows.  As noted previously, we addressed this outcome class imbalance using up-sampling and down-sampling of the outcome during model training.  

### auROC

The median auROC across the 30 test folds was high for the week (<!--AUC; range-->), day (<!--AUC; range-->), and hour (<!--AUC; range-->) prediction windows.  The left panel of figure 1 displays the Receiver Operating Characteristic curves by prediction window derived by aggregating predictions across the 30 held-out test folds.  Figure S2 presents the individual ROC curves for each of the 30 test folds.  

We used the auROCs for the week, day and hour models across the 30 test folds to estimate the posterior probability distribution for the auROC for each model.  These posterior probability distributions are displayed by prediction window in the right panels of figure 1.  


### Other Performance Metrics


### Feature Importance







```{r}
footnote_table_metrics <- "Insert footnote"
```

```{r table_metrics}
j_thres_week <- roc_week |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)

j_thres_day <- roc_day |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)

j_thres_hour <- roc_hour |> 
  mutate(j = sensitivity + specificity - 1) |> 
  slice_max(j) |> 
#  print() |> 
  pull(.threshold)


metrics_week <- preds_week |> 
  mutate(estimate = if_else(prob > j_thres_week, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(week = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics_day <- preds_day |> 
  mutate(estimate = if_else(prob > j_thres_day, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(day = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics_hour <- preds_hour |> 
  mutate(estimate = if_else(prob > j_thres_hour, "Lapse", "No lapse"),
         estimate = factor(estimate, levels = c("Lapse", "No lapse"))) |> 
  conf_mat(truth = label, estimate = estimate) |> 
  summary() |>
  mutate(.estimate = round(.estimate, 3)) |> 
  rename(hour = .estimate,
         metric = .metric) |> 
  select(-.estimator)

metrics <- metrics_week %>% 
  full_join(metrics_day, by = "metric") %>% 
  full_join(metrics_hour, by = "metric") %>% 
  filter(metric %in% c("accuracy", "sens", "spec", "ppv", "npv"))

auc <- tibble(metric = "auc", 
              week = preds_week %>% roc_auc(prob, truth = label) %>%  
                pull(.estimate) %>% round(3), 
              day = preds_day %>% roc_auc(prob, truth = label) %>%  
                pull(.estimate) %>% round(3),
              hour = preds_hour %>% roc_auc(prob, truth = label) %>%  
                pull(.estimate) %>% round(3))

metrics <- metrics %>% 
  bind_rows(auc)

metrics <- metrics[c(6,1,2,3,4,5),]

metrics %>%
 kbl(col.names = c("Metric", "Week", "Day", "Hour"),
      booktabs = TRUE,
      digits = 2,
      align = c("r"),
      caption = "Performance Metrics by Model",
     linesep = "") %>% 
  row_spec(row = 0, align ="r") %>% 
  kable_styling(position = "left") %>% 
  footnote(general=footnote_table_metrics)
```

\clearpage




<!-- AUC figure by model w/posterior probabilities-->  
```{r caption_roc_pp}
fig_caption_roc_pp <- "Insert note here"
```

```{r fig_roc_pp, fig.cap = fig_caption_roc_pp, fig.height = 4.5, fig.width = 7}

roc_plot <- roc_all %>% 
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"), 
                        labels = c("Week", "Day", "Hour"))) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_path(linewidth = 1.25, show.legend = FALSE) +
  geom_abline(lty = 3) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Specificity",
       y = "Sensitivity") +
  scale_x_continuous(breaks = seq(0,1,.25),
                     labels = sprintf("%.2f", seq(1,0,-.25))) 

pp_tidy <- pp %>% 
  tidy(seed = 123)

q = c(.025, .5, .975)
ci <- pp_tidy %>% 
  group_by(model) %>% 
  summarize(median = quantile(posterior, probs = q[2]),
            lower = quantile(posterior, probs = q[1]), 
            upper = quantile(posterior, probs = q[3])) %>% 
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour")),
         y = 1000) |> 
  arrange(model)

pp_plot <- pp_tidy %>% 
  mutate(model = factor(model, levels = c("week", "day", "hour"),
                        labels = c("Week", "Day", "Hour"))) %>%
  ggplot() + 
  geom_histogram(aes(x = posterior, fill = model), color = "black", alpha = .4, 
                 bins = 30) +
  geom_segment(mapping = aes(y = y+100, yend = y-100, x = median, xend = median,
                           color = model), show.legend = FALSE, data = ci) +
  geom_segment(mapping = aes(y = y, yend = y, x = lower, xend = upper, color = model),
              show.legend = FALSE, data = ci) +
  # geom_text(data = ci, x = c(.93, .907, .92), y = 1000, 
  #           label = str_c(round(ci$median, 2), " [", round(ci$lower, 2), ", ", round(ci$upper, 2), "]")) +
  facet_wrap(~model, ncol = 1) +
  scale_y_continuous("Posterior Probability", breaks = c(0, 500, 1000)) +
  xlab("Area Under ROC Curve") +
  theme(strip.background = element_blank(),
        strip.text.x = element_blank())

roc_plot + pp_plot +
  plot_layout (guides = "collect") &
  theme(legend.position = "bottom")
```




<!-- PR-->
<!-- get exact sensitivity at .75 PPV -->
<!-- no output-->
```{r}

#| output: false

ppv_70 <- pr_all %>% 
  mutate(recall = round(recall, 3),
         precision = round(precision, 3),
         .threshold = round(.threshold, 3),
         model = factor(model,
                        levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) %>% 
  filter(precision == .70) %>% 
  group_by(model, precision) %>% 
  summarise(recall = mean(recall),
            threshold = mean(.threshold),
            .groups = "drop")
```

<!-- Week sensitivity at .70 PPV =  `r round(pull(filter(ppv_70, model == "Week"), recall), 3)` -->

<!-- Day sensitivity at .70 PPV =  `r round(pull(filter(ppv_70, model == "Day"), recall), 3)` -->

<!-- Hour sensitivity at .70 PPV =  `r round(pull(filter(ppv_70, model == "Hour"), recall), 3)` -->



<!-- PR curves-->

```{r caption_pr}
fig_caption_pr <- "Precision-Recall Curves for models."
```

```{r fig_pr, fig.cap = fig_caption_pr, fig.height=7}
pr_all %>% 
  mutate(model = factor(model, levels = c("1week", "1day", "1hour"),
                        labels = c("Week", "Day", "Hour"))) %>%
  ggplot(aes(x = recall, y = precision, color = model)) +
  geom_path(linewidth = 1.25) +
  geom_segment(mapping = aes(y = .75, yend = .75, x = -.5, xend = recall,
                           color = model),
               linetype = "dashed",
               alpha = .8,
               show.legend = FALSE,
               data = ppv_70) +
  geom_segment(mapping = aes(y = -.5, yend = .75, x = recall, xend = recall,
                           color = model),
               linetype = "dashed",
               alpha = .8,
               show.legend = FALSE,
               data = ppv_70) +
  coord_fixed(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(x = "Sensitivity (Recall)",
       y = "Positive Predictive Value (Precision)")
```



<!-- SHAP Importance figure-->
```{r caption_shap}
fig_caption_shapgrouped <- "Variable Importance (SHAP Values) for each Model.  Raw EMA features are grouped by the original item from the EMA. Features from demographics and the day and hour for the start of the  prediction window are also included."
```

```{r fig_shap, fig.cap = fig_caption_shapgrouped, fig.height=7}
shap_global_all %>% 
  mutate(variable_grp = reorder(variable_grp, mean_value, sum)) %>% 
  ggplot() +
  geom_bar(aes(x = variable_grp, y = mean_value, fill = model), stat = "identity", alpha = .4) +
  ylab("Mean |SHAP| value") +
  xlab("") +
  coord_flip()
```

# Discussion

## 1. Model Performance

<!-- Overall model performance -->

Models across all three time windows performed exceptionally well, yielding AUC ROCs of [], [], and [] for hour level, day level, and week level models respectively. AUC ROCs summarize the model's ability to distinguish positive and negative classes over all possible decision thresholds. AUCs above .9 are generally described as having "excellent" performance, meaning that the model will correctly assign a larger probability to a positive case than a negative case 90% of the time (<!--Jayawant, 2010-->). All three of our models perform as well as or better than existing alcohol lapse prediction models to date (<!--Berenholtz et al,  2020-->). This indicates EMA data can be used to predict alcohol lapse in the next hour, next day, and next week with high levels of sensitivity and specificity. <!--[reference graph with specific sensitivity specificity values at certain cut-points?]-->

Aspects of the analytic design also likely contributed to overall high model performance. These include: using nonlinear algorithms to capture nonlinear relationship and higher-order interactions among predictors; including change scores in predictors over time within participants to capture within-subject variability; and using a high dimensional feature feature space to capture the complex interplay among proximal and distal risk factors. Moreover, using machine learning and cross-validation allowed us to benefit from these design features while feeling confident that our models were generalizing to new data. It is important to note that our method of model selection did not consider other types of algorithms or lower dimensional feature sets. Therefore, we cannot conclude that these specific algorithms and features are required for the reported performance estimates; rather, they are sufficient for producing the reported results.

Our study was also unique in its use of grouped, nested, k-fold cross-validation for generating our reported model performances. This is the most rigorous test of generalizability of EMA lapse prediction models in the literature to date. Nested cross-validation provides an unbiased estimate of model performance due to its separation of hyperparameter tuning and model selection processes from model performance estimation. Additionally, our approach grouped all observations from each participant within folds, meaning that observations from one participant were not used for both model building and evaluation. This method results in a more realistic assessment of how our models are expected to perform in unseen data from new participants, increasing our confidence that our model will generalize beyond this specific study.

In addition to improved performance, this project builds upon previous EMA lapse prediction work in several ways. First, our models were trained on a large, treatment-seeking sample of adults in early recovery from AUD. Although prediction work in other samples (e.g., college heavy drinkers) is valuable for risk detection within those groups, it is unlikely to port to clinical samples. This sample comprises individuals who would benefit from a risk prediction model, particularly if situated within a digital therapeutic to support ongoing recovery. Second, we specifically predicted episodes of unintentional alcohol use (i.e., "lapses"). The factors that predict goal-inconsistent use may differ substantially from factors that predict other types of alcohol use (e.g., episodes of binge drinking among college students, instances of drinking among people pursuing harm reduction goals). This relatively homogeneous prediction outcome likely increased the predictive ability of our model. Finally, we collected data from participants with high frequency (4x per day) over a clinically meaningful period of time. This sampling density allowed us not only to capture changes in dynamic risk factors with high precision but also to build models whose prediction windows updated frequently. Altogether, these specifications help to maximize both the predictive ability of our model as well as the eventual clinical utility of our predictions. 

## 2. Understanding & Contextualizing Model Performance

<!--SHAPs-->

We further examined the performance of our models via analysis of SHAP values. SHAP values are a method for interpreting the predictions of machine learning models. They provide insights into the contribution of each feature (i.e., predictor variable) in a prediction, allowing us to understand the importance of individual features in the model's decision-making process. Global importance provides information about which features are the most useful to the model as a whole (i.e., across all predictions), while local importance evaluates the contribution of each feature within a single context or prediction. The direction and magnitude of SHAP values correspond to the influence of that feature in generating a positive prediction.

Figure [X] displays the global importance values for features across our best performing week, day, and hour level models. Unsurprisingly, the largest contribution to prediction of a lapse is frequency of previously reported lapses. An individual who reports lapsing frequently is more likely to lapse at any given observation in the future. Additionally, the likelihood of a lapse increases when participants report lower ratings of confidence in their ability to maintain their goal of abstinence. Congruent with relapse prevention literature, we also see increased likelihood of lapse when participants report higher levels of craving, increased magnitude of stressful events experienced in the past 24 hours, and increased exposure to situations described as "risky for [your] recovery."

However, these features alone are not enough to obtain high predictive performance. As displayed in Figure [X], over 15 features contribute to any individual prediction of lapse. Additionally, we see in Figure [local shaps] that some features with low global importance yield high importance scores at the local level. [Explain some shaps with high local importance on graph...etc]. Local importance is particularly relevant for making risk predictions actionable. The locally important features that contribute to a specific prediction might represent targets for intervention. For example, if increased craving has high local importance for a given prediction, this might suggest that intervening in craving (e.g., with an urge surfing activity) could be effective to reduce lapse risk in the moment. Importantly, many of our features with high local and/or global importance align well with the risk factors and associated intervention strategies delineated in Marlatt's Relapse Prevention model. Thus, when thinking about situating our risk prediction model in a digital therapeutic, locally important features could provide a mapping to intervention targets and recommended treatment modules. 

Our included demographic variables did not yield high global or local importance scores across models. However, these conclusions are dependent on the representatives of the sample. Despite our data having wide representation with respect to SES, gender, and age characteristics, these features did not typically emerge as significantly contributing to the lapse prediction (as measured by SHAP values). While this does not rule out these features' predictive utility, it does suggest that other EMA variables (craving, past use) are more relevant for lapse prediction than these characteristics. Race and ethnicity also did not emerge in global or local SHAPs. However, the limited representation of Black and Brown populations in the current sample warrants caution in drawing conclusions about the predictive utility of race and ethnicity. Data collection is underway for a related project in our laboratory to build a lapse risk prediction model for individuals with opioid use disorder. Participants are being recruited nationally with the explicit goal of improving geographic, racial, and ethnic diversity to match national population data.

<!-- Model comparisons -->

SHAP values also assist in contextualizing the comparative performance of our week, day, and hour level models. Predicting lapse in the next week may seem less difficult than predicting lapse in the next hour because of the wider prediction window (i.e., less temporal precision required). However, Bayesian model comparisons demonstrate all three models have comparable predictive utility. In fact, prediction of lapse within the next hour yields slightly better performance than predicting lapse in the next day or week. Differences in global SHAP values across models help to understand why this might be the case. First, week-level models cannot capture time-relevant components of the lapse window such as temporally-based features (e.g., hour of day, day of week). Individuals were more likely to report lapsing in evening hours and on weekends, which resulted in day- and hour-related features having greater importance and predictive utility in more fine-grained prediction windows. Additionally, hour-level models can take greater advantage of time-varying predictors, specifically features occurring closer in time to the lapse event.

## 3. Clinical Implementation

The goal of the current project was to build models for lapse risk prediction using EMA data. Although this is a necessary first step, the ultimate goal of this line of work is to use this model clinically. Consequently, we conducted the current project and built these models with clinical implementation in mind. We believe these models may be most effective when embedded in a digital therapeutic context for reasons of access, availability, and affordability described previously. However, even within a digital therapeutic, there are several ways a model like ours might be clinically implemented. 

First, it might be used to communicate a patient's lapse risk to their treatment provider (e.g., therapist, primary care provider). For example, a treatment provider could receive notifications about which patients are at a high risk of lapsing at the start of each shift. They could then decide if they wanted to connect with a patient or use this information to triage their availability (i.e., which patients should be seen first). Although this information would likely be helpful to the treatment provider, it comes with several limitations. It requires that the treatment provider be willing to use the risk information and that they have the bandwidth to take on this additional load (e.g., monitoring, evaluating, and taking action based on lapse risk information for a high number of patients). Indeed, this seems unlikely, as it is well documented that treatment providers are over-extended and over-whelmed with patient loads [@thephysiciansfoundation2014; @nationalacademiesofsciencesFactorsContributingClinician2019 <!--81% of physicians report being over-extended or at capacity. High job demands cited as primary contributor to burnout.-->]. It also limits provider-initiated action to broader prediction windows: it is not feasible to expect an already-overburdened treatment provider to act on lapse risk information about the next hour or even next day. Instead, an application that communicates information to treatment providers might be constrained to the next week. This offers some clinical benefit, but it removes the possibility of just-in-time interventions to act on steep, rapidly-fluctuating increases in lapse risk. Finally, it requires that patients 1) have an established treatment provider, and 2) are willing to share information related to their lapse risk with their provider. For those without health insurance, gaining access to a treatment provider could be difficult if not impossible. For those with health insurance, willingness to share lapse risk with a provider might be lower if there is a chance of that information affecting insurance coverage or cost.

Second, a risk prediction model like ours might be used to communicate lapse risk directly to the individual. This removes the treatment provider as a gatekeeper of the risk information. As a result, individuals can be alerted to their lapse risk at anytime and at any desired threshold (i.e., not just at a threshold high enough to warrant clinician intervention). This may allow individuals to feel they have more control over their data and alleviate potential concerns about unauthorized use of their lapse risk information. Unfortunately, this information on its own may not be helpful to individuals monitoring their lapse risk and could even have unintended harmful consequences. This method requires an individual to know what action to take, or what module to use in a digital therapeutic, in response to the lapse risk information. Moreover, being alerted to a high risk of lapsing, without any accompanying intervention, could result in a self-fulfilling prophecy due to feelings of loss of control, failure, or associating their lapse risk with internal, stable, and global factors (e.g., abstinence violation effect).   <!-- GEF: probably good to have a citation related to this effect -->   

Third, this type of model might be used to recommend an action that an individual could take to reduce their risk of a lapse. In this situation, it could communicate an actionable treatment recommendation to the individual based on their lapse risk and the top features contributing to that risk (e.g., recommending an urge surfing activity in response to reported high cravings). Recommendations could be mapped onto existing therapeutic frameworks shown to be effective for alcohol use disorder (e.g., CBT, mindfulness-based relapse). A recommendation-guided digital therapeutic would reduce the risk of iatrogenic effects because suggested actions based on predicted risk would likely be helpful and positive regardless of one's actual risk. For example, if an individual receives a recommendation to complete a mindfulness meditation activity in response to high stress, this activity is likely to benefit the individual whether this was a true prediction or a "false positive." This approach would also capitalize on the benefits of existing self-guided digital therapeutics (e.g., reaching people not connected with a treatment provider, around-the-clock availability). Thus, this third approach would provide the benefits associated with therapeutics while optimizing their use to guide individuals towards specific recommendations based on personalized risk factors at the right times.

A critical piece affecting all three forms of clinical implementation is PPV. Whereas sensitivity describes how many of the true positives (actual lapses) are predicted as lapses by our model, PPV describes how many of our positive predictions are in fact true positives. In other words, how accurate are our lapse predictions, and how much confidence can an individual have that a lapse risk warning from our model confers real risk of lapsing? We found that when we set our decision threshold to .5 (i.e., all probabilities  > .5 are predicted to be a lapse; default threshold), PPV was low across models. Low PPV can be problematic in that it could involve mobilizing resources and/or alerting an individual that they may be at a risk of lapsing when they actually are not. PPV can be improved in two ways: increasing the prediction window, or adjusting the decision threshold. PPV is highly influenced by an unbalanced outcome variable (e.g., fewer lapses compared to no lapses). Therefore, we saw a natural increase in PPV as our prediction windows broadened, with one week prediction windows having the highest PPV <!-- insert value here -->. Additionally, increasing the decision threshold can improve PPV - as the threshold increases, the model needs to be "more confident" that a positive prediction represents a true positive. However, increasing the threshold comes at a cost to sensitivity. This means that we may miss some lapses, but the lapses we do predict are more likely to be true lapses. <!--give specific example from our data?-->     

Whether sensitivity or PPV matters more depends on the context and the user. For example, an individual using the app may wish to know that if they receive an alert, they can trust it; similarly, a provider may have limited resources to allocate and therefore wants to be near-certain that an alert confers true lapse risk. These contexts would support raising the decision threshold to improve PPV. Conversely, modules recommended in a digital therapeutic platform in response to a lapse risk alert are likely to benefit an individual whether they are truly at risk or not. Completing these modules may even serve as a protective factor or promote continued engagement with the digital therapeutic. There is also a high personal, health, and economic cost when an individual returns to heavy substance use. Thus, these perspectives would suggest keeping the decision threshold low to avoid missing true lapses. A recommendation-guided digital therapeutic could adjust the decision threshold based on overall costs of a recommendation (e.g., sending a reminder to check in on their recovery goals vs. reaching out to a supportive friend or family member), the availability of resources, and the user's own preference.

## 4. Additional Future Directions & Limitations

A primary future direction for this line of work is the eventual clinical implementation of a lapse risk prediction model, likely into a digital therapeutic platform. However, there are other future directions that may be pursued, particularly with the aim of addressing limitations of the current study.

Although we varied the duration of the outcome windows (i.e., one hour, one day, one week), each model currently has a lag time of 0. This means that a predicted lapse might occur anytime from the moment the prediction is updated through the end of the prediction window. This method takes advantage of the most and most up-to-date information with regard to risk factors - all data collected until that point can be used, and the most recent data occur immediately prior to the onset of the prediction window. This is likely quite practical for our hour model when we consider just-in-time interventions - we want to intervene immediately in response to changes in lapse risk driven by very recent changes in proximal risk factors. However, a lag time of 0 is less well-suited to a longer outcome window - in particular, our week model. A high likelihood of a lapse predicted by our week model means that the lapse could occur in the next hour or 6 days later. This model is not saying that an individual is likely to lapse in a week (i.e., a week from now); rather, it says an individual is likely to lapse anytime in the next week beginning right now. No lag creates potential problems for recommendations that take longer to implement. Suggesting someone make an appointment with their therapist would not be helpful if someone was going to lapse in the next hour; however, it could be helpful if they were at risk of lapsing a week into the future. Therefore, it is important to consider how lag time and prediction windows interact when making intervention recommendations. Narrow windows (e.g., next hour) with no lag could be particularly useful for making immediate intervention recommendations (e.g., urge surfing, calling a supportive contact, stimulus control techniques). Lagged broad windows (e.g., next week starting one week from now) could also be especially useful for incorporating interventions that may take longer. We plan to explore models with not only different outcome windows but also different lag times. We will investigate how these models differ with respect to performance and what this might suggest for potential clinical utility.

Our prediction algorithm was trained using participants with a goal of abstinence. We view this as both a strength and a limitation. As described above, a homogeneous outcome of goal-inconsistent use likely improved model performance. This outcome also set our study apart from previous work that has examined other types of drinking outcomes (e.g., binge drinking among college students) that is less closely connected with the type of clinical sample who might eventually use our models embedded within a digital therapeutic. Additionally, many individuals in recovery from alcohol and other substance use disorders do pursue abstinence goals, and this approach is still strongly recommended by some treatments (e.g., Alcoholics Anonymous). However, some individuals prefer to pursue other recovery goals more in line with moderation or harm reduction. It is likely that the factors that predict alcohol use among individuals pursuing abstinence as in the current study differ from the factors that predict alcohol use among individuals pursuing moderation goals. Moreover, within that population, alcohol use instances would need to be differentiated as goal-consistent use (e.g., having one drink) and goal-inconsistent use (e.g., having two drinks when your goal is one; having one drink for the second time that week when your goal is to drink once per week). It is likely that this model-building approach can be adapted for other recovery goals (e.g., moderation); however, it needs to be first tested, and it may require a completely new sample for model development and evaluation.

A final consideration is the assessment burden required of individuals to provide the data used to build and maintain lapse risk prediction models. In the current study, we used 4X daily EMA surveys, where each survey ranged in length from 7 to 10 items. These EMA surveys are considered "active" sensing, in that they require action on the part of the individual to provide data. In a previous paper from our laboratory using these data, we examined the burden of providing data for this project [@wyantAcceptabilityPersonalSensing2022]. Overall, individuals found the burden to be acceptable and reported being (hypothetically) willing to continue providing data for a full year. Despite these promising results, uncertainty remains as to how long this response rate could be maintained. AUD is a chronic, relapsing disorder that requires lifelong monitoring. Currently, this burden currently falls on the individual or (intermittently) their treatment provider. A risk prediction model embedded in a digital therapeutic could shoulder this burden, but only if an individual could provide data long-term - and potentially indefinitely. 

Several future directions for research may help investigate and address this concern. First, as mentioned previously, data collection efforts are underway for a nationally-recruited sample of individuals with opioid use disorder. These individuals will participate for a full year compared to the current study's duration of 3 months. We will be able to compare completion and attrition rates across these projects to explore how burden may change as duration increases (though with the confound of different substance-using populations). Second, the current study uses all four daily EMA surveys for risk prediction. All four EMAs contain the same 7 questions, but the morning EMA includes 3 additional questions asked only once daily. A future project could examine how well models perform using only the morning EMA survey, providing an estimate of how well we might predict using lower-burden data collection of only 1X daily EMA. Third, although the current study uses only active EMA and demographic characteristics as features, the broader parent project collected many other passive signals such as GPS location, cellular communications, and cellular metadata. Future projects will determine the predictive utility of these passive signals, both on their own and perhaps in conjunction with reduced actively sensed features, to understand whether burden could be lowered in these ways. Fourth, although it cannot be addressed with the current data, future research could explore adaptations for long-term data collection. The sampling density of actively sensed signals could be reduced as individuals enter sustained recovery/remission. Active sensing could be adapted on an individual basis to focus on features that emerge as primary global features or frequent locally important for that individual, reducing the number of items assessed regularly. These and other opportunities for adaptation may improve long-term engagement, but future research will be needed to test these ideas and explore any impact on prediction accuracy or other outcomes.
  
    





\clearpage


# References