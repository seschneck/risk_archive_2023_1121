---
title: "discussion hold"
format: html

---
# Discussion

## 1. Model Performance

<!-- Overall model performance -->

Models across all three time windows performed exceptionally well, yielding AUC ROCs of [], [], and [] for hour level, day level, and week level models respectively. AUC ROCs summarize the model's ability to distinguish positive and negative classes over all possible decision thresholds. AUCs above .9 are generally described as having "excellent" performance, meaning that the model will correctly assign a larger probability to a positive case than a negative case 90% of the time (<!--Jayawant, 2010-->). All three of our models perform as well as or better than existing alcohol lapse prediction models to date (<!--Berenholtz et al,  2020-->). This indicates EMA data can be used to predict alcohol lapse in the next hour, next day, and next week with high levels of sensitivity and specificity. <!--[reference graph with specific sensitivity specificity values at certain cut-points?]-->

Aspects of the analytic design also likely contributed to overall high model performance. These include: using nonlinear algorithms to capture nonlinear relationship and higher-order interactions among predictors; including change scores in predictors over time within participants to capture within-subject variability; and using a high dimensional feature feature space to capture the complex interplay among proximal and distal risk factors. Moreover, using machine learning and cross-validation allowed us to benefit from these design features while feeling confident that our models were generalizing to new data. It is important to note that our method of model selection did not consider other types of algorithms or lower dimensional feature sets. Therefore, we cannot conclude that these specific algorithms and features are required for the reported performance estimates; rather, they are sufficient for producing the reported results.

Our study was also unique in its use of grouped, nested, k-fold cross-validation for generating our reported model performances. This is the most rigorous test of generalizability of EMA lapse prediction models in the literature to date. Nested cross-validation provides an unbiased estimate of model performance due to its separation of hyperparameter tuning and model selection processes from model performance estimation. Additionally, our approach grouped all observations from each participant within folds, meaning that observations from one participant were not used for both model building and evaluation. This method results in a more realistic assessment of how our models are expected to perform in unseen data from new participants, increasing our confidence that our model will generalize beyond this specific study.

In addition to improved performance, this project builds upon previous EMA lapse prediction work in several ways. First, our models were trained on a large, treatment-seeking sample of adults in early recovery from AUD. Although prediction work in other samples (e.g., college heavy drinkers) is valuable for risk detection within those groups, it is unlikely to port to clinical samples. This sample comprises individuals who would benefit from a risk prediction model, particularly if situated within a digital therapeutic to support ongoing recovery. Second, we specifically predicted episodes of unintentional alcohol use (i.e., "lapses"). The factors that predict goal-inconsistent use may differ substantially from factors that predict other types of alcohol use (e.g., episodes of binge drinking among college students, instances of drinking among people pursuing harm reduction goals). This relatively homogeneous prediction outcome likely increased the predictive ability of our model. Finally, we collected data from participants with high frequency (4x per day) over a clinically meaningful period of time. This sampling density allowed us not only to capture changes in dynamic risk factors with high precision but also to build models whose prediction windows updated frequently. Altogether, these specifications help to maximize both the predictive ability of our model as well as the eventual clinical utility of our predictions. 

## 2. Understanding & Contextualizing Model Performance

<!--SHAPs-->

We further examined the performance of our models via analysis of SHAP values. SHAP values are a method for interpreting the predictions of machine learning models. They provide insights into the contribution of each feature (i.e., predictor variable) in a prediction, allowing us to understand the importance of individual features in the model's decision-making process. Global importance provides information about which features are the most useful to the model as a whole (i.e., across all predictions), while local importance evaluates the contribution of each feature within a single context or prediction. The direction and magnitude of SHAP values correspond to the influence of that feature in generating a positive prediction.

Figure [X] displays the global importance values for features across our best performing week, day, and hour level models. Unsurprisingly, the largest contribution to prediction of a lapse is frequency of previously reported lapses. An individual who reports lapsing frequently is more likely to lapse at any given observation in the future. Additionally, the likelihood of a lapse increases when participants report lower ratings of confidence in their ability to maintain their goal of abstinence. Congruent with relapse prevention literature, we also see increased likelihood of lapse when participants report higher levels of craving, increased magnitude of stressful events experienced in the past 24 hours, and increased exposure to situations described as "risky for [your] recovery."

However, these features alone are not enough to obtain high predictive performance. As displayed in Figure [X], over 15 features contribute to any individual prediction of lapse. Additionally, we see in Figure [local shaps] that some features with low global importance yield high importance scores at the local level. [Explain some shaps with high local importance on graph...etc]. Local importance is particularly relevant for making risk predictions actionable. The locally important features that contribute to a specific prediction might represent targets for intervention. For example, if increased craving has high local importance for a given prediction, this might suggest that intervening in craving (e.g., with an urge surfing activity) could be effective to reduce lapse risk in the moment. Importantly, many of our features with high local and/or global importance align well with the risk factors and associated intervention strategies delineated in Marlatt's Relapse Prevention model. Thus, when thinking about situating our risk prediction model in a digital therapeutic, locally important features could provide a mapping to intervention targets and recommended treatment modules. 

Our included demographic variables did not yield high global or local importance scores across models. However, these conclusions are dependent on the representatives of the sample. Despite our data having wide representation with respect to SES, gender, and age characteristics, these features did not typically emerge as significantly contributing to the lapse prediction (as measured by SHAP values). While this does not rule out these features' predictive utility, it does suggest that other EMA variables (craving, past use) are more relevant for lapse prediction than these characteristics. Race and ethnicity also did not emerge in global or local SHAPs. However, the limited representation of Black and Brown populations in the current sample warrants caution in drawing conclusions about the predictive utility of race and ethnicity. Data collection is underway for a related project in our laboratory to build a lapse risk prediction model for individuals with opioid use disorder. Participants are being recruited nationally with the explicit goal of improving geographic, racial, and ethnic diversity to match national population data.

<!-- Model comparisons -->

SHAP values also assist in contextualizing the comparative performance of our week, day, and hour level models. Predicting lapse in the next week may seem less difficult than predicting lapse in the next hour because of the wider prediction window (i.e., less temporal precision required). However, Bayesian model comparisons demonstrate all three models have comparable predictive utility. In fact, prediction of lapse within the next hour yields slightly better performance than predicting lapse in the next day or week. Differences in global SHAP values across models help to understand why this might be the case. First, week-level models cannot capture time-relevant components of the lapse window such as temporally-based features (e.g., hour of day, day of week). Individuals were more likely to report lapsing in evening hours and on weekends, which resulted in day- and hour-related features having greater importance and predictive utility in more fine-grained prediction windows. Additionally, hour-level models can take greater advantage of time-varying predictors, specifically features occurring closer in time to the lapse event.

## 3. Clinical Implementation

The goal of the current project was to build models for lapse risk prediction using EMA data. Although this is a necessary first step, the ultimate goal of this line of work is to use this model clinically. Consequently, we conducted the current project and built these models with clinical implementation in mind. We believe these models may be most effective when embedded in a digital therapeutic context for reasons of access, availability, and affordability described previously. However, even within a digital therapeutic, there are several ways a model like ours might be clinically implemented. 

First, it might be used to communicate a patient's lapse risk to their treatment provider (e.g., therapist, primary care provider). For example, a treatment provider could receive notifications about which patients are at a high risk of lapsing at the start of each shift. They could then decide if they wanted to connect with a patient or use this information to triage their availability (i.e., which patients should be seen first). Although this information would likely be helpful to the treatment provider, it comes with several limitations. It requires that the treatment provider be willing to use the risk information and that they have the bandwidth to take on this additional load (e.g., monitoring, evaluating, and taking action based on lapse risk information for a high number of patients). Indeed, this seems unlikely, as it is well documented that treatment providers are over-extended and over-whelmed with patient loads [@thephysiciansfoundation2014; @nationalacademiesofsciencesFactorsContributingClinician2019 <!--81% of physicians report being over-extended or at capacity. High job demands cited as primary contributor to burnout.-->]. It also limits provider-initiated action to broader prediction windows: it is not feasible to expect an already-overburdened treatment provider to act on lapse risk information about the next hour or even next day. Instead, an application that communicates information to treatment providers might be constrained to the next week. This offers some clinical benefit, but it removes the possibility of just-in-time interventions to act on steep, rapidly-fluctuating increases in lapse risk. Finally, it requires that patients 1) have an established treatment provider, and 2) are willing to share information related to their lapse risk with their provider. For those without health insurance, gaining access to a treatment provider could be difficult if not impossible. For those with health insurance, willingness to share lapse risk with a provider might be lower if there is a chance of that information affecting insurance coverage or cost.

Second, a risk prediction model like ours might be used to communicate lapse risk directly to the individual. This removes the treatment provider as a gatekeeper of the risk information. As a result, individuals can be alerted to their lapse risk at anytime and at any desired threshold (i.e., not just at a threshold high enough to warrant clinician intervention). This may allow individuals to feel they have more control over their data and alleviate potential concerns about unauthorized use of their lapse risk information. Unfortunately, this information on its own may not be helpful to individuals monitoring their lapse risk and could even have unintended harmful consequences. This method requires an individual to know what action to take, or what module to use in a digital therapeutic, in response to the lapse risk information. Moreover, being alerted to a high risk of lapsing, without any accompanying intervention, could result in a self-fulfilling prophecy due to feelings of loss of control, failure, or associating their lapse risk with internal, stable, and global factors (e.g., abstinence violation effect).   <!-- GEF: probably good to have a citation related to this effect -->   

Third, this type of model might be used to recommend an action that an individual could take to reduce their risk of a lapse. In this situation, it could communicate an actionable treatment recommendation to the individual based on their lapse risk and the top features contributing to that risk (e.g., recommending an urge surfing activity in response to reported high cravings). Recommendations could be mapped onto existing therapeutic frameworks shown to be effective for alcohol use disorder (e.g., CBT, mindfulness-based relapse). A recommendation-guided digital therapeutic would reduce the risk of iatrogenic effects because suggested actions based on predicted risk would likely be helpful and positive regardless of one's actual risk. For example, if an individual receives a recommendation to complete a mindfulness meditation activity in response to high stress, this activity is likely to benefit the individual whether this was a true prediction or a "false positive." This approach would also capitalize on the benefits of existing self-guided digital therapeutics (e.g., reaching people not connected with a treatment provider, around-the-clock availability). Thus, this third approach would provide the benefits associated with therapeutics while optimizing their use to guide individuals towards specific recommendations based on personalized risk factors at the right times.

A critical piece affecting all three forms of clinical implementation is PPV. Whereas sensitivity describes how many of the true positives (actual lapses) are predicted as lapses by our model, PPV describes how many of our positive predictions are in fact true positives. In other words, how accurate are our lapse predictions, and how much confidence can an individual have that a lapse risk warning from our model confers real risk of lapsing? We found that when we set our decision threshold to .5 (i.e., all probabilities  > .5 are predicted to be a lapse; default threshold), PPV was low across models. Low PPV can be problematic in that it could involve mobilizing resources and/or alerting an individual that they may be at a risk of lapsing when they actually are not. PPV can be improved in two ways: increasing the prediction window, or adjusting the decision threshold. PPV is highly influenced by an unbalanced outcome variable (e.g., fewer lapses compared to no lapses). Therefore, we saw a natural increase in PPV as our prediction windows broadened, with one week prediction windows having the highest PPV <!-- insert value here -->. Additionally, increasing the decision threshold can improve PPV - as the threshold increases, the model needs to be "more confident" that a positive prediction represents a true positive. However, increasing the threshold comes at a cost to sensitivity. This means that we may miss some lapses, but the lapses we do predict are more likely to be true lapses. <!--give specific example from our data?-->     

Whether sensitivity or PPV matters more depends on the context and the user. For example, an individual using the app may wish to know that if they receive an alert, they can trust it; similarly, a provider may have limited resources to allocate and therefore wants to be near-certain that an alert confers true lapse risk. These contexts would support raising the decision threshold to improve PPV. Conversely, modules recommended in a digital therapeutic platform in response to a lapse risk alert are likely to benefit an individual whether they are truly at risk or not. Completing these modules may even serve as a protective factor or promote continued engagement with the digital therapeutic. There is also a high personal, health, and economic cost when an individual returns to heavy substance use. Thus, these perspectives would suggest keeping the decision threshold low to avoid missing true lapses. A recommendation-guided digital therapeutic could adjust the decision threshold based on overall costs of a recommendation (e.g., sending a reminder to check in on their recovery goals vs. reaching out to a supportive friend or family member), the availability of resources, and the user's own preference.

## 4. Additional Future Directions & Limitations

A primary future direction for this line of work is the eventual clinical implementation of a lapse risk prediction model, likely into a digital therapeutic platform. However, there are other future directions that may be pursued, particularly with the aim of addressing limitations of the current study.

Although we varied the duration of the outcome windows (i.e., one hour, one day, one week), each model currently has a lag time of 0. This means that a predicted lapse might occur anytime from the moment the prediction is updated through the end of the prediction window. This method takes advantage of the most and most up-to-date information with regard to risk factors - all data collected until that point can be used, and the most recent data occur immediately prior to the onset of the prediction window. This is likely quite practical for our hour model when we consider just-in-time interventions - we want to intervene immediately in response to changes in lapse risk driven by very recent changes in proximal risk factors. However, a lag time of 0 is less well-suited to a longer outcome window - in particular, our week model. A high likelihood of a lapse predicted by our week model means that the lapse could occur in the next hour or 6 days later. This model is not saying that an individual is likely to lapse in a week (i.e., a week from now); rather, it says an individual is likely to lapse anytime in the next week beginning right now. No lag creates potential problems for recommendations that take longer to implement. Suggesting someone make an appointment with their therapist would not be helpful if someone was going to lapse in the next hour; however, it could be helpful if they were at risk of lapsing a week into the future. Therefore, it is important to consider how lag time and prediction windows interact when making intervention recommendations. Narrow windows (e.g., next hour) with no lag could be particularly useful for making immediate intervention recommendations (e.g., urge surfing, calling a supportive contact, stimulus control techniques). Lagged broad windows (e.g., next week starting one week from now) could also be especially useful for incorporating interventions that may take longer. We plan to explore models with not only different outcome windows but also different lag times. We will investigate how these models differ with respect to performance and what this might suggest for potential clinical utility.

Our prediction algorithm was trained using participants with a goal of abstinence. We view this as both a strength and a limitation. As described above, a homogeneous outcome of goal-inconsistent use likely improved model performance. This outcome also set our study apart from previous work that has examined other types of drinking outcomes (e.g., binge drinking among college students) that is less closely connected with the type of clinical sample who might eventually use our models embedded within a digital therapeutic. Additionally, many individuals in recovery from alcohol and other substance use disorders do pursue abstinence goals, and this approach is still strongly recommended by some treatments (e.g., Alcoholics Anonymous). However, some individuals prefer to pursue other recovery goals more in line with moderation or harm reduction. It is likely that the factors that predict alcohol use among individuals pursuing abstinence as in the current study differ from the factors that predict alcohol use among individuals pursuing moderation goals. Moreover, within that population, alcohol use instances would need to be differentiated as goal-consistent use (e.g., having one drink) and goal-inconsistent use (e.g., having two drinks when your goal is one; having one drink for the second time that week when your goal is to drink once per week). It is likely that this model-building approach can be adapted for other recovery goals (e.g., moderation); however, it needs to be first tested, and it may require a completely new sample for model development and evaluation.

A final consideration is the assessment burden required of individuals to provide the data used to build and maintain lapse risk prediction models. In the current study, we used 4X daily EMA surveys, where each survey ranged in length from 7 to 10 items. These EMA surveys are considered "active" sensing, in that they require action on the part of the individual to provide data. In a previous paper from our laboratory using these data, we examined the burden of providing data for this project [@wyantAcceptabilityPersonalSensing2022]. Overall, individuals found the burden to be acceptable and reported being (hypothetically) willing to continue providing data for a full year. Despite these promising results, uncertainty remains as to how long this response rate could be maintained. AUD is a chronic, relapsing disorder that requires lifelong monitoring. Currently, this burden currently falls on the individual or (intermittently) their treatment provider. A risk prediction model embedded in a digital therapeutic could shoulder this burden, but only if an individual could provide data long-term - and potentially indefinitely. 

Several future directions for research may help investigate and address this concern. First, as mentioned previously, data collection efforts are underway for a nationally-recruited sample of individuals with opioid use disorder. These individuals will participate for a full year compared to the current study's duration of 3 months. We will be able to compare completion and attrition rates across these projects to explore how burden may change as duration increases (though with the confound of different substance-using populations). Second, the current study uses all four daily EMA surveys for risk prediction. All four EMAs contain the same 7 questions, but the morning EMA includes 3 additional questions asked only once daily. A future project could examine how well models perform using only the morning EMA survey, providing an estimate of how well we might predict using lower-burden data collection of only 1X daily EMA. Third, although the current study uses only active EMA and demographic characteristics as features, the broader parent project collected many other passive signals such as GPS location, cellular communications, and cellular metadata. Future projects will determine the predictive utility of these passive signals, both on their own and perhaps in conjunction with reduced actively sensed features, to understand whether burden could be lowered in these ways. Fourth, although it cannot be addressed with the current data, future research could explore adaptations for long-term data collection. The sampling density of actively sensed signals could be reduced as individuals enter sustained recovery/remission. Active sensing could be adapted on an individual basis to focus on features that emerge as primary global features or frequent locally important for that individual, reducing the number of items assessed regularly. These and other opportunities for adaptation may improve long-term engagement, but future research will be needed to test these ideas and explore any impact on prediction accuracy or other outcomes.
  
    

