---
title: "VIP reprex"
author: "JJC"
date: "11/4/2021"
output: html_document
---

## Notes
See the vip [tutorial](https://koalaverse.github.io/vip/articles/vip.html)

See [tutorial](https://cran.r-project.org/web/packages/datarobot/vignettes/VariableImportance.html) on using permutation method for model agnostic vip


## Setup

packages
```{r}
library(tidyverse)
library(tidymodels)
library(vip)
```

data
```{r}
data_trn <- tibble(x_1 = rnorm(100, 0, 1),
            x_2 = rnorm(100, 0, 1),
            x_3 = rnorm(100, 0, 1),
            z = 1 * x_1 + -2 * x_2,
            p = exp(z) / (1 + exp(z)),
            y = rbinom(100, 1, p)) %>% 
  select(-p, -z) %>% 
  glimpse()
```

recipe and splits
```{r}
rec <- recipe(y ~ ., data = data_trn) %>% 
  step_num2factor(y, levels = c("no", "yes"), transform = function(x) x +1) %>% 
  step_normalize(all_predictors())

summary(rec)

set.seed(20140102)
splits_boot <- data_trn %>% 
   bootstraps(times = 10, strata = "y")  # toy example with 10 splits

splits_boot
```

## Select and Train Best Model

Select best model configuration
```{r}
grid_hp <- expand_grid(penalty = exp(seq(-4, 4, length.out = 50)),
                            mixture = seq(0, 1, .2))

fits <-
  logistic_reg(penalty = tune(), 
             mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification") %>% 
  tune_grid(preprocessor = rec, 
            resamples = splits_boot, grid = grid_hp, 
            metrics = metric_set(accuracy))

fits %>% show_best()
```


Train Final Model
```{r}
feat_trn <- rec %>%
  prep(training = data_trn, strings_as_factors = FALSE) %>%
  bake(new_data = data_trn) %>% 
  glimpse()

fit <-
  logistic_reg(penalty = select_best(fits)$penalty, 
             mixture = select_best(fits)$mixture) %>%
  set_engine("glmnet") %>% 
  set_mode("classification") %>% 
  fit(y ~ ., data = feat_trn)

fit %>% 
  tidy() %>% 
  print()
```

## Model Specific VI

Use `vi()` in `vip` package

This is an example for glmnet.   Will add for random forest later.  

* Must specify `lambda` or coefficients will not work.
* Can get other `lambda` values b/c fit fit a series of lambdas
* `vi` seems to already remove sign when calculating `Importance`.  No need for `abs()`
* `vi` sorts in decreasing importance by default
```{r}
fit %>%
  vi(lambda = select_best(fits)$penalty) %>%  
  mutate(
    Variable = fct_reorder(Variable, Importance)  # this is for plotting order
  ) 

fit %>%
  vi(lambda = select_best(fits)$penalty) %>%
  mutate(
    Variable = fct_reorder(Variable, Importance)  # this is for plotting order
  ) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```


## Model Agnostic VI

This will calculate the difference (or ratio if set explicitly) in the performance metric that occurs when the 
specific feature is permuted so that it is no longer related to the outcome.

Get x and y for use with vi_permute in vi()
```{r}
x <- feat_trn %>% 
  select(contains("x_")) %>% 
  as.matrix()

y <- feat_trn %>% 
  pull(y)
```


Functions for use with `vi_permute`

* metric function needs to have parameters named actual and predicted
* prediction function needs parameters named object, and newdata.  Needs to return a factor for classification
* Should also note that the model (object) is extracted
from the $fit field of the model fit object from tidy models internally by vi_permute()
```{r}

acc_perm <- function(actual, predicted) {
  accuracy_vec(actual, predicted)
}

pred_perm <- function(object, newdata) {
  preds <- predict(object, newdata, type = "response", s = select_best(fits)$penalty)[, 1]    # HACK: using global variable for lambda in function.
  
  preds <- if_else(preds>= .5, "yes", "no") 
  preds <- factor(preds, levels = c("no", "yes"))
  return(preds)
}

```

Test functions

* vi passes fit$fit to predict()
* also passes x
```{r}
pred_perm(fit$fit, x)

acc_perm(y, pred_perm(fit$fit, x))
```


```{r}
set.seed(1234)  # for reproducibility in vi_permute ()
fit %>%
  vi(method = "permute", 
     train = x, 
     target = y,
     metric = acc_perm,
     pred_wrapper = pred_perm,   
     smaller_is_better = FALSE, 
     nsim = 100) %>%   # consider best value for nsim for stability
  mutate(
    Variable = fct_reorder(Variable, Importance)  # this is for plotting order
  ) %>%
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

### Random Forest
Select best model configuration
```{r}
grid_hp <- expand_grid(mtry = c(1, 2),
                       min_n = 2,
                       trees = 3)

fits <-
  rand_forest(mtry = tune(),
              min_n = tune(),
              trees = tune()) %>%
  set_engine("ranger",
             importance = "impurity",
             respect.unordered.factors = "order",
             seed = 102030) %>%
  set_mode("classification") %>%
  tune_grid(preprocessor = rec, 
          resamples = splits_boot, grid = grid_hp, 
          metrics = metric_set(accuracy))

fits %>% show_best()
```


Train Final Model
```{r}
feat_trn <- rec %>%
  prep(training = data_trn, strings_as_factors = FALSE) %>%
  bake(new_data = data_trn) %>% 
  glimpse()

fit <-
  rand_forest(mtry = select_best(fits)$mtry,
              min_n = select_best(fits)$min_n,
              trees = select_best(fits)$trees) %>%
  set_engine("ranger",
             importance = "impurity",
             respect.unordered.factors = "order",
             seed = 102030) %>%
  set_mode("classification") %>%
  fit(y ~ .,
      data = feat_trn)
```

```{r}
x <- feat_trn

acc_perm <- function(actual, predicted) {
  accuracy_vec(truth = actual, estimate = predicted)
}

pred_perm <- function(object, newdata) {
  preds <- predict(object, data = newdata)$predictions
  
  preds <- if_else(preds[, 2] >= .5, "yes", "no") 
  preds <- factor(preds, levels = c("no", "yes"))
  return(preds)
}

```

Test functions

```{r}
pred_perm(fit$fit, x)

acc_perm(x$y, pred_perm(fit$fit, x))
```


```{r}
set.seed(1234)  # for reproducibility in vi_permute ()
fit %>%
  vi(method = "permute", 
     train = x, 
     target = "y",
     metric = acc_perm,
     pred_wrapper = pred_perm,  
     smaller_is_better = FALSE, 
     nsim = 1) %>%   # consider best value for nsim for stability
  mutate(
    Variable = fct_reorder(Variable, Importance)  # this is for plotting order
  ) %>%
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

### KNN
Select best model configuration
```{r}
grid_hp <- expand_grid(neighbors = seq(1, 3))

fits <-
  nearest_neighbor(neighbors = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") %>% 
  tune_grid(preprocessor = rec, 
            resamples = splits_boot, grid = grid_hp, 
            metrics = metric_set(accuracy))

fits %>% show_best()
```


Train Final Model
```{r}
feat_trn <- rec %>%
  prep(training = data_trn, strings_as_factors = FALSE) %>%
  bake(new_data = data_trn) %>% 
  glimpse()

fit <-
  nearest_neighbor(neighbors =  select_best(fits)$neighbors) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") %>% 
  fit(y ~ ., data = feat_trn)
```

```{r}
x <- feat_trn 

acc_perm <- function(actual, predicted) {
  accuracy_vec(truth = actual, estimate = predicted)
}


pred_perm <- function(object, newdata) {
  predict(object, newdata)
}


```

Test functions

```{r}
pred_perm(fit$fit, x)

acc_perm(x$y, pred_perm(fit$fit, x))
```



FIX: did we decide this is possible for KNN? getting a weird error when I try to run
```{r}
set.seed(1234)  # for reproducibility in vi_permute ()
fit %>%
   vi(method = "permute", 
     train = x, 
     target = "y",
     metric = acc_perm,
     pred_wrapper = pred_perm,  
     smaller_is_better = FALSE, 
     nsim = 100) %>%   # consider best value for nsim for stability
  mutate(
    Variable = fct_reorder(Variable, Importance)  # this is for plotting order
  ) %>%
  ggplot(aes(x = Importance, y = Variable)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

