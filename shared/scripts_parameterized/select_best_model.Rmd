---
title: "Selects and fits best model configuration for `r params$data_type` for `r params$window` window and `r params$lead` lead and `r params$version`"
author: "John Curtin & Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
params:
  study: "ema"
  data_type: "all"
  window: "1week"
  lead: 0
  version: "v4"
  cv: "nested"
---

### Code Status

In (beta) use for glmnet, random forest, and xgboost

### Notes
This script reads in aggregate CHTC performance metrics, selects the best model configuration, 
fits the model locally and saves out performance metrics and predictions.

### Set Up Environment

```{r set_params}
study <- params$study
data_type <- params$data_type
window <- params$window
lead <- params$lead 
version <- params$version
cv <- params$cv
```


Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) # detect and warn about function conflicts
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("slice", "dplyr")
conflict_prefer("spec", "yardstick")
conflict_prefer("col_factor", "vroom")
conflict_prefer("vi", "vip")

library(here)  # establish project directory consistently as working directory
```


Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(vroom)
library(purrr)
library(furrr)
library(janitor)
library(ggplot2)
library(kableExtra)
library(vip)

theme_set(theme_classic()) 
```

Absolute paths
```{r, absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- str_c("P:/studydata/risk/chtc/", study)
          path_processed <- str_c("P:/studydata/risk/data_processed/", study)
          path_models <- str_c("P:/studydata/risk/models/", study)},

        # IOS paths
        Darwin = {
          path_input <- str_c("/Volumes/private/studydata/risk/chtc/", study)
          path_processed <- str_c("/Volumes/private/studydata/risk/data_processed/", study)
          path_models <- str_c("/Volumes/private/studydata/risk/models/", study)}
        )
```


Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```


Source training controls 
```{r}
source(here("../lab_support/chtc/static_files/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))
```

Detect n cores
```{r}
(n_core <- parallel::detectCores(logical = FALSE))
```

Read in aggregate CHTC metrics
```{r}
metrics_raw <- 
  vroom(here(path_processed, str_c("metrics_raw_train_", window, "_", lead, "_", version, "_", cv, ".csv")), 
        col_types = "iiiiccdddcdddddddi",
        show_col_types = FALSE)
```


### Non-nested CV (Kfold and boot)

#### Review best performing model configuration 

```{r best_model_info_kfold_and_boot}
if (cv == "kfold" | cv == "boot") {
  # Average metrics across folds for each configuration
  metrics_avg <- metrics_raw %>% 
    group_by(algorithm, feature_set, hp1, hp2, hp3, resample) %>%
     summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                       mean),
                n_jobs = n(), .groups = "drop") %>%
     relocate(n_jobs)

  # Best AUC for algorithms x feature sets X resample
  # ie best set of hps for algo
  metrics_avg %>% 
    group_by(algorithm, feature_set, resample) %>% 
    arrange(desc(roc_auc)) %>% 
    slice(1) %>% 
    ungroup %>% 
    arrange(desc(roc_auc)) %>% 
    print_kbl()

  config_best <- metrics_avg %>% 
    arrange(desc(roc_auc)) %>% 
    slice(1) %>% 
    glimpse()

  algorithm_best <- config_best %>% 
    pull(algorithm)
}
```


#### Refit resamples for best model locally

Fit or Read metrics for best model configuration
Replicates resampling metrics for best configuration using characteristics specified in study's training controls  
```{r resample_best_config_kfold_and_boot}
if (cv == "kfold" | cv == "boot") {
  path_best <- here(path_input, 
                    str_c("train_", data_type, "_", window, "_", lead, "_", version, "_", algorithm_best, "_", cv), 
                    "input")   
  source(here(path_best, "training_controls.R"))

  chunks <- str_split_fixed(data_trn, "\\.", n = Inf) # parse name from extensions
  if (length(chunks) == 2) {
    fn <- str_c("data_trn.", chunks[[2]])
  } else {
    fn <- str_c("data_trn.", chunks[[2]], ".", chunks[[3]])
  }

  # open based on file type
  if (str_detect(fn, "csv")) {
    d <- vroom(here(path_best, fn), show_col_types = FALSE) 
  } else {
    d <- readRDS(here(path_best, fn))
  }

  d <- d %>% 
    rename(y = {{y_col_name}})
    
  # build recipe
  rec <- build_recipe(d = d, job = config_best)
  
  #create splits
  set.seed(102030)
  splits <- d %>% 
    make_splits(cv_resample_type, cv_resample, cv_outer_resample, cv_inner_resample, cv_group)
  
  # resample best model config
  plan(multisession, workers = n_core)
  fits_best <- eval_best_model(config_best = config_best, rec = rec, splits = splits)
  plan(sequential) 
  
  # pull out metrics for each fold  
  metrics_best <- fits_best[[1]] 
  
  # pull out predictions with label_num
  num_fits_best_splits <- length(fits_best[[3]]$splits)
  
  for (i in 1:num_fits_best_splits) {
    
    # pull out preds
    preds_tmp <- fits_best[[3]]$.predictions[[i]] %>% 
      select(label_num = .row,
             truth = y,
             estimate = .pred_class,
             prob = .pred_pos) %>% 
      # update factor labels based on old code
      mutate(truth = if_else(truth == "no", "no_lapse", "lapse"),
             truth = factor(truth, levels = c("no_lapse", "lapse")),
             estimate = if_else(estimate == "no", "no_lapse", "lapse"),
             estimate = factor(estimate, levels = c("no_lapse", "lapse")),
             split_num = i)
    
      # join with other splits
      if (i == 1) {
        preds_best <- preds_tmp
      } else {
        preds_best <- preds_best %>% 
          rbind(preds_tmp)
      }
  }
}
```


### Nested CV

#### Review best model configurations 
Pull out best model configuration for each outer fold
```{r}
if (cv == "nested") {
  metrics_avg <- metrics_raw %>%
    group_by(outer_split_num, algorithm, feature_set, hp1, hp2, hp3, resample) %>%
    summarize(roc_auc_inner = mean(roc_auc),
               n_jobs_inner = n(), 
              .groups = "drop") %>%
    relocate(outer_split_num, n_jobs_inner) %>% 
    group_by(outer_split_num) %>%
    arrange(desc(roc_auc_inner)) %>%
    slice(1) %>%
    ungroup() %>%
    glimpse()
}
```

#### Fit each best configuration locally to get held out performance metrics
Loop over outer splits to fit best config and predict into held-out
loop over each of ten rows in metrics average
get training_outer and test_outer
Fit the specific model config from that row in training outer
predict into test_outer and get all metrics and predictions
put metrics into metrics_avg on appropriate row
bind together all predictions (labeling which outer split they can from and also add label_num)

```{r}
if (cv == "nested") {
 
  # FIX: READ IN DATA OUTSIDE LOOP?
  
  # loop over outer splits
  for (i in 1:length(metrics_avg$outer_split_num)) {
    
    # slice down to single outer split
    config_best <- metrics_avg %>%
      slice(i)
    
    # Read in data and training controls for top model configuration
    path_best <- here(path_input, 
                      str_c("train_", data_type, "_", window, "_", lead, "_", version, "_", config_best$algorithm, "_", cv), 
                      "input")   
    source(here(path_best, "training_controls.R"))

    chunks <- str_split_fixed(data_trn, "\\.", n = Inf) # parse name from extensions
    if (length(chunks) == 2) {
      fn <- str_c("data_trn.", chunks[[2]])
    } else {
      fn <- str_c("data_trn.", chunks[[2]], ".", chunks[[3]])
    }

    # open based on file type
    if (str_detect(fn, "csv")) {
      d <- vroom(here(path_best, fn), show_col_types = FALSE) 
    } else {
      d <- readRDS(here(path_best, fn))
    }
  
    d <- d %>% 
      rename(y = {{y_col_name}})
    
    # build recipe
    rec <- build_recipe(d = d, job = config_best)
  
    # create splits
    # FIX: FIGURE OUT HOW TO CREATE SPLITS OUTSIDE LOOP
    set.seed(102030)
    splits <- d %>% 
      make_splits(cv_resample_type, cv_resample, cv_outer_resample, cv_inner_resample, cv_group)
  
    # Pull out outer fold training and testing data
    training_outer <- training(splits$splits[[i]]) 
    testing_outer <- testing(splits$splits[[i]]) 
    
    # make features
    feat_in <- rec %>% 
      prep(training = training_outer, strings_as_factors = FALSE) %>% 
      bake(new_data = NULL)
  
    feat_out <- rec %>% 
      prep(training = training_outer, strings_as_factors = FALSE) %>% 
      bake(new_data = testing_outer)
    
    
    # Fit model
    if (config_best$algorithm == "xgboost") {
      model <- boost_tree(learn_rate = config_best$hp1,
                          tree_depth = config_best$hp2,
                          mtry = config_best$hp3,
                          trees = 100,  # set high but use early stopping
                          stop_iter = 10) %>% 
      set_engine("xgboost",
                 validation = 0.2) %>% 
      set_mode("classification") %>%
      fit(y ~ ., data = feat_in)
    }
      
    
    if (config_best$algorithm == "glmnet") {
      model <- logistic_reg(penalty = config_best$hp2,
                            mixture = config_best$hp1) %>%
      set_engine("glmnet") %>%
      set_mode("classification") %>%
      fit(y ~ ., data = feat_in)
    }
    
      
    if (config_best$algorithm == "random_forest") {
      model <- rand_forest(mtry = config_best$hp1,
                         min_n = config_best$hp2,
                         trees = config_best$hp3) %>%
      set_engine("ranger",
                 importance = "none",
                 respect.unordered.factors = "order",
                 oob.error = FALSE,
                 seed = 102030) %>%
      set_mode("classification") %>%
      fit(y ~ .,
          data = feat_in)
    }
      
    
    # use get_metrics function to get a tibble that shows performance metrics
    results_i <- get_metrics(model = model, feat_out = feat_out) %>% 
      pivot_wider(., names_from = "metric",
                  values_from = "estimate") %>%   
      relocate(roc_auc_outer = roc_auc,  accuracy_outer = accuracy, bal_accuracy_outer = bal_accuracy,
             sens_outer = sens, spec_outer = spec, ppv_outer = ppv, npv_outer = npv) %>% 
      mutate(outer_split_num = i)
    
    if (i == 1) {
      results <- results_i
    } else {
      results <- results %>% 
        rbind(results_i)
    }
    
    # pull out predictions
    preds_i <- testing_outer %>% 
      select(label_num, truth = y) %>% 
      cbind(predict(model, feat_out) %>% 
              select(estimate = .pred_class)) %>% 
      cbind(predict(model, feat_out, type = "prob") %>% 
              select(prob = .pred_pos)) %>% 
      # update factor labels to be consistent with kfold
      # FIX: EVENTUALLY TRUTH SHOULD BE CHANGED TO NEG INSTEAD OF NO IN RAW FEATURES FOR CONSISTENCY
      mutate(truth = if_else(truth == "no", "no_lapse", "lapse"),
             truth = factor(truth, levels = c("no_lapse", "lapse")),
             estimate = if_else(estimate == "neg", "no_lapse", "lapse"),
             estimate = factor(estimate, levels = c("no_lapse", "lapse")),
             outer_split_num = i) 
    
      # join with other splits
      if (i == 1) {
        preds_best <- preds_i
      } else {
        preds_best <- preds_best %>%
          rbind(preds_i)
      }
    
  }
  
  # join outer performance metrics with metrics_avg
  metrics_best <- metrics_avg %>% 
    full_join(results, by = c("outer_split_num")) 
}
```

Check metrics_avg and look at avg performance across all outer folds
```{r}
if (cv == "nested") {
  metrics_avg %>% 
    glimpse()
  
  metrics_best %>% 
    summarise(across(ends_with("_outer"), mean)) %>% 
    glimpse()
}
```

Check preds 
```{r}
if (cv == "nested") {
  preds_best %>% 
    glimpse()
  
  if (nrow(d) == nrow(preds_best)) {
    message("Number of predictions = Number of rows in data set. Correct!")
  }
  
  # Should be 1
  preds_best %>% 
    tabyl(label_num) %>% 
    pull(n) %>% 
    max()
}
```



FIX: DO WE WANT TO SELECT BEST MODEL WITH KFOLD USING ALL THE DATA IN THIS SCRIPT?


### Save out avg and best model metrics and predictions
```{r}
metrics_best  %>% 
  saveRDS(here(path_models, str_c("resample_metrics_best_", data_type, "_", window, "_", lead, "_", version, "_", 
                                  cv, ".rds")))

preds_best %>%
  saveRDS(here(path_models, str_c("resample_preds_best_", data_type, "_", window, "_", lead, "_", version, "_", 
                                  cv, ".rds")))
```
