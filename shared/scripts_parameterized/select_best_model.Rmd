---
title: "Selects and fits best model configuration for `r params$data_type` for `r params$window` window and `r params$lead` lead and `r params$version`"
author: "John Curtin & Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
params:
  study: "ema"
  data_type: "all"
  window: "1week"
  lead: 0
  version: "v4"
  cv: "kfold"
---

### Code Status

In (beta) use for glmnet, random forest, and xgboost

### Notes
This script reads in aggregate CHTC performance metrics, selects the best model configuration, 
fits the model locally and saves out performance metrics and predictions.

### Set Up Environment

```{r set_params}
study <- params$study
data_type <- params$data_type
window <- params$window
lead <- params$lead 
version <- params$version
cv <- params$cv
```


Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) # detect and warn about function conflicts
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("slice", "dplyr")
conflict_prefer("spec", "yardstick")
conflict_prefer("col_factor", "vroom")
conflict_prefer("vi", "vip")

library(here)  # establish project directory consistently as working directory
```


Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(vroom)
library(purrr)
library(furrr)
library(janitor)
library(ggplot2)
library(kableExtra)
library(vip)

theme_set(theme_classic()) 
```

Absolute paths
```{r, absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- str_c("P:/studydata/risk/chtc/", study)
          path_processed <- str_c("P:/studydata/risk/data_processed/", study)
          path_models <- str_c("P:/studydata/risk/models/", study)},

        # IOS paths
        Darwin = {
          path_input <- str_c("/Volumes/private/studydata/risk/chtc/", study)
          path_processed <- str_c("/Volumes/private/studydata/risk/data_processed/", study)
          path_models <- str_c("/Volumes/private/studydata/risk/models/", study)}
        )
```


Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```


Source training controls 
```{r}
source(here("../lab_support/chtc/static_files/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))
```

Detect n cores
```{r}
(n_core <- parallel::detectCores(logical = FALSE))
```

Read in aggregate CHTC metrics
```{r}
# FIX: Need to change job parameters in metrics_raw_train to work with this new workflow
# Needs split_num, outer_split_num, and inner_split_num
# Temporarily fix this for 1x10 kfold with this code (will need to resave files after discussing with JC)
# metrics_raw <- 
#   vroom(here(path_processed, str_c("metrics_raw_train_", window, "_", lead, "_", version, "_", cv, ".csv")),
#         show_col_types = FALSE) %>% 
#   mutate(split_num = case_when(n_repeat == 1 & n_fold == 1 ~ 1,
#                                n_repeat == 1 & n_fold == 2 ~ 2,
#                                n_repeat == 1 & n_fold == 3 ~ 3,
#                                n_repeat == 1 & n_fold == 4 ~ 4,
#                                n_repeat == 1 & n_fold == 5 ~ 5,
#                                n_repeat == 1 & n_fold == 6 ~ 6,
#                                n_repeat == 1 & n_fold == 7 ~ 7,
#                                n_repeat == 1 & n_fold == 8 ~ 8,
#                                n_repeat == 1 & n_fold == 9 ~ 9,
#                                n_repeat == 1 & n_fold == 10 ~ 10),
#          outer_split_num = NA,
#          inner_split_num = NA) %>% 
#   select(-c(n_repeat, n_fold)) %>% 
#   relocate(job_num, split_num, outer_split_num, inner_split_num)

metrics_raw <- 
  vroom(here(path_processed, str_c("metrics_raw_train_", window, "_", lead, "_", version, "_", cv, ".csv")), 
        col_types = "iiiiccdddcdddddddi",
        show_col_types = FALSE)
```


### Non-nested CV (Kfold and boot)

#### Review best performing model configuration 

```{r best_model_info_kfold_and_boot}
if (cv == "kfold" | cv == "boot") {
  # Average metrics across folds for each configuration
  metrics_avg <- metrics_raw %>% 
    group_by(algorithm, feature_set, hp1, hp2, hp3, resample) %>%
     summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                       mean),
                n_jobs = n(), .groups = "drop") %>%
     relocate(n_jobs)

  # Best AUC for algorithms x feature sets X resample
  # ie best set of hps for algo
  metrics_avg %>% 
    group_by(algorithm, feature_set, resample) %>% 
    arrange(desc(roc_auc)) %>% 
    slice(1) %>% 
    ungroup %>% 
    arrange(desc(roc_auc)) %>% 
    print_kbl()

  config_best <- metrics_avg %>% 
    arrange(desc(roc_auc)) %>% 
    slice(1) %>% 
    glimpse()

  algorithm_best <- config_best %>% 
    pull(algorithm)
}
```


#### Refit resamples for best model locally

Fit or Read metrics for best model configuration
Replicates resampling metrics for best configuration using characteristics specified in study's training controls  
```{r resample_best_config_kfold_and_boot}
if (cv == "kfold" | cv == "boot") {
  path_best <- here(path_input, 
                    str_c("train_", data_type, "_", window, "_", lead, "_", version, "_", algorithm_best, "_", cv), 
                    "input")   
  source(here(path_best, "training_controls.R"))

  chunks <- str_split_fixed(data_trn, "\\.", n = Inf) # parse name from extensions
  if (length(chunks) == 2) {
    fn <- str_c("data_trn.", chunks[[2]])
  } else {
    fn <- str_c("data_trn.", chunks[[2]], ".", chunks[[3]])
  }

  # open based on file type
  if (str_detect(fn, "csv")) {
    d <- vroom(here(path_best, fn), show_col_types = FALSE) 
  } else {
    d <- readRDS(here(path_best, fn))
  }

  d <- d %>% 
    rename(y = {{y_col_name}})
    
  # build recipe
  rec <- build_recipe(d = d, job = config_best)
  
  # create splits
  set.seed(102030)
  
  # FIX: Old versions of training controls need to be updated to work with make_splits
  # Temp code to convert old training controls into new parameters to work
  # cv_resample_type <- cv
  # cv_resample <- "1_x_10"
  # cv_outer_resample <- NA
  # cv_inner_resample <- NA
  # cv_group <- "subid"
  
  splits <- d %>% 
    make_splits(cv_resample_type, cv_resample, cv_outer_resample, cv_inner_resample, cv_group)
  
  # resample best model config
  plan(multisession, workers = n_core)
  fits_best <- eval_best_model(config_best = config_best, rec = rec, splits = splits)
  plan(sequential) 
  
  # pull out metrics for each fold  
  metrics_best <- fits_best[[1]] 
  
  # pull out predictions with label_num
  num_fits_best_splits <- length(fits_best[[3]]$splits)
  
  for (i in 1:num_fits_best_splits) {
    
    # pull out preds
    preds_tmp <- fits_best[[3]]$.predictions[[i]] %>% 
      select(label_num = .row,
             truth = y,
             estimate = .pred_class,
             prob = .pred_yes) %>% 
      # update factor labels based on old code
      mutate(truth = if_else(truth == "no", "no_lapse", "lapse"),
             truth = factor(truth, levels = c("no_lapse", "lapse")),
             estimate = if_else(estimate == "no", "no_lapse", "lapse"),
             estimate = factor(estimate, levels = c("no_lapse", "lapse")),
             split_num = i)
    
      # join with other splits
      if (i == 1) {
        preds_best <- preds_tmp
      } else {
        preds_best <- preds_best %>% 
          rbind(preds_tmp)
      }
  }
}
```

#### Fit best final model in all data and save

```{r}
fit_best <- fit_best_model(config_best, rec, d)

# save this model 
fit_best  %>% 
  saveRDS(here(path_models, str_c("best_model_fit_", data_type, "_", window, "_", lead, "_", version, ".rds")))
```


### Nested CV

#### Review best model configurations 
Pull out best model configuration for each outer fold
```{r}
if (cv == "nested") {
  metrics_avg <- metrics_raw %>%
    group_by(outer_split_num, algorithm, feature_set, hp1, hp2, hp3, resample) %>%
    summarize(roc_auc_inner = mean(roc_auc),
               n_jobs_inner = n(), 
              .groups = "drop") %>%
    relocate(outer_split_num, n_jobs_inner) %>% 
    group_by(outer_split_num) %>%
    arrange(desc(roc_auc_inner)) %>%
    slice(1) %>%
    ungroup() %>%
    glimpse()
}
```

#### Fit each best configuration locally to get held out performance metrics
Loop over outer splits to fit best config and predict into held-out
loop over each of ten rows in metrics average
get training_outer and test_outer
Fit the specific model config from that row in training outer
predict into test_outer and get all metrics and predictions
put metrics into metrics_avg on appropriate row
bind together all predictions (labeling which outer split they can from and also add label_num)

```{r}
if (cv == "nested") {
 
  # CONSIDER: data_trn is the same across all algorithms.  Perhaps find way to read it in once rather than
  # in loop repeatedly below.   data_trn lives in folders for all of the specific algorithms that 
  # were trained
  
  # loop over outer splits
  for (i in 1:length(metrics_avg$outer_split_num)) {
    
    # slice down to single outer split
    config_best <- metrics_avg %>%
      slice(i)
    
    # Read in data and training controls for top model configuration
    path_best <- here(path_input, 
                      str_c("train_", data_type, "_", window, "_", lead, "_", version, "_", config_best$algorithm, "_", cv), 
                      "input")   
    source(here(path_best, "training_controls.R"))

    chunks <- str_split_fixed(data_trn, "\\.", n = Inf) # parse name from extensions
    if (length(chunks) == 2) {
      fn <- str_c("data_trn.", chunks[[2]])
    } else {
      fn <- str_c("data_trn.", chunks[[2]], ".", chunks[[3]])
    }

    # open based on file type
    if (str_detect(fn, "csv")) {
      d <- vroom(here(path_best, fn), show_col_types = FALSE) 
    } else {
      d <- readRDS(here(path_best, fn))
    }
  
    d <- d %>% 
      rename(y = {{y_col_name}})
    
    # build recipe
    rec <- build_recipe(d = d, job = config_best)
  
    # create splits
    set.seed(102030)
    splits <- d %>% 
      make_splits(cv_resample_type, cv_resample, cv_outer_resample, cv_inner_resample, cv_group)
  
    # Pull out outer fold training and testing data
    training_outer <- training(splits$splits[[i]]) 
    testing_outer <- testing(splits$splits[[i]]) 
    
    # make features
    feat_in <- rec %>% 
      prep(training = training_outer, strings_as_factors = FALSE) %>% 
      bake(new_data = NULL)
  
    feat_out <- rec %>% 
      prep(training = training_outer, strings_as_factors = FALSE) %>% 
      bake(new_data = testing_outer)
    
    
    # Fit model
    if (config_best$algorithm == "xgboost") {
      model <- boost_tree(learn_rate = config_best$hp1,
                          tree_depth = config_best$hp2,
                          mtry = config_best$hp3,
                          trees = 100,  # set high but use early stopping
                          stop_iter = 10) %>% 
      set_engine("xgboost",
                 validation = 0.2) %>% 
      set_mode("classification") %>%
      fit(y ~ ., data = feat_in)
    }
      
    
    if (config_best$algorithm == "glmnet") {
      model <- logistic_reg(penalty = config_best$hp2,
                            mixture = config_best$hp1) %>%
      set_engine("glmnet") %>%
      set_mode("classification") %>%
      fit(y ~ ., data = feat_in)
    }
    
      
    if (config_best$algorithm == "random_forest") {
      model <- rand_forest(mtry = config_best$hp1,
                         min_n = config_best$hp2,
                         trees = config_best$hp3) %>%
      set_engine("ranger",
                 importance = "none",
                 respect.unordered.factors = "order",
                 oob.error = FALSE,
                 seed = 102030) %>%
      set_mode("classification") %>%
      fit(y ~ .,
          data = feat_in)
    }
      
    
    # use get_metrics function to get a tibble that shows performance metrics
    results_i <- get_metrics(model = model, feat_out = feat_out) %>% 
      pivot_wider(., names_from = "metric",
                  values_from = "estimate") %>%   
      relocate(roc_auc_outer = roc_auc,  accuracy_outer = accuracy, bal_accuracy_outer = bal_accuracy,
             sens_outer = sens, spec_outer = spec, ppv_outer = ppv, npv_outer = npv) %>% 
      mutate(outer_split_num = i)
    
    if (i == 1) {
      results <- results_i
    } else {
      results <- results %>% 
        rbind(results_i)
    }
    
    # pull out predictions
    preds_i <- testing_outer %>% 
      select(label_num, truth = y) %>% 
      cbind(predict(model, feat_out) %>% 
              select(estimate = .pred_class)) %>% 
      cbind(predict(model, feat_out, type = "prob") %>% 
              select(prob = .pred_pos)) %>% 
      # update factor labels to be consistent with kfold
      # FIX: EVENTUALLY TRUTH SHOULD BE CHANGED TO NEG INSTEAD OF NO IN RAW FEATURES FOR CONSISTENCY
      mutate(truth = if_else(truth == "no", "no_lapse", "lapse"),
             truth = factor(truth, levels = c("no_lapse", "lapse")),
             estimate = if_else(estimate == "neg", "no_lapse", "lapse"),
             estimate = factor(estimate, levels = c("no_lapse", "lapse")),
             outer_split_num = i) 
    
      # join with other splits
      if (i == 1) {
        preds_best <- preds_i
      } else {
        preds_best <- preds_best %>%
          rbind(preds_i)
      }
    
  }
  
  # join outer performance metrics with metrics_avg
  metrics_best <- metrics_avg %>% 
    full_join(results, by = c("outer_split_num")) 
}
```

Check metrics_avg and look at avg performance across all outer folds
```{r}
if (cv == "nested") {
  metrics_best %>% 
    print_kbl()
  
  metrics_best %>% 
    summarise(across(ends_with("_outer"), mean)) %>% 
    glimpse()
}
```

Check preds 
```{r}
if (cv == "nested") {
  preds_best %>% 
    glimpse()
  
  if (nrow(d) == nrow(preds_best)) {
    message("Number of predictions = Number of rows in data set. Correct!")
  } else stop("Number of predictions != Number of rows in data set!")
  
  # Should be 1
  preds_best %>% 
    tabyl(label_num) %>% 
    pull(n) %>% 
    max()
}
```

NOTE: Best model still needs to be identified and  trained using k-fold on the full data. This will need to be done on CHTC.  In most instances, this will likely
already have been done in earlier workflow before we switched to nestd.   Otherwise, we will need to train that now and use the kfold version to get best model

### Save out avg and best model metrics and predictions
```{r}
metrics_best  %>% 
  saveRDS(here(path_models, str_c("resample_metrics_best_", data_type, "_", window, "_", lead, "_", version, "_", 
                                  cv, ".rds")))

preds_best %>%
  saveRDS(here(path_models, str_c("resample_preds_best_", data_type, "_", window, "_", lead, "_", version, "_", 
                                  cv, ".rds")))
```
