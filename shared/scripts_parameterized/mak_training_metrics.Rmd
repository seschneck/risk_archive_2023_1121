---
title: "Combines training jobs from CHTC for `r params$data_type` for `r params$window` window and `r params$lead` lead and `r params$version`"
author: "John Curtin & Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
params:
  study: "messages"
  data_type: "all"
  window: "1day"
  lead: 0
  version: "v1"
  cv: "kfold"
  algorithms: "xgboost"   # "all" or name of specific algorithm
---

### Code Status

In (beta) use for glmnet, random forest, and xgboost

### Notes
This script aggregates all results/metrics for a batch or batches of jobs
that train all model configurations for a specific outcome/label window.  

### Set Up Environment

```{r set_params}
study <- params$study
data_type <- params$data_type
window <- params$window
lead <- params$lead 
version <- params$version
cv <- params$cv
algorithms <- params$algorithms
```


Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) # detect and warn about function conflicts
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("spec", "yardstick")
conflict_prefer("col_factor", "vroom")

library(here)  # establish project directory consistently as working directory
```


Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(vroom)
library(purrr)
library(furrr)
library(janitor)
library(ggplot2)
library(kableExtra)
library(vip)

theme_set(theme_classic()) 
```

Absolute paths
```{r, absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- str_c("P:/studydata/risk/chtc/", study)
          path_processed <- str_c("P:/studydata/risk/data_processed/", study)},

        # IOS paths
        Darwin = {
          path_input <- str_c("/Volumes/private/studydata/risk/chtc/", study)
          path_processed <- str_c("/Volumes/private/studydata/risk/data_processed/", study)}
        )
```


Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```


Source training controls 
```{r}
source(here("../lab_support/chtc/static_files/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))
```


### Check for proper results, error, and output files

Will do this part in parallel as needed
```{r}
(n_core <- parallel::detectCores(logical = FALSE))
plan(multisession, workers = n_core)
```

Set up object for results
```{r}
metrics_raw_all <- NULL
```

#### Batch 1: GLMNET

```{r glmnet}

if (str_detect(algorithms, "all") || str_detect(algorithms, "glmnet")) {
  (batch_name <- str_c("train_", data_type, "_", window, "_", lead, "_", version, "_glmnet"))


  # Read in jobs file
  jobs <- vroom(here(path_input, batch_name, "input", "jobs.csv"), show_col_types = FALSE)
  (n_jobs <- nrow(jobs))


  # Check counts of error and results
  err_files <- map_dfr(list.files(here(path_input, batch_name, "output", "error"), 
                                 full.names = TRUE), 
                      file.info)
  n_err_files <- nrow(err_files)
  
  results_files <- map_dfr(list.files(here(path_input, batch_name, "output", "results"), 
                                 full.names = TRUE), 
                      file.info)
  n_results_files <- nrow(results_files)

  if (!(n_jobs == n_err_files)) {
    stop(n_jobs, " jobs != ", n_err_files, " error files!") 
  } else {
    message(n_err_files, " error files detected.  Correct!")
  }
  
  if (!(n_jobs == n_results_files)) {
    stop(n_jobs, " jobs != ", n_results_files, " feature files!")
  } else {
    message(n_results_files, " results files detected.  Correct!")
  }

  # Display path/filename of non-zero error files
  err_files %>%
      filter(size > 0) %>%
      rownames_to_column("path") %>%
      pull(path)


  # read in all result CSVs

  metrics_raw_batch <- results_files %>% 
    future_map_dfr(read_csv, col_types = readr::cols())


  # Basic counts and checks


  job_nums <- as.numeric(unique(metrics_raw_batch$job_num))
  if (!(n_jobs == length(job_nums))) {
    stop(n_jobs, " jobs != ", length(job_nums), "  job_nums in metrics!") 
  } else {
    message(length(job_nums), " unique job numbs detected.  Correct!")
  }

  metrics_raw_batch %>% tabyl(job_num)
  metrics_raw_batch %>% tabyl(n_repeat)
  metrics_raw_batch %>% tabyl(n_fold)
  metrics_raw_batch %>% tabyl(algorithm)
  metrics_raw_batch %>% tabyl(feature_set)
  metrics_raw_batch %>% tabyl(hp1)
  metrics_raw_batch %>% tabyl(hp2)
  metrics_raw_batch %>% tabyl(hp3)
  metrics_raw_batch %>% tabyl(resample)
  metrics_raw_batch %>% tabyl(n_feats)


  # Add batch 1 to all metrics
  metrics_raw_all <- metrics_raw_all %>% 
    bind_rows(metrics_raw_batch)

}
```

#### Batch 2: Random Forest

```{r random_forest}
if (str_detect(algorithms, "all") || str_detect(algorithms, "random_forest")) {
  
  (batch_name <- str_c("train_", data_type, "_", window, "_", lead, "_", version, "_random_forest"))

  # Read in jobs file
  jobs <- vroom(here(path_input, batch_name, "input", "jobs.csv"), show_col_types = FALSE)
  
  (n_jobs <- nrow(jobs))

  # Check counts of error, out, and results
  err_files <- map_dfr(list.files(here(path_input, batch_name, "output", "error"), 
                                 full.names = TRUE), 
                      file.info)
  n_err_files <- nrow(err_files)
  
  results_files <- map_dfr(list.files(here(path_input, batch_name, "output", "results"), 
                                 full.names = TRUE), 
                      file.info)
  n_results_files <- nrow(results_files)

  if (!(n_jobs == n_err_files)) {
    stop(n_jobs, " jobs != ", n_err_files, " error files!") 
  } else {
    message(n_err_files, " error files detected.  Correct!")
  }
  
  if (!(n_jobs == n_results_files)) {
    stop(n_jobs, " jobs != ", n_results_files, " feature files!")
  } else {
    message(n_results_files, " results files detected.  Correct!")
  }

  # Display path/filename of non-zero error files
  err_files %>%
      filter(size > 0) %>%
      rownames_to_column("path") %>%
      pull(path)

  # read in all result CSVs
  metrics_raw_batch <- results_files %>% 
    future_map_dfr(read_csv, col_types = readr::cols())

  # Basic counts and checks
  job_nums <- as.numeric(unique(metrics_raw_batch$job_num))
  if (!(n_jobs == length(job_nums))) {
    stop(n_jobs, " jobs != ", length(job_nums), "  job_nums in metrics!") 
  } else {
    message(length(job_nums), " unique job numbs detected.  Correct!")
  }
  
  metrics_raw_batch %>% tabyl(job_num)
  metrics_raw_batch %>% tabyl(n_repeat)
  metrics_raw_batch %>% tabyl(n_fold)
  metrics_raw_batch %>% tabyl(algorithm)
  metrics_raw_batch %>% tabyl(feature_set)
  metrics_raw_batch %>% tabyl(hp1)
  metrics_raw_batch %>% tabyl(hp2)
  metrics_raw_batch %>% tabyl(hp3)
  metrics_raw_batch %>% tabyl(resample)
  metrics_raw_batch %>% tabyl(n_feats)


  # Add batch 2 to all metrics
  metrics_raw_all <- metrics_raw_all %>% 
    bind_rows(metrics_raw_batch)
}
```

#### Batch 3: XGBoost

```{r xgboost}

if (str_detect(algorithms, "all") || str_detect(algorithms, "xgboost")) {
  
  (batch_name <- str_c("train_", data_type, "_", window, "_", lead, "_", version, "_xgboost", "_", cv))

  # Read in jobs file
  jobs <- vroom(here(path_input, batch_name, "input", "jobs.csv"), show_col_types = FALSE)
  
  (n_jobs <- nrow(jobs))

  # Check counts of error and results
  err_files <- map_dfr(list.files(here(path_input, batch_name, "output", "error"), 
                                 full.names = TRUE), 
                      file.info)
  n_err_files <- nrow(err_files)
  
  results_files <- map_dfr(list.files(here(path_input, batch_name, "output", "results"), 
                                 full.names = TRUE), 
                      file.info)
  n_results_files <- nrow(results_files)


  if (!(n_jobs == n_err_files)) {
    stop(n_jobs, " jobs != ", n_err_files, " error files!") 
  } else {
    message(n_err_files, " error files detected.  Correct!")
  }
  
  if (!(n_jobs == n_results_files)) {
    stop(n_jobs, " jobs != ", n_results_files, " feature files!")
  } else {
    message(n_results_files, " results files detected.  Correct!")
  }

  # Display path/filename of non-zero error files
  err_files %>%
      filter(size > 0) %>%
      rownames_to_column("path") %>%
      pull(path)


  # read in all result CSVs
  metrics_raw_batch <- rownames(results_files) %>% 
    future_map_dfr(read_csv, col_types = readr::cols())

  # Basic counts and checks
  job_nums <- as.numeric(unique(metrics_raw_batch$job_num))
  if (!(n_jobs == length(job_nums))) {
    warning(n_jobs, " jobs != ", length(job_nums), "  job_nums in metrics!") 
  } else {
    message(length(job_nums), " unique job numbs detected.  Correct!")
  }
  
  metrics_raw_batch %>% tabyl(job_num)
  
  # FIX: CHANGE n_repeat AND n_fold TO split_num OR NESTED EQUIVALENT
  metrics_raw_batch %>% tabyl(n_repeat)
  metrics_raw_batch %>% tabyl(n_fold)
  
  metrics_raw_batch %>% tabyl(algorithm)
  metrics_raw_batch %>% tabyl(feature_set)
  metrics_raw_batch %>% tabyl(hp1)
  metrics_raw_batch %>% tabyl(hp2)
  metrics_raw_batch %>% tabyl(hp3)
  metrics_raw_batch %>% tabyl(resample)
  metrics_raw_batch %>% tabyl(n_feats)

  # Add batch 3 to all metrics
  metrics_raw_all <- metrics_raw_all %>% 
    bind_rows(metrics_raw_batch)
}
```



### Wrap up processing of raw metrics


back to sequential
```{r}
plan(sequential)
```


Save raw metrics file
```{r}
# FIX: ADD NESTED TO FILE NAME TO NOT OVERWRITE NON-NESTED METRICS

metrics_raw_all %>% 
  vroom_write(here(path_processed, str_c("metrics_raw_train_", window, "_", lead, "_", version, ".csv")), delim = ",")
```

### Average metrics across folds for model configurations


```{r}
## FIX: For nested, group by outer split num first?
# metrics_avg <- metrics_raw_all %>%
#   group_by(outer_split_num, algorithm, feature_set, hp1, hp2, hp3, resample) %>%
#   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
#                     mean),
#              n_jobs = n(), .groups = "drop") %>%
#   relocate(outer_split_num, n_jobs) %>% 
#   glimpse()

## Then get best model for each outer fold
# metrics_avg <- metrics_avg %>% 
#   group_by(outer_split_num) %>% 
#   arrange(desc(roc_auc)) %>% 
#   slice(1) %>%
#   ungroup() %>% 
#   glimpse()

## Average performances
# metrics_avg_best <- metrics_avg %>%
#   group_by(n_jobs, algorithm) %>%  # to retain these columns
#   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
#                     mean), .groups = "drop") %>%
#   glimpse()



metrics_avg <- metrics_raw_all %>% 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample) %>% 
   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                    mean),
             n_jobs = n(), .groups = "drop") %>% 
  relocate(n_jobs)
```

Brief check of job counts.  
Number of jobs should = folds X repeats 
```{r}
metrics_avg %>% 
  group_by(algorithm) %>% 
  tabyl(n_jobs)
```

Save average metrics
```{r}
# FIX: ADD NESTED TO FILE NAME TO NOT OVERWRITE NON-NESTED METRICS

metrics_avg %>% 
  vroom_write(here(path_processed, str_c("metrics_avg_train_", window, "_", lead, "_", version, "_", cv, ".csv")), delim = ",")
```


### View best performing models 


FIX: FEATURE SET AND RESAMPLE NOT RETAINED IN NESTED?

Best AUC for algorithms x feature sets
```{r}
metrics_avg %>% 
  group_by(algorithm, feature_set, resample) %>% 
  arrange(desc(roc_auc)) %>% 
  slice(1) %>% 
  ungroup %>% 
  arrange(desc(roc_auc)) %>% 
  print_kbl()
```


### Plot hyperparameters

FIX: HYPERPARAMETERS NOT RETAIN IN NESTED AVERAGED METRICS

```{r}
# update algorithms to actual ones in the tibble
algorithms <- unique(metrics_avg$algorithm) 

feature_sets <- unique(metrics_avg$feature_set) 

for (k in algorithms) {
  
  results_k <- metrics_avg %>% 
      filter(algorithm == k)
  
  for (i in feature_sets) {
  
    results_i <- results_k %>% 
      filter(feature_set == i)
    
    
    # glmnet
    if (k == "glmnet") {
  
      plot_title <- str_c("Plotting glmnet hyperparameters for ", i, " feature set")
  
  
      plot_i <- results_i %>%
        mutate(hp1 = factor(hp1, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = log(hp2), 
                         y = roc_auc, 
                         group = hp1, 
                         color = hp1)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "mixture (alpha)") +
          labs(title = plot_title, x = "penalty (lambda)", y = "ROC AUC")
  
      print(plot_i)
    }


    # random forest
    if (k == "random_forest") {
      
      plot_title <- str_c("Plotting RF hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
        mutate(hp2 = factor(hp2, ordered = TRUE),
              resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = roc_auc, 
                         group = hp2, 
                         color = hp2)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "min n") +
          labs(title = plot_title, x = "mtry", y = "ROC AUC")
      
       print(plot_i)
    }  
    
    # XGBoost
    if (k == "xgboost") {
      
      plot_title <- str_c("Plotting XGBoost hyperparameters for ", i, " feature set and DOWNSAMPLE")
      plot_i <- results_i %>%
        mutate(log_hp1 = log10(hp1),
               hp2 = factor(hp2, ordered = TRUE),
               hp3 = factor(hp3, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        filter(resample == "down") %>% 
        ggplot(mapping = aes(x = log_hp1, 
                         y = roc_auc, 
                         group = hp3, 
                         color = hp3)) +
          geom_line() +
          facet_grid(vars(hp2)) +
          scale_color_discrete(name = "mtry") +
          labs(title = plot_title, x = "learning rate", y = "ROC AUC")
      
       print(plot_i)
       
      plot_title <- str_c("Plotting XGBoost hyperparameters for ", i, " feature set and UPSAMPLE")
      plot_i <- results_i %>%
        filter(!hp3 == 2) %>% 
        mutate(log_hp1 = log10(hp1),
               hp2 = factor(hp2, ordered = TRUE),
               hp3 = factor(hp3, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        filter(resample == "up") %>% 
        ggplot(mapping = aes(x = log_hp1, 
                         y = roc_auc, 
                         group = hp3, 
                         color = hp3)) +
          geom_line() +
          facet_grid(vars(hp2)) +
          scale_color_discrete(name = "mtry") +
          labs(title = plot_title, x = "log10 learning rate", y = "ROC AUC")
      
       print(plot_i)
    }  
  
    
    
    # # knn
    # if (k == "knn") {
    #   
    #   plot_title <- str_c("Plotting knn hyperparameters for ", i, " feature set")
    #   
    #   plot_i <- results_i %>%
    #     mutate(resample = case_when(resample == "none" ~ "none_19",
    #                                 TRUE ~ resample)) %>% 
    #     separate(resample, c("resample", "under_ratio"), "_") %>% 
    #     mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
    #     ggplot(mapping = aes(x = hp1, 
    #                      y = bal_accuracy)) +
    #       geom_line() +
    #       facet_grid(under_ratio ~ resample) +
    #       labs(title = plot_title, x = "neighbors", y = "balanced accuracy")
    #   
    #     print(plot_i)
    # }
  }
}
```



