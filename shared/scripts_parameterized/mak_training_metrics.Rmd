---
title: "Combines training jobs from CHTC for `r params$data_type` for `r params$window` window and `r params$lead` lead and `r params$version`"
author: "John Curtin & Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
params:
  data_type: "ema"
  window: "1week"
  lead: 0
  version: "v4"
---

### Code Status

In (beta) use for glmnet, random forest, and xgboost

### Notes
This script aggregates all results/metrics for a batch or batches of jobs
that train all model configurations for a specific outcome/label window.  

### Set Up Environment

```{r set_params}
data_type <- params$data_type
window <- params$window
lead <- params$lead 
version <- params$version
```


Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) # detect and warn about function conflicts
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("spec", "yardstick")
conflict_prefer("col_factor", "vroom")

library(here)  # establish project directory consistently as working directory
```


Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(vroom)
library(purrr)
library(furrr)
library(janitor)
library(ggplot2)
library(kableExtra)
library(vip)

theme_set(theme_classic()) 
```

Absolute paths
```{r, absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- str_c("P:/studydata/risk/chtc/", data_type)
          path_processed <- str_c("P:/studydata/risk/data_processed/", data_type)},

        # IOS paths
        Darwin = {
          path_input <- str_c("/Volumes/private/studydata/risk/chtc/", data_type)
          path_processed <- str_c("/Volumes/private/studydata/risk/data_processed/", data_type)}
        )
```


Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```


Source training controls 
```{r}
source(here("../lab_support/chtc/static_files/input/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))

```


### Check for proper results, error, and output files

Will do this part in parallel as needed
```{r}
(n_core <- parallel::detectCores(logical = FALSE))
plan(multisession, workers = n_core)
```

#### Batch 1: GLMNET

```{r start_glmnet}
(batch_name <- str_c("train_", window, "_", lead, "_", version, "_glmnet"))
```

Read in jobs file
```{r}
jobs <- vroom(here(path_input, batch_name, "input", "jobs.csv"), show_col_types = FALSE)

(n_jobs <- nrow(jobs))
```


```{r}
err_files <- map_dfr(list.files(here(path_input, batch_name, "output", "error"), 
                               full.names = TRUE), 
                    file.info)
n_err_files <- nrow(err_files)

results_files <- map_dfr(list.files(here(path_input, batch_name, "output", "results"), 
                               full.names = TRUE), 
                    file.info)
n_results_files <- nrow(results_files)
```

Check counts of error and results
```{r}
if (!(n_jobs == n_err_files)) {
  stop(n_jobs, " jobs != ", n_err_files, " error files!") 
} else {
  message(n_err_files, " error files detected.  Correct!")
}

if (!(n_jobs == n_results_files)) {
  stop(n_jobs, " jobs != ", n_results_files, " feature files!")
} else {
  message(n_results_files, " results files detected.  Correct!")
}
```

Display path/filename of non-zero error files
```{r}
err_files %>%
    filter(size > 0) %>%
    rownames_to_column("path") %>%
    pull(path)
```

read in all result CSVs
```{r}
metrics_raw_batch <- list.files(here(path_input, batch_name, "output", "results"), 
                                full.names = TRUE) %>% 
  future_map_dfr(read_csv, col_types = readr::cols())
```


Basic counts and checks

```{r}
job_nums <- as.numeric(unique(metrics_raw_batch$job_num))
if (!(n_jobs == length(job_nums))) {
  stop(n_jobs, " jobs != ", length(job_nums), "  job_nums in metrics!") 
} else {
  message(length(job_nums), " unique job numbs detected.  Correct!")
}

metrics_raw_batch %>% tabyl(job_num)
metrics_raw_batch %>% tabyl(n_repeat)
metrics_raw_batch %>% tabyl(n_fold)
metrics_raw_batch %>% tabyl(algorithm)
metrics_raw_batch %>% tabyl(feature_set)
metrics_raw_batch %>% tabyl(hp1)
metrics_raw_batch %>% tabyl(hp2)
metrics_raw_batch %>% tabyl(hp3)
metrics_raw_batch %>% tabyl(resample)
metrics_raw_batch %>% tabyl(n_feats)
```

Add batch 1 to all metrics
```{r}
metrics_raw_all <- metrics_raw_batch
```



#### Batch 2: Random Forest



```{r start_random_forest}
(batch_name <- str_c("train_", window, "_", lead, "_", version, "_random_forest"))
```

Read in jobs file
```{r}
jobs <- vroom(here(path_input, batch_name, "input", "jobs.csv"), show_col_types = FALSE)

(n_jobs <- nrow(jobs))
```


```{r}
err_files <- map_dfr(list.files(here(path_input, batch_name, "output", "error"), 
                               full.names = TRUE), 
                    file.info)
n_err_files <- nrow(err_files)

results_files <- map_dfr(list.files(here(path_input, batch_name, "output", "results"), 
                               full.names = TRUE), 
                    file.info)
n_results_files <- nrow(results_files)
```

Check counts of error, out, and results
```{r}
if (!(n_jobs == n_err_files)) {
  stop(n_jobs, " jobs != ", n_err_files, " error files!") 
} else {
  message(n_err_files, " error files detected.  Correct!")
}

if (!(n_jobs == n_results_files)) {
  stop(n_jobs, " jobs != ", n_results_files, " feature files!")
} else {
  message(n_results_files, " results files detected.  Correct!")
}
```

Display path/filename of non-zero error files
```{r}
err_files %>%
    filter(size > 0) %>%
    rownames_to_column("path") %>%
    pull(path)
```

read in all result CSVs
```{r}
metrics_raw_batch <- list.files(here(path_input, batch_name, "output", "results"), 
                                full.names = TRUE) %>% 
  future_map_dfr(read_csv, col_types = readr::cols())
```


Basic counts and checks

```{r}
job_nums <- as.numeric(unique(metrics_raw_batch$job_num))
if (!(n_jobs == length(job_nums))) {
  stop(n_jobs, " jobs != ", length(job_nums), "  job_nums in metrics!") 
} else {
  message(length(job_nums), " unique job numbs detected.  Correct!")
}

metrics_raw_batch %>% tabyl(job_num)
metrics_raw_batch %>% tabyl(n_repeat)
metrics_raw_batch %>% tabyl(n_fold)
metrics_raw_batch %>% tabyl(algorithm)
metrics_raw_batch %>% tabyl(feature_set)
metrics_raw_batch %>% tabyl(hp1)
metrics_raw_batch %>% tabyl(hp2)
metrics_raw_batch %>% tabyl(hp3)
metrics_raw_batch %>% tabyl(resample)
metrics_raw_batch %>% tabyl(n_feats)
```

Add batch 2 to all metrics
```{r}
metrics_raw_all <- metrics_raw_all %>% 
  bind_rows(metrics_raw_batch)
```


#### Batch 3: XGBoost

```{r start_xgboost}
(batch_name <- str_c("train_", window, "_", lead, "_", version, "_xgboost"))
```

Read in jobs file
```{r}
jobs <- vroom(here(path_input, batch_name, "input", "jobs.csv"), show_col_types = FALSE)

(n_jobs <- nrow(jobs))
```


```{r}
err_files <- map_dfr(list.files(here(path_input, batch_name, "output", "error"), 
                               full.names = TRUE), 
                    file.info)
n_err_files <- nrow(err_files)

results_files <- map_dfr(list.files(here(path_input, batch_name, "output", "results"), 
                               full.names = TRUE), 
                    file.info)
n_results_files <- nrow(results_files)
```

Check counts of error and results
```{r}
if (!(n_jobs == n_err_files)) {
  stop(n_jobs, " jobs != ", n_err_files, " error files!") 
} else {
  message(n_err_files, " error files detected.  Correct!")
}

if (!(n_jobs == n_results_files)) {
  stop(n_jobs, " jobs != ", n_results_files, " feature files!")
} else {
  message(n_results_files, " results files detected.  Correct!")
}
```

Display path/filename of non-zero error files
```{r}
err_files %>%
    filter(size > 0) %>%
    rownames_to_column("path") %>%
    pull(path)
```

read in all result CSVs
```{r}
metrics_raw_batch <- list.files(here(path_input, batch_name, "output", "results"), 
                                full.names = TRUE) %>% 
  future_map_dfr(read_csv, col_types = readr::cols())
```


Basic counts and checks

```{r}
job_nums <- as.numeric(unique(metrics_raw_batch$job_num))
if (!(n_jobs == length(job_nums))) {
  warning(n_jobs, " jobs != ", length(job_nums), "  job_nums in metrics!") 
} else {
  message(length(job_nums), " unique job numbs detected.  Correct!")
}

metrics_raw_batch %>% tabyl(job_num)
metrics_raw_batch %>% tabyl(n_repeat)
metrics_raw_batch %>% tabyl(n_fold)
metrics_raw_batch %>% tabyl(algorithm)
metrics_raw_batch %>% tabyl(feature_set)
metrics_raw_batch %>% tabyl(hp1)
metrics_raw_batch %>% tabyl(hp2)
metrics_raw_batch %>% tabyl(hp3)
metrics_raw_batch %>% tabyl(resample)
metrics_raw_batch %>% tabyl(n_feats)
```

Add batch 3 to all metrics
```{r}
metrics_raw_all <- metrics_raw_all %>% 
  bind_rows(metrics_raw_batch)
```



### Wrap up processing of raw metrics


back to sequential
```{r}
plan(sequential)
```


Save raw metrics file
```{r}
metrics_raw_all %>% 
  vroom_write(here(path_processed, str_c("metrics_raw_train_", window, "_", lead, "_", version, ".csv")), delim = ",")
```

### Average metrics across folds for model configurations


```{r}
metrics_avg <- metrics_raw_all %>% 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample) %>% 
   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                    mean),
             n_jobs = n(), .groups = "drop") %>% 
  relocate(n_jobs)
```

Brief check of job counts.  
Number of jobs should = folds X repeats 
```{r}
metrics_avg %>% 
  group_by(algorithm) %>% 
  tabyl(n_jobs)
```

Save average metrics
```{r}
metrics_avg %>% 
  vroom_write(here(path_processed, str_c("metrics_avg_train_", window, "_", lead, "_", version, ".csv")), delim = ",")
```


### View best performing models   

Best AUC for algorithms x feature sets
```{r}
metrics_avg %>% 
  group_by(algorithm, feature_set, resample) %>% 
  arrange(desc(roc_auc)) %>% 
  slice(1) %>% 
  ungroup %>% 
  arrange(desc(roc_auc)) %>% 
  print_kbl()
```


### Plot hyperparameters

```{r}
algorithms <- unique(metrics_avg$algorithm) 
feature_sets <- unique(metrics_avg$feature_set) 

for (k in algorithms) {
  
  results_k <- metrics_avg %>% 
      filter(algorithm == k)
  
  for (i in feature_sets) {
  
    results_i <- results_k %>% 
      filter(feature_set == i)
    
    
    # glmnet
    if (k == "glmnet") {
  
      plot_title <- str_c("Plotting glmnet hyperparameters for ", i, " feature set")
  
  
      plot_i <- results_i %>%
        mutate(hp1 = factor(hp1, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = log(hp2), 
                         y = roc_auc, 
                         group = hp1, 
                         color = hp1)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "mixture (alpha)") +
          labs(title = plot_title, x = "penalty (lambda)", y = "ROC AUC")
  
      print(plot_i)
    }


    # random forest
    if (k == "random_forest") {
      
      plot_title <- str_c("Plotting RF hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
        mutate(hp2 = factor(hp2, ordered = TRUE),
              resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = roc_auc, 
                         group = hp2, 
                         color = hp2)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "min n") +
          labs(title = plot_title, x = "mtry", y = "ROC AUC")
      
       print(plot_i)
    }  
    
    # XGBoost
    if (k == "xgboost") {
      
      plot_title <- str_c("Plotting XGBoost hyperparameters for ", i, " feature set and DOWNSAMPLE")
      plot_i <- results_i %>%
        mutate(log_hp1 = log10(hp1),
               hp2 = factor(hp2, ordered = TRUE),
               hp3 = factor(hp3, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        filter(resample == "down") %>% 
        ggplot(mapping = aes(x = log_hp1, 
                         y = roc_auc, 
                         group = hp3, 
                         color = hp3)) +
          geom_line() +
          facet_grid(vars(hp2)) +
          scale_color_discrete(name = "mtry") +
          labs(title = plot_title, x = "learning rate", y = "ROC AUC")
      
       print(plot_i)
       
      plot_title <- str_c("Plotting XGBoost hyperparameters for ", i, " feature set and UPSAMPLE")
      plot_i <- results_i %>%
        filter(!hp3 == 2) %>% 
        mutate(log_hp1 = log10(hp1),
               hp2 = factor(hp2, ordered = TRUE),
               hp3 = factor(hp3, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        filter(resample == "up") %>% 
        ggplot(mapping = aes(x = log_hp1, 
                         y = roc_auc, 
                         group = hp3, 
                         color = hp3)) +
          geom_line() +
          facet_grid(vars(hp2)) +
          scale_color_discrete(name = "mtry") +
          labs(title = plot_title, x = "log10 learning rate", y = "ROC AUC")
      
       print(plot_i)
    }  
  
    
    
    # # knn
    # if (k == "knn") {
    #   
    #   plot_title <- str_c("Plotting knn hyperparameters for ", i, " feature set")
    #   
    #   plot_i <- results_i %>%
    #     mutate(resample = case_when(resample == "none" ~ "none_19",
    #                                 TRUE ~ resample)) %>% 
    #     separate(resample, c("resample", "under_ratio"), "_") %>% 
    #     mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
    #     ggplot(mapping = aes(x = hp1, 
    #                      y = bal_accuracy)) +
    #       geom_line() +
    #       facet_grid(under_ratio ~ resample) +
    #       labs(title = plot_title, x = "neighbors", y = "balanced accuracy")
    #   
    #     print(plot_i)
    # }
  }
}
```



