---
title: "Fits and evaluates best model configs in outer loop of nested for `r params$window` window and `r params$lead` lead and `r params$version`"
author: "John Curtin & Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
format:
  html:
    embed-resources: true
params:
  study: "ema"
  window: "1hour"
  lead: 0
  version: "v5"
  cv: "nested"
editor_options: 
  chunk_output_type: console
---

### Code Status

In use for EMA study

### Notes
This script reads in CHTC performance metrics from the inner loops of CV, selects the best model configuration for each outer loop, trains those models and predicts into the outer held-out folds.  Returns metrics, predictions (probabilities) and SHAPs

This script creates the following files in the `models` folder

* outer_metrics_*.rds
* outer_preds_*.rds
* outer_shaps_*.rds
* outer_shapsgrp_*.rds

where * = window_lead_version_cv


### To Do

SHAPS

* We get SHAPS in this script with 30 outer folds (3x10fold).   It will be SHAPS from different models BUT they are currently all XGBoost and regardless, SHAP could work with any algorithm.  We could ALSO average across same person for 3 repeats or just have 3x sample size SHAPs for each feature.  We choose to **average across repeats**.  

* Could get SHAPS when we did simple 3x10-fold on full sample to figure out the ONE best configuration.  First run on CHTC to get best config.  Then use that best config to predict into 30  held out folds and combine.

* Could train that ONE best config on full sample and use it for SHAPS in the full sample.   



### Set Up Environment

```{r set_params}
study <- params$study
data_type <- params$data_type
window <- params$window
lead <- params$lead 
version <- params$version
cv <- params$cv
```

Function conflicts
```{r, packages_workflow}
#| message: false
#| warning: false

# source
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")

# handle conflicts
options(conflicts.policy = "depends.ok")
tidymodels_conflictRules()
```

Packages for script
```{r, packages_script}
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(probably)
```

Source support functions
```{r source_functions}
# EDA
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")

# CHTC support functions
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/chtc/static_files/fun_chtc.R?raw=true")
```

Absolute paths
```{r, absolute_paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- str_c("P:/studydata/risk/chtc/", study)
          path_processed <- str_c("P:/studydata/risk/data_processed/", study)
          path_models <- str_c("P:/studydata/risk/models/", study)},

        # IOS paths
        Darwin = {
          path_input <- str_c("/Volumes/private/studydata/risk/chtc/", study)
          path_processed <- str_c("/Volumes/private/studydata/risk/data_processed/",
                                  study)
          path_models <- str_c("/Volumes/private/studydata/risk/models/", study)},
        
        # Linux paths
        Linux = {
          path_input <- str_c("~/mnt/private/studydata/risk/chtc/", study)
          path_processed <- str_c("~/mnt/private/studydata/risk/data_processed/",
                                  study)
          path_models <- str_c("~/mnt/private/studydata/risk/models/", study)}
        )
```


Chunk Defaults
```{r defaults}
#| include: false

knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```
*

### Script Functions

Function to clean up poor choices for feature names for SHAPs!!
```{r function_1}
clean_feature_names <- function(feat_name){
  new_name <- gsub(".l0", "", feat_name)
  new_name <- gsub("rratecount.count", "raw_count", new_name)
  new_name <- gsub("dratecount.count", "diff_count", new_name)
  new_name <- gsub("drecent_response", "diff_recent", new_name)
  new_name <- gsub("rrecent_response", "raw_recent", new_name)
  new_name <- gsub("dmin_response", "diff_min", new_name)
  new_name <- gsub("rmin_response", "raw_min", new_name)
  new_name <- gsub("dmax_response", "diff_max", new_name)
  new_name <- gsub("rmax_response", "raw_max", new_name)
  new_name <- gsub("dmedian_response", "diff_median", new_name)
  new_name <- gsub("rmedian_response", "raw_median", new_name)
  new_name <- gsub("label_", "", new_name)
  new_name <- gsub("demo_", "", new_name)
  new_name <- gsub("High.school.or.less", "high.school", new_name)
  new_name <- gsub("Some.college", "some.college", new_name)
  new_name <- gsub("Mon", "mon", new_name)
  new_name <- gsub("Tue", "tue", new_name)
  new_name <- gsub("Wed", "wed", new_name)
  new_name <- gsub("Thu", "thu", new_name)
  new_name <- gsub("Fri", "fri", new_name)
  new_name <- gsub("Sat", "sat", new_name)
  new_name <- gsub("Sun", "sun", new_name)
  new_name <- gsub("Never.Married", "never.married", new_name)
  new_name <- gsub("Never.Other", "never.other", new_name)
  new_name <- gsub("White.Caucasian", "caucasian", new_name)
  new_name <- gsub("Male", "male", new_name)
  new_name <- gsub("p12.raw_count.lapse", "lapse.p12.raw_count", new_name)
  new_name <- gsub("p24.raw_count.lapse", "lapse.p24.raw_count", new_name)
  new_name <- gsub("p48.raw_count.lapse", "lapse.p48.raw_count", new_name)
  new_name <- gsub("p72.raw_count.lapse", "lapse.p72.raw_count", new_name)
  new_name <- gsub("p168.raw_count.lapse", "lapse.p168.raw_count", new_name)
  new_name <- gsub("p12.diff_count.lapse", "lapse.p12.diff_count", new_name)
  new_name <- gsub("p24.diff_count.lapse", "lapse.p24.diff_count", new_name)
  new_name <- gsub("p48.diff_count.lapse", "lapse.p48.diff_count", new_name)
  new_name <- gsub("p72.diff_count.lapse", "lapse.p72.diff_count", new_name)
  new_name <- gsub("p168.diff_count.lapse", "lapse.p168.diff_count", new_name)
  new_name <- gsub("p12.raw_count.ema", "missing.p12.raw_count", new_name)
  new_name <- gsub("p24.raw_count.ema", "missing.p24.raw_count", new_name)
  new_name <- gsub("p48.raw_count.ema", "missing.p48.raw_count", new_name)
  new_name <- gsub("p72.raw_count.ema", "missing.p72.raw_count", new_name)
  new_name <- gsub("p168.raw_count.ema", "missing.p168.raw_count", new_name)
  new_name <- gsub("p12.diff_count.ema", "missing.p12.diff_count", new_name)
  new_name <- gsub("p24.diff_count.ema", "missing.p24.diff_count", new_name)
  new_name <- gsub("p48.diff_count.ema", "missing.p48.diff_count", new_name)
  new_name <- gsub("p72.diff_count.ema", "missing.p72.diff_count", new_name)
  new_name <- gsub("p168.diff_count.ema", "missing.p168.diff_count", new_name)
  return(new_name) 
}
```

Function to fit, predict, and calc metrics, preds, shaps
```{r function_2}
fit_predict_eval <- function(split_num, splits, configs_best){

  # write tmp file to repo to track progress through loop
  # delete this file when script is complete.  
  write_csv(tibble(stage = "eval",
                   outer_split_num = split_num, 
                   start_time = Sys.time()),
            file.path(str_c("tmp_metrics_outer_progress_", study, "_",window)),
            append = TRUE)
  
  d_in <- training(splits$splits[[split_num]]) |> 
    select(-id_obs)  # not used for training; only needed in d_out to tag SHAPs
  d_out <- testing(splits$splits[[split_num]])
  
  config_best <- configs_best |> 
    slice(split_num) |> 
    rename(n_jobs_in = n_jobs, accuracy_in = accuracy, 
           bal_accuracy_in = bal_accuracy,
           roc_auc_in = roc_auc, sens_in =  sens, spec_in = spec, 
           ppv_in = ppv, npv_in = npv)
    
  rec <- build_recipe(d = d_in, config = config_best)
  model_best <- fit_best_model(config_best, rec, d_in, "classification")
  
  feat_out <- rec %>% 
    prep(training = d_in, strings_as_factors = FALSE) %>% 
    bake(new_data = d_out)   # no id_obs because not included in d_in
  
  # metrics from raw (uncalibrated) predictions for held out fold
  preds_prob <- predict(model_best, feat_out,
                        type = "prob")
  preds_class <- predict(model_best, feat_out, type = "class")$.pred_class

  roc <- tibble(truth = feat_out$y, 
                prob = preds_prob[[str_c(".pred_", y_level_pos)]]) %>% 
      roc_auc(prob, truth = truth, event_level = "first") %>% 
      select(metric = .metric, 
             estimate = .estimate)
  
  cm <- tibble(truth = feat_out$y, estimate = preds_class) %>% 
    conf_mat(truth, estimate)
    
  metrics_out <- cm |> 
    summary(event_level = "first") |>   
    select(metric = .metric,
           estimate = .estimate) |> 
    filter(metric %in% c("sens", "spec", "ppv", "npv", "accuracy", "bal_accuracy")) |> 
    suppressWarnings() |>  # warning not about metrics we are returning
    bind_rows(roc) |> 
    pivot_wider(names_from = "metric", values_from = "estimate") |>    
    relocate(roc_auc, sens, spec, ppv, npv, accuracy, bal_accuracy) |> 
    bind_cols(config_best) |>
    relocate(outer_split_num, algorithm, feature_set, hp1, hp2, hp3, 
             resample) |> 
    relocate(accuracy_in, bal_accuracy_in, .after = last_col())

  # train calibration model train/test split on held in data
  set.seed(2468)
  cal_split <- d_in |> 
    group_initial_split(group = all_of(cv_group), prop = 3/4)
  d_cal_in <- training(cal_split)
  d_cal_out <- testing(cal_split)
  
  feat_cal_out <- rec %>% 
    prep(training = d_cal_in, strings_as_factors = FALSE) %>% 
    bake(new_data = d_cal_out) 

  model_cal <- fit_best_model(config_best, rec, d_cal_in, "classification")
  
  # iso calibration
  iso <- predict(model_cal, feat_cal_out,
                 type = "prob") |> 
    mutate(truth = feat_cal_out$y) |> 
    cal_estimate_isotonic(truth = truth,
                          estimate = dplyr::starts_with(".pred_"))
  preds_prob_iso <- preds_prob |> 
    cal_apply(iso)
  
  # logistic calibration
  logi <- predict(model_cal, feat_cal_out,
                 type = "prob") |> 
    mutate(truth = feat_cal_out$y) |> 
    cal_estimate_logistic(truth = truth,
                           estimate = dplyr::starts_with(".pred_"),
                           smooth = TRUE)
  preds_prob_logi <- preds_prob |> 
    cal_apply(logi)
  
  # beta calibration
  beta <- predict(model_cal, feat_cal_out,
                 type = "prob") |> 
    mutate(truth = feat_cal_out$y) |> 
    cal_estimate_beta(truth = truth,
                           estimate = dplyr::starts_with(".pred_"),
                           smooth = TRUE)
  preds_prob_beta <- preds_prob |> 
    cal_apply(beta)
  
  # combine raw and calibrated probs
  probs_out <- tibble(outer_split_num = rep(split_num, nrow(preds_prob)),
                  prob_raw = preds_prob[[str_c(".pred_", y_level_pos)]],
                  prob_iso = preds_prob_iso[[str_c(".pred_", y_level_pos)]],
                  prob_logi = preds_prob_logi[[str_c(".pred_", y_level_pos)]],
                  prob_beta = preds_prob_beta[[str_c(".pred_", y_level_pos)]],
                  label = d_out$y) |> 
    mutate(label = fct_recode(label, "No lapse" = "no",
                              "Lapse" = "yes"))
  
  #SHAP in held out fold
  shaps_out <- SHAPforxgboost::shap.prep(xgb_model = extract_fit_engine(model_best),
                     X_train = feat_out |> select(-y) |>  as.matrix()) |> 
   # add id_obs by multiple of number of features
    mutate(id_obs = rep(d_out$id_obs, times = ncol(feat_out) - 1),
           split_num = split_num) |>  
    relocate(id_obs, split_num)
  
  return(list(probs_out = probs_out, 
              metrics_out = metrics_out, 
              shaps_out = shaps_out))
}
```

### Read in aggregate CHTC metrics for inner folds
```{r read_inner_metrics}
metrics_raw <- 
  read_csv(file.path(path_models, str_c("inner_metrics_", window, 
                                   "_", lead, "_", version, "_", cv, ".csv")), 
           col_types = "iiiiccdddcdddddddi") |> 
  glimpse()
```


### Identify best config for each outer fold (i.e., across inner folds)

Average metrics for each configuration across inner folds for each outer fold
```{r best_model_1}
metrics_avg <- metrics_raw |> 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample, outer_split_num) |> 
   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                     median),
              n_jobs = n(), .groups = "drop") |> 
  relocate(outer_split_num, n_jobs) |> 
  arrange(outer_split_num, desc(roc_auc))
```

Best configuration for each outer fold
```{r best_model_2}
configs_best <- metrics_avg |> 
  group_by(outer_split_num) |> 
  arrange(desc(roc_auc)) |> 
  slice(1) |> 
  ungroup() |> 
  relocate(roc_auc, .before = accuracy)

configs_best |> print_kbl()
```

```{r}
configs_best |> pull(roc_auc) |> mean()
configs_best |> pull(roc_auc) |> median()
```

```{r}
configs_best |> 
  ggplot(aes(x = roc_auc)) +
  geom_histogram(bins = 10)
```

### Fit best model for each outer fold and get/save metrics, preds, SHAPs

Get data from ANY batch (all same) and make splits

ASSUMPTIONS: 

* Data are same for all batches
* format_data() is same for all batches
* Assumes full recipe is for all algorithms is present in all training controls with branches/ifs to select proper algorithm specific steps

Map over all outer splits to get predicted probabilities, metrics, and SHAPs from held out outer folds.  Then save predicted probs, metrics, and SHAPs

NOTE: Delete `outer_metrics_*` or this code chunk won't run!
```{r eval_outer_folds}
if(!file.exists(file.path(path_models, str_c("outer_metrics_", 
                                  window, "_", lead, "_", version, "_", 
                                  cv, ".rds")))){ 
  
  # can source any training control given assumptions above
  batch_names <- list.dirs(path_input, full.names = FALSE, recursive = FALSE)
  batch_names <- batch_names[str_detect(batch_names, "train") & 
                               str_detect(batch_names, cv) &
                               str_detect(batch_names, version) &
                               str_detect(batch_names, window)]
  batch_name <- batch_names[1] # can source any batch given assumptions above
  path_batch <- file.path(path_input, batch_name)
  source(file.path(path_batch, "input", "training_controls.R"))
  # NOTE: training controls overwrites path_batch but it matches   
                    
  chunks <- str_split_fixed(data_trn, "\\.", n = Inf) # parse name from extensions
  if (length(chunks) == 2) {
    fn <- str_c("data_trn.", chunks[[2]])
  } else {
    fn <- str_c("data_trn.", chunks[[2]], ".", chunks[[3]])
  }
    
  # open based on file type
  if (str_detect(fn, "csv")) {
    d <- read_csv(file.path(path_batch, "input", fn), show_col_types = FALSE) 
  } else {
    d <- read_rds(file.path(path_batch, "input", fn))
  }
  
  d <- format_data(d) |> 
    mutate(id_obs = 1:nrow(d))  # tmp add for linking obs to SHAPs
  
  splits <- d %>% 
    make_splits(cv_resample_type, cv_resample, cv_outer_resample, 
                cv_inner_resample, cv_group, seed_splits)
    
  all <- configs_best$outer_split_num |> 
    map(\(split_num) fit_predict_eval(split_num, splits, configs_best))  
  
  
  rm(splits)  # save a bit of memory!
  
  write_csv(tibble(stage = "metrics_save",
                   outer_split_num = NA, 
                   start_time = Sys.time()),
            file.path(path_models, str_c("tmp_metrics_outer_progress_", window)),
            append = TRUE)
  metrics_out <- all |> 
    map(\(l) pluck(l, "metrics_out")) |> 
    list_rbind() |> 
    write_rds(file.path(path_models, str_c("outer_metrics_", 
                                           window, "_", lead, "_", version, "_", 
                                           cv, ".rds")))
  write_csv(tibble(stage = "probs_save",
                   outer_split_num = NA, 
                   start_time = Sys.time()),
            file.path(path_models, str_c("tmp_metrics_outer_progress_",window)),
            append = TRUE)  
  probs_out <- all |> 
    map(\(l) pluck(l, "probs_out")) |> 
    list_rbind() |> 
    write_rds(file.path(path_models, str_c("outer_preds_", 
                                           window, "_", lead, "_", version, "_", 
                                           cv, ".rds")))

    write_csv(tibble(stage = "shaps_save",
                   outer_split_num = NA, 
                   start_time = Sys.time()),
            file.path(path_models, str_c("tmp_metrics_outer_progress_", window)),
            append = TRUE)    
  shaps_out <- all |> 
    map(\(l) pluck(l, "shaps_out")) |>
    list_rbind() |> 
    # clean feature names;  See function above
    mutate(variable = fct_relabel(variable, clean_feature_names)) |> 
    # average SHAP metrics across repeats for same id_obs
    group_by(id_obs, variable) |> 
    summarize(value = mean(value), 
              # rfvalue is same across repeats but want included 
              rfvalue =  mean(rfvalue),  
              mean_value = mean(mean_value)) |> 
    write_rds(file.path(path_models, str_c("outer_shaps_", 
                                           window, "_", lead, "_", version, "_", 
                                           cv, ".rds")))
    
} else {
  message("Resampled performance from nested CV previously calculated")
  
  metrics_out <- read_rds(file.path(path_models, str_c("outer_metrics_", 
                                                       window, "_", lead, "_", 
                                                       version, "_", 
                                                       cv, ".rds")))
  # not needed for remainder of script
  # probs_out <- read_rds(file.path(path_models, str_c("outer_preds_", 
  #                                                    window, "_", lead, "_", 
  #                                                    version, "_", 
  #                                                    cv, ".rds")))

  shaps_out <- read_rds(file.path(path_models, str_c("outer_shaps_", 
                                                     window, "_", lead, "_", 
                                                     version, "_", 
                                                     cv, ".rds")))
}
```

Now group SHAPs
NOTE: Delete `outer_shapsgrp_*` or this code chunk won't run!
```{r calc_grouped_shaps}
if(!file.exists(file.path(path_models, str_c("outer_shapsgrp_",
                                            window, "_", lead, "_", version, "_",
                                            cv, ".rds")))){

  message("Calculating grouped SHAPs")
  write_csv(tibble(stage = "shapsgrp_save",
                   outer_split_num = NA, 
                   start_time = Sys.time()),
            file.path(path_models, str_c("tmp_metrics_outer_progress_",window)),
            append = TRUE)
  
  shaps_out_grp <- shaps_out %>% 
    mutate(variable_grp = if_else(str_detect(variable, "lapse."), 
                           "past use (EMA item)", 
                           variable),
           variable_grp = if_else(str_detect(variable_grp, "ema_2"), 
                           "craving (EMA item)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "ema_3"), 
                           "past risky situation (EMA item)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "ema_4"), 
                           "past stressful event (EMA item)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "ema_5"), 
                           "past pleasant event (EMA item)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "ema_6"), 
                           "valence (EMA item)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "ema_7"), 
                           "arousal (EMA item)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "ema_8"), 
                           "future risky situation (EMA item)",
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "ema_9"), 
                           "future stressful event (EMA item)",
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "ema_10"), 
                           "future efficacy (EMA item)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "missing."), 
                           "missing surveys (other)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "day"), 
                           "lapse day (other)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "hour"), 
                           "lapse hour (other)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "age"), 
                           "age (demographic)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "sex"), 
                           "sex (demographic)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "marital"), 
                           "marital (demographic)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "race"), 
                           "race (demographic)", 
                           variable_grp),
           variable_grp = if_else(str_detect(variable_grp, "educ"), 
                           "education (demographic)", 
                           variable_grp)) %>%
    mutate(variable_grp = factor(variable_grp)) %>% 
    group_by(id_obs, variable_grp) %>% # values are already averaged across repeats
    summarize(value = sum(value))
  
  shaps_out_grp %>% write_rds(file.path(path_models, 
                                          str_c("outer_shapsgrp_", window, "_", 
                                                lead, "_", version, "_",
                                                cv, ".rds")))
}
```


### Final Review performance eval from outer loop

Done more in depth later script but here is a quick look
```{r print_metrics}
metrics_out |> 
  print_kbl()

metrics_out |> 
  summarize(median(roc_auc), mean(roc_auc), min(roc_auc), max(roc_auc))

metrics_out |> 
  ggplot(aes(x = roc_auc)) +
  geom_histogram(bins = 10)

# delete tracking file
file.remove(file.path(path_models, str_c("tmp_metrics_outer_progress_", window)))
```

IMPORTANT:  We still need to select ONE final best config using the inner resampling approach AND then we need to fit that best config to ALL the data.
