---
title: "Convert CHTC text log to tibble and get summary statistics for `r params$data_type` for `r params$window` window and `r params$lead` lead and `r params$version` for `r params$algorithm` algorithm"
author: "Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
params:
  study: "meta"
  data_type: "all"
  window: "1day"
  lead: 0
  version: "v2"
  algorithm: "glmnet"
---

### Code Status

Revised version of script after meeting with John

### Notes
This script reads in a chtc log and jobs file, runs summary statistics, and outputs 
a csv log tibble with one row per job.  


### Set Up Environment

```{r set_params}
study <- params$study
data_type <- params$data_type
window <- params$window
lead <- params$lead 
version <- params$version
algorithm <- params$algorithm
```


```{r defaults, include = FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')
options(tibble.print_max = Inf)
```


libraries
```{r warning = FALSE, message = FALSE}
library(tidyverse)
library(here)
library(ggplot2)
library(skimr)
library(janitor)
library(vroom)
```

source function
```{r warning = FALSE, message = FALSE}
source(here("shared/fun_local.R"))

job_path <- str_c("P:/studydata/risk/chtc/", study, "/train_", data_type, "_", window, 
                  "_", lead, "_", version, "_", algorithm)
```

### Read in data

Read in log
```{r}
log_raw <- read_lines(here(job_path, "output",
                           list.files(here(job_path, "output"), pattern = "*.log")))

# check log file read in
if (is_empty(log_raw)) {
  stop("Log not read in. Check log is in ", job_path, "output")
}
```

Read in jobs
```{r}
jobs <- vroom(here(job_path, "input/jobs.csv"), col_types = vroom::cols())

# check jobs file is read in
if (is_empty(jobs)) {
  stop("Jobs file not read in. Check jobs file is in ", job_path, "input")
}
```

### Convert log to tibble   
```{r}
log <- convert_log(log_raw, jobs) %>% 
  glimpse()
```

Save log
```{r}
write_csv(log, here(job_path, "output/log.csv"))
```


### Check for unfinished jobs

`r nrow(subset(log, is.na(termination_dttm)))` incomplete jobs
```{r}
if (nrow(subset(log, is.na(termination_dttm))) != 0) {
  log %>% 
    filter(is.na(termination_dttm)) %>% 
    print(n = Inf)

  log %>% 
    filter(is.na(termination_dttm)) %>% 
    tabyl(resample)
}
```


### Create skim object
```{r}
skim_log <- log %>% 
  group_by(resample) %>% 
  skim_without_charts()
```



### Run time

Min, max, and median for `run_time` in minutes by resample type.
```{r}
skim_log %>% 
  yank("difftime") %>% 
  select(skim_variable, resample, min, max, median)
```


### Memory

Min, max, and median for `memory_usage` and `memory_requested` by resample type.
```{r}
skim_log %>% 
  filter(skim_variable == "memory_usage" | skim_variable == "memory_requested") %>% 
  select(skim_variable, skim_type, resample, min = numeric.p0, max = numeric.p100, median = numeric.p50) %>% 
  yank("numeric")
```

Histogram of memory usage
```{r}
log %>% 
  # filter out unfinished jobs
  filter(!is.na(termination_dttm)) %>% 
  group_by(resample) %>% 
  ggplot(aes(x = memory_usage)) +
  geom_histogram(bins = 20, color = "black", fill = "light grey") +
  facet_wrap(~ resample, scales = "free_x", ncol = 1) +
  theme_classic()
```


### Disk space

Min, max, and median for `disk_usage` and `disk_requested` by resample type.
```{r}
skim_log %>% 
  filter(skim_variable == "disk_usage" | skim_variable == "disk_requested") %>% 
  select(skim_variable, skim_type, resample, min = numeric.p0, max = numeric.p100, median = numeric.p50) %>% 
  yank("numeric")
```




