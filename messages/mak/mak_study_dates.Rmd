---
title: "Make Study Dates"
author: "John Curtin"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk/knits/gps", 
      "/Volumes/private/studydata/risk/knits/gps")
    )
  })
---

### Code Status
In progress.  This is based on the script from GPS as a starting point.   GPS had
already dropped the subjects with problematic EMA/lapses.  However, it also drops
five subjects for bad GPS and adjusts start dates for may more subjects due to GPS 
gaps.   We will start by first just dropping bad subjects but not dropping gaps.
It will hurt GPS but retain more data.   Lets see what we get from that.  



### Conclusions   

- John and Kendra have decided to drop subid 104's data for the following reasons:   
  - They had lapses every day on study except one day.    
  - Only had 75 surveys where a lapse was not reported.   
  - Viewing their lapse times it appears they were drinking constantly (morning and 
  night).   
  - They consistently report being uncertain that their goal is to be abstinent 
  (uncertain on 125 of 137 lapses. They also report they are uncertain in this goal 
  at followup 1 and 2.    
  - They are ultimately discontinued since they were struggling to gain sobriety.   
  - Unfortunately this drops 109 valid lapses.    


- John and Kendra have decided to drop subid 269's data for the following reasons:       
  - They completed 10-15 surveys on many days on study (avg number of surveys per 
  day is 6.76).  
  - Their responses indicate careless responding - they were filling 2-4 surveys out 
  within an hour of each other and the responses to the questions were completely different.     
  - They have questionable no lapse labels - they reported no lapses while on study but 
  according to notes left two messages for study staff where they admitted to drinking 
  over the weekend.   
  

- John and Kendra have decided to drop subid 204's data for the following reasons:    
  - Subid 204 had extremely poor compliance. 33 out of 89 study days had an EMA completed. 
  They only did a total of 5 surveys between followup 2 and 3.    
  - We don't trust their lapse labels - They report several lapses during their interviews 
  but times appear questionable (same time every night). They only report 1 lapse with EMA.
  - From notes - "Participant did not do many surveys during their second month of participation. 
  At their Follow-up 2 visit they reported several lapses that were not documented in their 
  EMAs - estimated lapse days/times in subid's raw data log."  
  - JC note: "There are issues with 204. They are missing lapses reported by interview. But they  
  also stopped doing any ema by 5/17 even though their study end date was 6/13. Probably need to 
  drop them for lapse analyses for anything after 5/17.  Probably also need to add in their 
  reported lapses at follow-up 2. OR we drop them at the end of follow-up 1 or wherever their 
  ema gets sketchy"    


- John and Kendra have decided to decided to retain 128's data even though they have over 100 lapses for 
the following reasons:   
  - Compliance is good (averaged over 3 surveys per day, no days without an EMA).       
  - completed the study for the full 90 days.    
  - appeared to still want abstinence as they reported they were uncertain to ema_1_5 
  on only 3 surveys. They reported they were uncertain that their goal was to remain 
  abstinent at followup 1 and confirmed their goal was to remain abstinent at followup 2.    
  - Has more non-lapse surveys than lapse surveys.   
  

- All final timezones are in America/Chicago timezone.    

- 5 participants were dropped b/c of very high levels (>= 50%) missing GPS
and no big continuous subsets of data to retain.  See details in 
mak_gps_enriched.Rmd.  These are

* 190, 56, 2, 51, 21


- For the GPS study, study start or end dates were also adjusted to accommodate large gaps in GPS data  See details in 
mak_gps_enriched.Rmd.  However, These adjustments are not yet made here in the service of retaining more data.   We can 
compare the two options eventually.   THe subjects that coudl be adjusted are

* 80, 74, 84, 65, 117, 19, 42, 167, 207, 28, 218, 39, 214, 47, 203, 40, 76, 78, 161, 180, 169, 34, 265, 30, 63, 178



### Set Up Environment

Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

Absolute paths
```{r, absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_processed <- "P:/studydata/risk/data_processed/shared"
          path_messages <- "P:/studydata/risk/data_processed/messages"},

        # IOS paths
        Darwin = {
          path_processed <- "/Volumes/private/studydata/risk/data_processed/shared"
          path_messages <- "/Volumes/private/studydata/risk/data_processed/messages"}
        )
```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) # detect and warn about function conflicts
  conflict_prefer("filter", "dplyr")
  conflict_prefer("select", "dplyr")

library(here)  # establish project directory consistently as working directory
```


Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
# for data wrangling
library(tidyverse)
library(vroom)
library(janitor)
library(lubridate)
```

### Visit/EMA Dates

visit dates
```{r}
dates <- vroom(here(path_processed, "visit_dates.csv"), col_types = vroom::cols()) %>%
  select(subid, study_start = start_study, study_end = end_study, followup_1) %>% 
  mutate(study_start = force_tz(study_start, tzone = "America/Chicago"),
         study_end = force_tz(study_end, tzone = "America/Chicago")) 
```

Filter out subids who did not make it to followup_1
```{r}
dates <- dates %>% 
  filter(!is.na(followup_1)) %>% 
  select(-followup_1)
```

Filter out excluded subids for problems with lapse reports
```{r}
dates <- dates %>% 
  filter(!subid %in% c(104, 269, 204)) %>% 
  glimpse()
```

Filter out excluded subids for problems with GPS
```{r}
dates <- dates %>% 
  filter(!subid %in% c(2, 21, 51, 56, 190)) %>% 
  glimpse()
```

Adjust start start and end dates for missing GPS.  Notes in mak_gps_enriched.rds

* This is commmented out for now!
```{r}
# dates$study_end[dates$subid == 80] = as_datetime("2018-07-31", tz = "America/Chicago")
# dates$study_start[dates$subid == 74] = as_datetime("2018-07-19", tz = "America/Chicago")
# dates$study_end[dates$subid == 84] = as_datetime("2018-08-06", tz = "America/Chicago")
# dates$study_start[dates$subid == 65] = as_datetime("2018-07-18", tz = "America/Chicago")
# dates$study_start[dates$subid == 117] = as_datetime("2018-10-14", tz = "America/Chicago")
# dates$study_start[dates$subid == 19] = as_datetime("2017-11-26", tz = "America/Chicago")
# dates$study_end[dates$subid == 19] = as_datetime("2018-01-17", tz = "America/Chicago")
# dates$study_start[dates$subid == 42] = as_datetime("2018-04-11", tz = "America/Chicago")
# dates$study_start[dates$subid == 167] = as_datetime("2019-02-06", tz = "America/Chicago")
# dates$study_end[dates$subid == 207] = as_datetime("2019-05-19", tz = "America/Chicago")
# dates$study_start[dates$subid == 28] = as_datetime("2017-12-18", tz = "America/Chicago")
# dates$study_end[dates$subid == 218] = as_datetime("2019-06-04", tz = "America/Chicago")
# dates$study_end[dates$subid == 39] = as_datetime("2018-05-11", tz = "America/Chicago")
# dates$study_start[dates$subid == 214] = as_datetime("2019-04-17", tz = "America/Chicago")
# dates$study_end[dates$subid == 47] = as_datetime("2018-04-16", tz = "America/Chicago")
# dates$study_end[dates$subid == 203] = as_datetime("2019-05-19", tz = "America/Chicago")
# dates$study_end[dates$subid == 40] = as_datetime("2018-04-28", tz = "America/Chicago")
# dates$study_end[dates$subid == 76] = as_datetime("2018-08-28", tz = "America/Chicago")
# dates$study_end[dates$subid == 78] = as_datetime("2018-08-28", tz = "America/Chicago")
# dates$study_start[dates$subid == 161] = as_datetime("2018-12-20", tz = "America/Chicago")
# dates$study_start[dates$subid == 180] = as_datetime("2019-03-02", tz = "America/Chicago")
# dates$study_start[dates$subid == 169] = as_datetime("2019-02-19", tz = "America/Chicago")
# dates$study_end[dates$subid == 34] = as_datetime("2018-03-01", tz = "America/Chicago")
# dates$study_end[dates$subid == 265] = as_datetime("2019-10-07", tz = "America/Chicago")
# dates$study_end[dates$subid == 30] = as_datetime("2017-12-24", tz = "America/Chicago")
# dates$study_end[dates$subid == 63] = as_datetime("2018-06-07", tz = "America/Chicago")
# dates$study_start[dates$subid == 178] = as_datetime("2019-02-25", tz = "America/Chicago")
```



### EMA End Date

ema
```{r}
ema <- vroom(here(path_processed, "ema_morning.csv"), col_types = vroom::cols()) %>% 
    select(subid, start_date) %>% 
  bind_rows(vroom(here(path_processed, "ema_later.csv"), col_types = vroom::cols()) %>% 
    select(subid, start_date)) %>% 
  mutate(subid = as.numeric(subid),
         start_date = with_tz(start_date, tzone = "America/Chicago")) %>% 
  rename(ema_end = start_date) %>% 
  group_by(subid) %>% 
  arrange(subid, desc(ema_end)) %>% 
  slice(1) %>% 
  ungroup()
  
  
```


Add ema_end date
```{r}
dates <- dates %>% 
  left_join(ema, by = "subid") %>% 
  relocate(subid, study_start, study_end, ema_end) 
```



### Save

save dates
```{r}
dates %>% 
  write_csv(here(path_messages, "study_dates.csv")) %>% 
  glimpse()
```
