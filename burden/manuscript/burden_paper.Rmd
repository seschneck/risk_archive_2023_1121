---
title: "Acceptability of Personal Sensing among People with Alcohol Use Disorder"
shorttitle        : "Personal Sensing"

author: 
  - name          : "Kendra Wyant"
    affiliation   : "1"
  - name          : "Hannah Moshontz"
    affiliation   : "1"
  - name          : "Stephanie B. Ward"
    affiliation   : "1"
  - name          : "John J. Curtin"
    affiliation   : "1"
    corresponding : yes 
    address       : "1202 West Johnson St, Madison, WI 53706"
    email         : "jjcurtin@wisc.edu"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, University of Wisconsin - Madison"


authornote: |
  Enter author note here. 
  
  Each new line herein must be indented, like this line.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

floatsintext      : yes
figurelist        : yes
tablelist         : no
footnotelist      : yes
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
header-includes:
  - \raggedbottom
  - \usepackage{booktabs}
  - \definecolor{lightred}{RGB}{255,222,222}
  - \definecolor{lightblue}{RGB}{219,248,255}
  - \usepackage[justification=raggedright]{caption}
  - \usepackage{colortbl}


  
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk/manuscripts/burden", 
      "/Volumes/private/studydata/risk/manuscripts/burden")
    )
  })
bibliography: burden.bib
csl: journal-of-medical-internet-research.csl
---

```{r setup, include = FALSE}
library(here)
library(papaja)
library(knitr)
library(tidyverse)
library(kableExtra)
library(janitor)
library(corx)
library(patchwork)
library(ggtext)
library(vroom)
library(lubridate)

knitr::opts_chunk$set(echo = FALSE)
options(knitr.kable.NA = '')
```

```{r absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_burden <- "P:/studydata/risk/data_processed/burden"
          path_shared <- "P:/studydata/risk/data_processed/shared"},
        # IOS paths
        Darwin = {
          path_burden <- "/Volumes/private/studydata/risk/data_processed/burden"
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"}
       )
```

```{r relative paths and source}
path_ana <- "burden/ana_scripts"

source(here(path_ana, "fun_burden.R"))
```


```{r burden data}
data <- vroom::vroom(here(path_burden, "acceptability.csv"), col_types = vroom::cols())

# pull out last observation for each participant
# Last available sleep monitor may be earlier than last survey date for some due 
# to discontinuation of monitor - handle separately
data_sleep <- data %>% 
  filter(!is.na(sleep_interfere)) %>% 
  group_by(subid) %>% 
  arrange(desc(date)) %>% 
  slice(1) %>% 
  ungroup() %>% 
  select(c(subid, starts_with("sleep_")))

data_last <- data %>% 
  select(-c(starts_with("sleep_"))) %>% 
  group_by(subid) %>% 
  arrange(desc(date)) %>% 
  slice(1) %>% 
  ungroup() %>% 
  full_join(data_sleep, by = c("subid")) %>% 
  select(-c(contains("wristband"), contains("carrying_phone"))) 
```

```{r screen data}
screen <- vroom::vroom(here(path_shared, "screen.csv"), col_types = vroom::cols())

# include only participants used in analyses (n = 154) - use visit dates to determine
sample_fu1 <- vroom::vroom(here(path_shared, "visit_dates.csv"), col_types = vroom::cols()) %>% 
  filter(!(is.na(followup_1)))

screen <- screen %>% 
  filter(subid %in% sample_fu1$subid)
```

```{r disc data}
# read in notes on incomplete participants
notes_incomplete <- read_csv(file.path(path_burden, "notes_discontinue.csv"), col_types = cols()) %>% 
  filter(screen != "cancelled" & screen != "no show")
```

```{r ema and audio data}
ema_m <- vroom(file.path(path_shared, "ema_morning.csv"), col_types = vroom::cols()) %>% 
  mutate(start_date = with_tz(start_date, tzone = "America/Chicago"),
         subid = as.numeric(subid))
ema_l <- vroom(file.path(path_shared, "ema_later.csv"), col_types = vroom::cols()) %>% 
  mutate(start_date = with_tz(start_date, tzone = "America/Chicago"),
         subid = as.numeric(subid))
audio <- vroom(file.path(path_shared, "audio.csv"), col_types = vroom::cols()) %>% 
  mutate(subid = as.numeric(subid))
```


<!-- Need to decide on terminology RE mental health, mental illness, psychatric disorders.   Currently we use mental health in the context of health generally and then psychiatric disorders.   Lets make sure we use these two terms consistently-->

<!--Density, depth, and duration.  Report for EMA in method--->

<!-- John, I cannot seem to improve tables any further - still need to control width, change spacing (maybe single?) to fit on a single page, and get line returns in footnote to work. I have latex line breaks and it looks like it is trying to add a new line based on weird spacing in knit file but I think it might be interfering with papaja template. Additionally, I cannot seem to color the tables as I did in word. -->  

<!-- https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_pdf.pdf-->
<!-- Kendra line return in footnotes solved, see table 3 -->
<!-- Kendra we can use kable_styling(latex_options = "scale_down") if you want to shrink tables to fit on a single page. However it doesn't work on longtable/threeparttable so those would need additional formatting. Looking at JMIR's example doc, i'm not clear that this is required, as it says they'll do it during typesetting.-->

<!-- My figures are also at a standstill - outstanding issues include placement (I can not seem to alter the float parameter to get the figures to stay in the correct manuscript place) and fig.caption parsing error where figure# appears after Figure X.-->  

<!-- @Kendra re: fig placement: Although they're now moved to end, if wanted in text, use fig.pos and (out.extra="" or out.width="x%") in chunk options. It won't respect fig.pos setting unless a) at least one of those other two is specified and b) there is enough room where you want it, so you can always start with out.width="50%" just to force them where you wanted them. -->

<!-- @Kendra re: fig.caption parsing error: 2 issues here. 1) figure names cannot have spaces or underscores, I replaced them with hyphens; 2) fig.cap must be specified for it to be inserted into latex's figure workspace.-->

<!-- @Kendra re: line breaks in figure captions: use \\linebreak and the added usepackage{caption} where it specifies allowing ragged right margin. https://mirror.math.princeton.edu/pub/CTAN/macros/latex/contrib/caption/caption-eng.pdf for further documentation -->

## Personal Sensing

The World Health Organization's Global Observatory for eHealth has concluded that "the use of mobile and wireless technologies to support the achievement of health objectives has the potential to transform the face of health service delivery across the   globe" [@whoglobalobservatoryforehealthMHealthNewHorizons2011]. This conclusion applies to research and care for mental health as well as other traditional health services. These opportunities are now possible in part because of rapid advances in smartphone and related mobile technologies [@majumderSmartphoneSensorsHealth2019] and high levels of smartphone access across race, socioeconomic status, geographic region, and other demographic characteristics [@pewresearchcenterMobileFactSheet2021].

Personal sensing may become an important component of these digital health advances [@healthWhatDigitalHealth2020]. Personal sensing is a method for longitudinal measurement in situ; i.e., real-world measurement that is embedded in individuals' day to day lives [@mohrPersonalSensingUnderstanding2017;  @klasnjaExploringPrivacyConcerns2009;  @huckvaleClinicalDigitalPhenotyping2019]. Raw data streams are collected by smartphones, wearable sensors, or other smart devices. These raw data streams can consist of self-reports or more novel data streams such as geolocations, cellular communications, social media activity, or physiology. Subsequent processing can extract psychiatric or health relevant measures of thoughts, feelings, behavior, and even interpersonal interactions.

Ecological momentary assessment (EMA), a personal sensing method that collects brief self-reports about momentary states multiple times per day, has been used for many years in short-term longitudinal studies of psychiatric disorders. For example, EMA research on substance use disorders has identified proximal causes and risk factors for drug craving and relapse [@morgensternEcologicalMomentaryAssessment2014; @fronkStressAllostasisSubstance2020; @schultzStressorelicitedSmokingCravingInpress]. It has also characterized the time course and nature of drug withdrawal [@piaseckiSmokingWithdrawalDynamics2003a; @mccarthyLifeQuittingSmoking2006]. EMA research on major depressive and bipolar disorders has identified predictors of daily mood fluctuations, documented treatment efficacy, and been paired with other human neuroscience methods to explore biological mechanisms [@aanhetrotMoodDisordersEveryday2012]. Much of this research could not have been accomplished with other measurement methods.

More recently, mental health relevant research using personal sensing of raw data streams other than self report is emerging. This includes methods to sense geolocation [@epsteinPredictionStressDrug2020; @palmiusDetectingBipolarDepression2016; @moshontzProspectivePredictionLapses2021], cellular communications [@jacobsonDigitalBiomarkersSocial2020; @wangExaminingCorrelationDepression2021; @razaviDepressionScreeningUsing2020; @baiTrackingMonitoringMood2021; @moshontzProspectivePredictionLapses2021], sleep [@baiTrackingMonitoringMood2021], and physiology [@kleimanUsingWearablePhysiological2019; @kleimanCanPassiveMeasurement2021] as examples. These alternative personal sensing methods provide benefits and opportunities not possible with EMA. For example, many of these data streams can be sensed passively such that they have very low assessment burden. This may allow their use for long-term longitudinal monitoring of patients that would not be feasible with EMA, which requires more active effort for data collection. These data streams can also support measures that are not possible by self report.

Personal sensing is a powerful tool for mental health research[@shiffmanEcologicalMomentaryAssessment2008a]. These data are inherently longitudinal, which allows observation of the temporal ordering for putative etiologic mechanisms and their effects. Longitudinal measurement is also critical for many mental health constructs that display meaningful, and often frequent, temporal variation within person (e.g., psychiatric symptoms). Measures based on personal sensing data generally have high ecological validity because they are collected in situ.  Personal sensing measures also have low retrospective bias because they are often collected in real-time. Furthermore, personal sensing can derive measures from raw data streams (e.g., in situ behavior, physiology, interpersonal interactions) that are difficult or even impossible to obtain through other traditional research measurement methods.

Personal sensing may have even higher value in the future for mental health applications that target patient mental health care than it does for research[@huckvaleClinicalDigitalPhenotyping2019; @onnelaHarnessingSmartphoneBasedDigital2016; @torousRealizingPotentialMobile2015].<!--All the citations in this paragraph are focussed on passive/more novel personal sensing methods. I wonder if we should make this clear in the topic sentence. Maybe "Personal sensing may have even higher value in the future for mental health applications that target patient mental health care as more passive and novel data streams become feasible."--> Data collected by personal sensing methods may be used for preliminary screening for psychiatric disorders [@eichstaedtFacebookLanguagePredicts2018; @razaviDepressionScreeningUsing2020]. These methods can also be used to monitor psychiatric symptoms or even predict future risk for symptom reoccurrence, relapse, or other harmful behaviors (e.g., suicide attempts)[@barnettRelapsePredictionSchizophrenia2018; @chihPredictiveModelingAddiction2014; @epsteinPredictionStressDrug2020; @jashinskyTrackingSuicideRisk2013; @jacobsonPassiveSensingPrediction2020]. Personal sensing measures or risk indicators may be shared with health care providers to allow for cost effective, targeted allocation of limited mental health resources to patients with greatest or most urgent need [@quanbeckIntegratingAddictionTreatment2014]. Personal sensing has the potential to support precision mental health care by adapting and timing interventions based on characteristics of the patient and the moment in time [@nahum-shaniJustinTimeAdaptiveInterventions2018; @aggarwalAdvancingArtificialIntelligence2020; @kaiserObamaGivesEast2015]. To be clear, these applications of personal sensing are currently aspirational rather than available for clinical implementation today. However, clinical research is advancing us rapidly toward these goals [@moshontzProspectivePredictionLapses2021; @chihPredictiveModelingAddiction2014; @sheikhWearableEnvironmentalSmartphoneBased2021; @baiTrackingMonitoringMood2021].

Mental health research and applications with emerging, often more passively sensed, novel data streams like geolocation and cellular communications is still nascent.  This research has predominately involved "proof-of-concept" studies that typically include only healthy controls or other convenience samples rather than patients with psychiatric disorders [@razaviDepressionScreeningUsing2020; @jacobsonDigitalBiomarkersSocial2020; @wangExaminingCorrelationDepression2021].  It has also often used very small sample sizes and/or short monitoring periods [@palmiusDetectingBipolarDepression2016; @jacobsonDigitalBiomarkersSocial2020; @kleimanUsingWearablePhysiological2019; @kleimanCanPassiveMeasurement2021].  Recent reviews of this emerging literature have highlighted gaps in reporting on participant exclusions, attrition, and compliance that are necessary to assess selection biases and feasibility of these more novel personal sensing methods [@deangelDigitalHealthTools2022; @ortizAppsGapsBipolar2021; @faurholt-jepsenSmartphonebasedObjectiveMonitoring2018].


## Acceptability of Personal Sensing

Further development and use of personal sensing necessitates better understanding of its acceptability to research participants and patients targeted for mental health applications. Will individuals consent to the use of personal sensing methods?  Will they opt-in to allow for passive measurement methods?  Can they sustain the behaviors necessary for active measurement methods for longer periods of time?  Do they perceive specific personal sensing methods as burdensome or dislike them?  Answers to these questions about the acceptability of personal sensing methods are central to its feasibility for both mental health research and applications.  

The acceptability of a personal sensing method may be influenced by the degree of active effort required from the participant or patient to collect the raw data (i.e., the method's assessment burden) and other factors (e.g., the sensitivity of the data collected).  As such, acceptability may vary across different personal sensing methods and comparisons across methods within the same individuals is thus warranted. Furthermore, comprehensive assessment of both subjective perceptions and behavioral measures (e.g., compliance) of acceptability may better anticipate potential issues for recruitment, consent, compliance, and attrition when they are used for either research or clinical applications.

Much of what is known about the acceptability of personal sensing is limited to EMA.  Studies that have accessed participants' perceptions of EMA methods have generally concluded that it is acceptable to participants from both non-clinical and clinical samples[@stoneIntensiveMomentaryReporting2003; @kirkExposureAssessmentCurrent2013; @ramseyFeasibilityAcceptabilitySmartphone2016; @yangFeasibilityAcceptabilitySmartphoneBased2015; @moitraFeasibilityAcceptabilityPosthospitalization2017]. Similarly, participants display moderate or better compliance with respect to response rates even with relatively high sampling density (e.g., 6-9 daily assessments)[@eiseleEffectsSamplingFrequency2020; @stoneIntensiveMomentaryReporting2003; @wenComplianceMobileEcological2017]. However, these studies generally assessed participants' perceptions and compliance over short monitoring periods (i.e., 2-6 weeks).  Less is known about the use of EMA over longer duration monitoring periods (e.g., months) as would be necessary for clinical applications.

Assessment burden may impact the relative acceptability of EMA.  EMA assessment burden can increase as more active effort is required from participants when EMA survey density (i.e., frequency of surveys) or depth (number of items/length of surveys) increases.  However, the impact of active effort due to density and depth may not be comparable.  Increased density does not appear to robustly impact perceptions or compliance [@eiseleEffectsSamplingFrequency2020; @stoneIntensiveMomentaryReporting2003; @jonesComplianceEcologicalMomentary2019; @wrzusEcologicalMomentaryAssessment2022 but see @wenComplianceMobileEcological2017].  However, greater survey depth may decrease both perceptions of acceptability and compliance  [@eiseleEffectsSamplingFrequency2020].

The duration of the EMA monitoring period may also increase assessment burden and affect acceptability but findings have been mixed.  Some studies suggest that as duration increases beyond only a few weeks, compliance decreases [@wrzusEcologicalMomentaryAssessment2022; @yangFeasibilityAcceptabilitySmartphoneBased2015; @onoWhatAffectsCompletion2019]. Yet other studies find no changes in compliance over time [@wenComplianceMobileEcological2017; @kirkExposureAssessmentCurrent2013]. Importantly, very few studies have looked at compliance over longer duration (e.g., > 6 weeks). One recent study asked people about their perceptions of EMA studies (without deploying the EMA protocol). They found shorter study duration to be associated with greater willingness to participate and a perception that the study would be more enjoyable. However, even in this hypothetical scenario the long duration condition was only 3 weeks [@smythInfluenceEcologicalMomentary2021].

Existing research also raises some concern about perceptions and compliance with EMA protocols in patients with substance use disorders relative to other groups.  Specifically, a recent meta-analysis confirmed decreased compliance with EMA protocols in patients with substance use disorder diagnoses vs. recreational substance users [@jonesComplianceEcologicalMomentary2019].  However, a separate meta-analysis by [@wrzusEcologicalMomentaryAssessment2022] showed that compliance rates did not differ between healthy, physically ill, mentally ill, and mixed samples, which suggests that compliance concerns may be limited to applications with patients with substance use disorders rather than all psychiatric disorders more generally.

Far less is known about participants' perceptions and compliance with more passive personal sensing methods. Some research has presented hypothetical scenarios to participants to assess their perceptions about personal sensing methods [@duncanAcceptabilitySmartphoneApplications2019; @riegerPsychiatryOutpatientsWillingness2019; @bessenyeiComfortabilityPassiveCollection2021].  Participants' willingness to share sensed data appears to vary by the data type (e.g., sleep, geolocation, social media activity).  However, it is difficult to determine how well participants' perceptions in these hypothetical scenarios would generalize to real world collection of these data. And, of course, it is impossible to measure attrition and compliance outside of explicit implementation of these sensing methods. 

Preliminary research has begun to examine perceptions and compliance during real world use of passive personal sensing methods. However, this research has generally been limited by small sample sizes [@lindEffortlessAssessmentRisk2018; @ben-zeevMobileBehavioralSensing2016], use of convenience samples (e.g., students) [@lindEffortlessAssessmentRisk2018;  @kirkExposureAssessmentCurrent2013; @rooksbyStudentPerspectivesDigital2019], short monitoring duration [@lindEffortlessAssessmentRisk2018; @kleimanUsingWearablePhysiological2019; @ben-zeevMobileBehavioralSensing2016; @raughDigitalPhenotypingAdherence2021], and coarse, incomplete or aggregate reporting of perceptions, compliance and related participant behaviors[@lindEffortlessAssessmentRisk2018; @ben-zeevMobileBehavioralSensing2016; @kirkExposureAssessmentCurrent2013]. These are important first efforts but more research into the feasibility of personal sensing methods is clearly warranted.    

## Study Goals

This study reports on the acceptability of both active and passive personal sensing methods in a sample of patients with moderate to severe alcohol use disorder.  These participants were enrolled early in their recovery (i.e., 1 - 8 weeks after becoming abstinent) and followed for 3 months. We used active personal sensing methods to collect EMA, daily audio check-ins, sleep quality, and selected physiology. We used primarily passive methods to collect moment-by-moment<!-- Are we keeping moment-by-moment geolocation or just geolocation?--> geolocation, cellular communications logs, and text message content. We assessed participants' subjective perceptions of the acceptability each of these personal sensing methods, separately, by self report.  We also assessed participants' choices about their participation in the study at various stages in the study procedure (e.g., consent, enrollment, data collection), their choice to opt-in to provide data associated with each personal sensing method, and their reasons for discontinuation when available. Finally, for active measures, we assess their compliance with providing those raw data streams for up to 3 months of their study participation. We believe these data provide an unparalleled and transparent window into the feasibility of using numerous personal sensing methods with individuals with alcohol use disorder, a highly stigmatized psychiatric disorder.


# Methods

```
Comment from Hannah to Address
I think that the distinction between active/passive doesn't belong in the methods section and should come earlier. I think we can then just use subheadings of 'active' and 'passive' in the methods > procedure section.

Similarly, the explanation of how acceptability was operationalized and what the compensation scheme was might be better suited to sections other than the measures section (which I see as typically giving very precise operationalization information rather than procedural info or study design reasoning). I'd recommend providing a list of the variables rather than a narrative / paragraph.

Same basic point, but since we refer to variables later ("dislike", "willingness to use for one year"), I think we should define these variables explicitly in a list in the measures section rather than in a narrative paragraph.

Some sections are a little wordy -- didn't feel necessary to cut down, but if we are up against space limitations, I can suggest some more concise language throughout the methods section.

There is some flip-flopping between passive voice and active voice in this section (e.g., between the subsections of the personal sensing header in procedure). I find passive voice more readable, personally, but most of the section is in active voice.

I think that the personal sensing section in the procedure fits better in the measures section and is consistent with the choice to describe the surveys there.

I think any 'individual differences' variables that we report in this study (like in tables) should be listed in the measures section rather than in the appendix.

There is some redundant information in the analysis strategy section (e.g., the first sentence of the self-reported acceptability subsection). Also, this section seems to introduce new information about the timing of measurement.

I disagree that one-sample t-tests can tell us about polarization. When I read 'to detect polarized perceptions' I'm expecting something that can test whether a distribution is bimodal.
I'd suggest just cutting the clause on line ~311.

I think it's worth leveraging the error-prevention benefits of Rmd and swapping out the typed numbers in text for printed stored variables or in-line computations.
```

## Research Transparency

We value the principles of research transparency that are essential to the robustness and reproducibility of science [@schonbrodtVoluntaryCommitmentResearch2015]. Consequently, we maximized transparency through several complementary methods. First, we report how we determined our sample size, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]. Second, we completed a transparency checklist, which can be found in the supplement of this paper (Multimedia Appendix 1) [@aczelConsensusbasedTransparencyChecklist2019]. Third, we made the data, analysis scripts and annotated results, self-report surveys, and other study materials associated with this report publicly available [<https://osf.io/cjsvk>].

## Participants

We collected the study data between 2017 -- 2019 as part of a larger grant-funded parent project (RO1 AA024391). The sample size was determined based on power analyses for the aims of that project. We used all available participants for this study. <!-- Is this accurate if we only used participants who completed through follow-up 1?-->

We recruited participants in early recovery (1 -- 8 weeks of abstinence) from AUD within the Madison area to participate in a 3-month longitudinal study. Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required that participants:

1.  were 18 years of age or older,
2.  were able to write and read in English,
3.  had at least moderate AUD (\>= 4 DSM-5 AUD symptoms ^[We measured DSM-5 symptoms with a self-report survey administered to participants during the screening visit.]),
4.  were abstinent from alcohol for at least 1 week but no longer than 2 months,
5.  were willing to use a single smartphone (their personal phone or one provided by us) while enrolled in the study.

We also excluded participants if they exhibited severe symptoms of psychosis or paranoia ^[Psychosis and paranoia were defined as scores greater than 2.2 or 2.8, respectively, on the psychosis or paranoia scales of the on the Symptom Checklist – 90 (SCL-90) [@derogatisSCL90OutpatientPsychiatric1973].]. All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

We assessed eligibility and exclusion criteria using a brief phone screen followed by a more detailed in person screening visit. One hundred ninety-two participants met criteria for enrollment. Of these participants, 191 consented to participate in the study at the screening session and 169 subsequently enrolled in the study at the enrollment visit which occurred approximately one week later. Fifteen participants discontinued prior to the first follow-up visit at one month. The remaining 154 participants provided study measures for 1 (N = 14), 2 (N = 7) or 3 (N = 133) months. We provide a study participation flow chart in Figure 1.

## Procedure

Participants completed 5 study visits over the course of approximately 3 months. Participants first attended a screening visit where we determined eligibility, obtained informed consent, and collected self-report measures of individual differences (e.g., demographics, alcohol use history). We scheduled eligible and consented participants to enroll in the study approximately 1 week later. During this enrollment visit, we collected additional self-report and interview measures. Participants completed an additional 3 follow-up visits that occurred about every 30 days. We collected self-report and interview measures and downloaded cellular communications logs (text message and phone call) at these visits. Finally, we collected various raw data streams (e.g., geolocation, cellular communication logs, EMA) using personal sensing to monitor participants throughout the 3-month study period. A full description of the procedure and data collected at each visit can be found at the study's OSF page (<https://osf.io/cjsvk>).

## Personal Sensing

Personal sensing methods can be coarsely classified as active or passive. Active personal sensing requires active effort from the participant to provide the raw data streams whereas passive personal sensing data are collected automatically (either asynchronously or continuously) with little to no effort required by the participant. Our study obtained several active signals that varied somewhat in the amount of effort required by the participant. Specifically, we used active methods to collect EMA, daily audio check-ins, sleep quality, and selected physiology. We used primarily passive methods to collect moment-by-moment<!-- Remove moment-by-moment. Elsewhere we just call it geolocation (i.e., subheadings, plots/tables, results, and in intro when describing other studies using geolocation)--> geolocation, cellular communications logs, and text message content. More detail about each raw data stream collected by personal sensing is provided below.

### Audio Check-in

Participants recorded a diary-style audio response on their smartphone to an open-ended prompt each day following a reminder from us that was sent via text message. They responded to the prompt ("How are you feeling about your recovery today?"), which stayed the same throughout the entire study. We instructed them that their responses should be approximately 15-30 seconds in duration. These recordings were sent to us by text message.

### EMA

Participants completed a brief EMA 4 times each day following reminders from us that were sent by text message. These text messages included a link to a Qualtrics survey that was optimized for completion on their smartphone. All 4 EMAs included items that asked about any alcohol use that had not yet been reported, current affective state (pleasantness and arousal), greatest urge to drink alcohol since the last EMA, any pleasant or positive events and any hassles or stressful events that occurred since the last EMA, any exposure to risky situations (i.e., people, places, or things) since the last EMA. The first EMA each day asked an additional three questions about how likely participants were to encounter a risky situation, encounter a stressful event, and drink alcohol in the upcoming week. The first and last EMAs of the day were scheduled within 1 hour of participants' typical wake and sleep times. The other two EMAs were each scheduled randomly within the first and second halves of the participants' typical day. All EMAs were separated from each other by at least 1 hour.

### Sleep Quality

We collected information about participants' sleep duration, timing, and overall quality with a Beddit sleep monitor (Beddit Oy Inc., Espoo, Finland) that was placed in their beds and connected to their smartphones. We used an early version of the sleep monitor that required participants to actively start and stop the monitor when they entered and exited their bed each night and morning, respectively. These data are available for only 82 participants because Beddit Oy was acquired by Apple Inc. during data collection for this study. Apple discontinued cloud support for data collection with the sleep monitor in November 2018, which prevented its further use for our remaining participants.

### Physiology

We continuously monitored participants' physiology (heart rate, electrodermal activity, skin temperature) using an early version of the Empatica E4 wristband monitor (by Empatica Inc., Boston, MA). However, this early version did not adequately support Bluetooth streaming of data to the cloud. Instead, participants had to manually connect the wristband each night to a tablet we provided to upload their data. This and other software bugs made use of the wristband too complicated for many participants. Therefore, we discontinued use of the wristband after we collected data from 9 participants. Given this small sample size, we did not include the wristband in our primary analyses. We do provide self-reported acceptability ratings for this signal from this small sample in Multimedia Appendix 3 (Figure S1).

### Geolocation

We continuously collected participants' moment-by-moment<!-- I think here it makes sense to mention moment-by-moment--> geolocation using location services on their smartphones in combination with commercial software that accessed these geolocation data and saved them in the cloud. At the start of the study, we used the Moves app (developed by ProtoGeo Oy, Helsinki, Finland). However, Facebook acquired ProtoGeo Oy and shut down use of the Moves app in July 2018. At this point, we switched to using the FollowMee GPS tracking mobile app (FollowMee LLC, Murphy, TX). Measurement of geolocation required only initial installation of the app by the participants. Subsequent measurement and transfer of the data to the cloud was completed automatically with no input or effort by the participant. Both apps allowed participants to temporarily disable location sharing if they deemed it necessary for short periods of time.

### Cellular Communication Logs

We collected cellular communication logs that include meta-data about smartphone communications involving both text messages and phone calls. For each communication entry, these logs include the phone number of the other party, the type of call or message (i.e., incoming, outgoing, missed, rejected), the name of the party if listed in the phone contacts, the date and time the message or call occurred, whether the log entry was read (text messages only), and the duration of the call (voice calls only). These data are saved passively on the phone with no additional input or effort on the part of the participant. We downloaded these logs from participants' phones at each 1-month follow-up visit. Participants were informed that they could delete any text message or voice call log entries prior to the download if they desired. 

### Text Message Content 

We also collected the message content from participants' text messages on their smartphone. As with the logs, content from individual text messages is saved passively on the phone with no additional input or effort on the part of the participant. We downloaded text message content at each 1-month follow-up visit and participants could delete text messages prior to the download. Note that we did not have a parallel method to gain access to phone call content. Thus, we had meta-data from communication logs for both text messages and phone calls but had content of the communication only for text messages.

## Measures

### Individual Differences

We collected demographic information and information relevant to participants' alcohol use and DSM-5 AUD symptoms at the screening visit. ^[Additional variables were measured as part of the parent project aims. We share the full surveys on OSF.] 


### Behavioral Measures of Acceptability

Coarse assessment of the acceptability of the personal sensing methods can be made based on participants' behaviors. Specifically, we assessed three categories of behavior. First, we assessed participants' choices about their participation in the study at various stages in the study procedure (e.g., consent, enrollment, data collection) and their reasons for discontinuation when available. Second, we assessed their choice to opt-in to provide data associated with each personal sensing method. Participants were allowed to participate in the study without opting-in to any specific personal sensing method other than EMA. Instead, they were paid a monthly bonus for each sensing method they chose to opt-in to that ranged from \$10-\$25. Finally, for a subset of the active measures (EMA, audio check-in), we assess their behavioral compliance for up to 3 months of study participation.

### Self-reported Measures of Acceptability

To assess participants' subjective experience of the acceptability of the personal sensing methods in this study, each month they rated each method on three acceptability relevant dimensions (see Multimedia Appendix 2). Specifically, participants were asked to indicate how much they agreed with each of the following 3 statements on a 5-point bipolar scale (strongly disagree, disagree, undecided, agree, strongly agree) for the personal sensing signals ^[Participants provided ratings of dislike and willingness separately for text message and phone call logs. However, participants’ ratings for each item were highly correlated across the two logs (r = 0.83 for dislike; r = 0.79 for willingness). Furthermore, permissions for API access to text messages and phone call logs are linked for both Android and iOS operating systems such that researchers and app developers get access to both if permission is granted. Therefore, we decided to combine ratings of dislike and willingness for each of these two logs.]:


1.  [Personal sensing method name] interfered with my daily activities.
2.  I disliked [Personal sensing method name].
3.  I would be willing to use [Personal sensing method name] for 1 year to help with my recovery.

The interference item (item 1) was collected only for the active methods because the passive methods require no effort and therefore cannot interfere with daily activities. Dislike and willingness to use for one year (items 2 & 3, respectively) were collected for all methods.

## Data Analytic Strategy

We conducted all analyses in R version 4.1.1 [@rcoreteamLanguageEnvironmentStatistical2021] using RStudio [@rstudioteamRStudioIntegratedDevelopment2020] and the tidyverse ecosystem of packages [@wickhamWelcomeTidyverse2019]. 

### Behavioral Measures of Acceptability

We provide descriptive data on participants' choices about their participation in the study at various stages in the study procedure (e.g., consent, enrollment, data collection). We provide both coarse and more granular tabulation of their reasons for discontinuation when available. We report the percentages of participants who opted-in to provide us with the raw data streams we collected via personal sensing. We also report compliance measures for two of the active personal sensing methods (EMA and audio check-in). Formal measures of compliance could not be calculated for geolocation, cellular communication logs, text message content, and sleep quality because it was not possible to distinguish between missing data due to compliance (e.g., deleting phone calls or messages, turning off location services on the phone, failing to start sleep monitoring at bedtime) and valid reasons (no calls made during the day, no movement, erratic sleep patterns).

### Self-reported Measures of Acceptability

Participants responded to the three self-report items related to acceptability (interference, dislike, and willingness to use for 1 year) on a 5-point bipolar scale (strongly disagree, disagree, undecided, agree, strongly agree). We retained these ordinal labels for visual display of these data in figures but ordered the labels such that higher scores represent greater acceptability (i.e., strongly agree for willingness to use for 1 year and strongly disagree for interference and dislike. For analyses, we re-coded these items to a numeric scale ranging from -2 to 2 with 0 representing the neutral (undecided) midpoint and higher scores representing greater acceptability.

Participants responded to these items at each monthly follow-up visit. Therefore, participants had up to 3 responses for each item depending on when they ended their participation. We analyzed their last available response in our primary analyses to allow us to include all participants and to represent their final perception of each personal sensing signal. However, mean responses across each time point remained relatively constant for all signals (see Figure S2 in Multimedia Appendix 3).

To detect polarized perceptions of the personal sensing signals (i.e., mean responses to any items that are different from 0/undecided), we conducted one sample t-tests for the three self-report items for each personal sensing signal. To examine relative perceptions of the signals, we compared perceptions of the active vs. passive categories of signals using within-sample t-tests for dislike and willingness to use for 1 year ^[Participants did not provide ratings of interference for passive signals so the comparisons of active vs. passive categories were limited to dislike and willingness to use for one year. Also, due to high proportion of missing data for the sleep monitor, we excluded this signal from these analyses and the intra-class correlations described next.]. We also report pairwise comparisons among all personal sensing signals using within-sample t-tests for each of the three self-report items in Table S1 in Multimedia Appendix 3.

Finally, we conducted two analyses to examine the consistency of perceptions across personal sensing signals (e.g., do participants who dislike one signal also dislike the other signals?). First, we calculated bivariate correlations among the personal sensing signals for each item. Second, we calculated intraclass correlations (single, case 3 [@shroutIntraclassCorrelationsUses1979]) separately for each item to quantify agreement in participants' perceptions across the signals.

# Results

## Participant Characteristics

One hundred fifty-four participants completed at least 1 monthly follow-up visit and provided self-report acceptability ratings for interference, dislike, and willingness to use for 1 year. These participants serve as our primary sample for our analyses. Table 1 presents demographic information for these participants. Table 2 characterizes alcohol use and AUD-relevant information for these participants. We compared demographics and AUD information for participants who were included in the analyses vs. eligible participants who did not provide study measures (i.e., did not enroll or discontinued prior to the first month follow-up; N = 36 ^[We are missing demographic data for one participant who consented but did not subsequently enroll in the study.]) and found no significant differences (see  Table S2 in Multimedia Appendix 3 for more detail on these analyses).

\newpage

```{r table-1-code}
dem <- screen %>% 
  summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
            SD = as.character(round(sd(dem_1, na.rm = TRUE), 1))) %>% 
  mutate(var = "Age",
         n = as.numeric(""),
         perc = as.numeric("")) %>% 
  select(var, n, perc, everything()) %>% 
  full_join(screen %>% 
  select(var = dem_2) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_3) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("American Indian/Alaska Native", "Asian", "Black/African American",
                           "White/Caucasian", "Other/Multiracial")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_4) %>% 
  mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
                         TRUE ~ "Yes"),
         var = fct_relevel(factor(var, c("Yes", "No")))) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_5) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Less than high school or GED degree", "High school or GED", 
                           "Some college", "2-Year degree", "College degree", "Advanced degree")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_6, dem_6_1) %>% 
  mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
                         dem_6_1 == "Part-time" ~ "Employed part-time",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Employed full-time", "Employed part-time", "Full-time student",
                           "Homemaker", "Disabled", "Retired", "Unemployed", 
                           "Temporarily laid off, sick leave, or maternity leave",
                           "Other, not otherwise specified")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  summarise(mean = as.character(round(mean(dem_7, na.rm = TRUE), 0)),
            SD = as.character(round(sd(dem_7, na.rm = TRUE), 0))) %>% 
  mutate(var = "Income",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>% 
  full_join(screen %>% 
  select(var = dem_8) %>% 
  mutate(var = case_when(var == "Never Married" ~ "Never married",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Never married", "Married", "Divorced", "Separated",
                           "Widowed")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))
```


```{r table-1}
dem %>% 
  kbl(booktabs = TRUE,
      caption = "Demographics",
      col.names = c("", "n", "%", "M", "SD"),
      align = c("l", "c", "c", "c", "c"),
      digits = 1,
      longtable = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>% 
  row_spec(row = 0, align = "c", italic = TRUE) %>% 
  pack_rows("Sex", 2, 3, bold = FALSE) %>% 
  pack_rows("Race", 4, 8, bold = FALSE) %>%
  pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) %>%
  pack_rows("Education", 11, 16, bold = FALSE) %>%
  pack_rows("Employment", 17, 25, bold = FALSE) %>%
  pack_rows("Marital Status", 27, 31, bold = FALSE) %>% 
  footnote("N = 154")
```

\newpage

```{r table-2-code}
auh <- screen %>%
  summarise(mean = mean(auh_1, na.rm = TRUE),
            SD = sd(auh_1, na.rm = TRUE)) %>%
  mutate(var = "Age of first drink",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_2, na.rm = TRUE),
            SD = sd(auh_2, na.rm = TRUE)) %>%
  mutate(var = "Age of regular drinking",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_3, na.rm = TRUE),
            SD = sd(auh_3, na.rm = TRUE)) %>%
  mutate(var = "Age at which drinking became problematic",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_4, na.rm = TRUE),
            SD = sd(auh_4, na.rm = TRUE)) %>%
  mutate(var = "Age of first quit attempt",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_5, na.rm = TRUE),
            SD = sd(auh_5, na.rm = TRUE)) %>%
  mutate(var = "Number of Quit Attempts",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  select(var = auh_6_1) %>%
  mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ mos.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_2) %>%
  mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 mos.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_3) %>%
  mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_4) %>%
  mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_5) %>%
  mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_6) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_7) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_7) %>%
  mutate(var = fct_relevel(factor(var, c("Yes", "No")))) %>%
  group_by(var) %>%
  summarise(n = n()) %>%
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) %>%
  rowwise() %>%
  # calculate dsm5 score by adding up dsm5_1 through dsm5_11
  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7,
                            dsm5_8, dsm5_9, dsm5_10, dsm5_11))) %>%
  ungroup() %>%
  summarise(mean = mean(dsm5_total),
            SD = sd(dsm5_total)) %>%
  mutate(var = "AUD DSM-5 Symptom Count",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  select(var = assist_1_1) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_2) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Cannabis (marijuana, pot, grass, hash, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_3) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Cocaine (coke, crack, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_4) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_5) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Inhalants (nitrous, glue, petrol, paint thinner, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_6) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_7) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_8) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Opioids (heroin, morphine, methadone, codeine, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc"))
```

```{r table-2}
auh %>% 
  kbl(booktabs = TRUE,
      caption = "Alcohol Related Characteristics for the Sample",
      col.names = c("", "n", "%", "M", "SD"),
      align = c("l", "c", "c", "c", "c"),
      digits = 1,
      longtable = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>% 
  row_spec(row = 0, align = "c", italic = TRUE) %>% 
  pack_rows("AUD Milestones", 1, 4, bold = FALSE) %>%    
  pack_rows("Types of Treatment (Can choose more than 1)", 6, 12, bold = FALSE) %>%
  pack_rows("Received Medication for AUD", 13, 14, bold = FALSE) %>%
  pack_rows("Lifetime Drug Use", 16, 23, bold = FALSE) %>%
  footnote("N = 154")
```

\newpage

## Behavioral Measures of Acceptability

### Participation

Figure 1 shows participant attrition/discontinuation at each phase of the study. Of the 192 eligible participants at screening, only 1 did not consent after hearing the details of the study. Enrollment occurred during a second visit 1 week later. A total of 169 participants completed enrollment.

Study phases where we lost eligible participants are highlighted in red in Figure 1. In addition, we coarsely tabulated participants stated reasons for discontinuation as due to acceptability, other reasons, or unknown in this figure. Eleven participants (5.7%) were lost due to acceptability-relevant causes (e.g., no longer interested, non-compliance with data, or citing study demands as too burdensome). Other reasons for discontinuation not related to the acceptability of the signals include circumstances such as moving or no longer wishing to abstain from alcohol. It should be noted that 31 participants (16.1%) were lost to follow-up such that we had no information about their reasons for discontinuation. We provide more granular tabulation of these reasons for discontinuation in Table 3. <!-- Denominator for above percentages is 192-->

### Opt-In and Compliance

<!-- JOHN- all these descriptives below are based on our analysis sample (N = 154). Not sure if we should clarify this somewhere.  -->

<!--JJC - participatns didnt "opt-in" for EMA.   It was required.  Not sure about sleep or audio-->
All participants (100%) opted-in to provide data for EMA, sleep quality, and all passive personal sensing methods (geolocation, cellular communication logs, text message content). Three participants (2%) did not provide any audio check-ins while on study. 

<!--combine two EMA compliance figures into one 2-panel figure-->
<!--histogram for audio check-in compliance?-->
Daily compliance rates were relatively high for EMA such that 94% of participants completed at least 1 of the 4 EMAs every day. Figure 2 shows mean weekly compliance with completing 1 EMA per day for each week on study. On average, participants completed 3.2 EMAs every day. The overall compliance rate for all requested EMAs was 81%. Figure 3 shows mean weekly compliance with our 4-time daily EMA protocol. Participants' completion rate for the audio check-in was 55%. That is, of their total days on study, participants completed an audio check-in on approximately half of them. Figure 4 shows mean weekly compliance for completing the daily audio check-in. In Multimedia Appendix 3 we also report compliance for participants who completed the 3-month study compared to those who dropped out prior to completion (Figures S3 -- S5).



\newpage

```{r table-3-code}
notes_incomplete <- notes_incomplete %>%
  filter(screen != "no consent") %>%
  mutate(characterize = case_when(characterize == "no_transportation" ~ "No longer has transportation",
                                  characterize == "not_sober" ~ "No longer sober or no longer wishes to abstain from alcohol",
                                  characterize == "rescheduled" ~ "Rescheduled multiple times before cancelling/no showing",
                                  characterize == "treatment" ~ "No longer sober or no longer wishes to abstain from alcohol",
                                  characterize == "compliance" ~ "Noncompliance with providing data",
                                  characterize == "lapse" ~ "No longer sober or no longer wishes to abstain from alcohol",
                                  characterize == "cell_service" ~ "Cell service shut off",
                                  characterize == "move" ~ "Moved out of state",
                                  characterize == "study_demands" ~ "Cited study demands as too burdensome",
                                  characterize == "Unreachable" ~ "Unknown",
                                  characterize == "Unspecified concerns from staff" ~ "Staff concerns",
                                  characterize == "inappropriate behavior" ~ "Staff concerns",
                                  characterize == "mental health" ~ "Mental health concerns",
                                  characterize == "no aud" ~ "Does not meet criteria for moderate or severe AUD",
                                  characterize == "phone" ~ "Ineligible phone",
                                  is.na(characterize) ~ "Unknown",
                                  TRUE ~ characterize))
```



```{r table-3-code-2}
a <- notes_incomplete %>% 
  filter(screen != "ineligible") %>% 
  filter(intake != "complete" | is.na(intake)) %>% 
  filter(intake != "ineligible" | is.na(intake)) %>% 
  group_by(characterize) %>% 
  summarise(n = n()) %>% 
  mutate(percent = n/sum(n)*100) %>% 
  adorn_totals("row") 

b <- notes_incomplete %>% 
  filter(screen != "ineligible") %>% 
  filter(completed_1month == "no") %>% 
  filter(intake == "complete") %>% 
  group_by(characterize) %>% 
  summarise(n = n()) %>% 
  mutate(percent = n/sum(n)*100) %>% 
  adorn_totals("row") 

c <- notes_incomplete %>% 
  filter(screen != "ineligible") %>% 
  filter(completed_1month == "yes") %>% 
  group_by(characterize) %>% 
  summarise(n = n()) %>% 
  mutate(percent = n/sum(n)*100) %>% 
  adorn_totals("row") 

footnote_table_3 <- "Bolded rows depict acceptability-related discontinuation." 
footnote_table_3a <- "These participants are labeled as `Not Enrolled' in Figure 1." 
footnote_table_3b <- "These participants are labeled as `Discontinued' in Figure 1." 
footnote_table_3c <- "These participants are labeled as `Participated through 1st month follow-up' or `Participated through 2nd month follow-up' in Figure 1."
```
<!-- Kendra two notes here. First, linebreaks don't work in footnotes, but you can have separate footnotes that appear on separate lines. See below for syntax. Also, I fixed the open single quote not rendering correctly by replacing it with a backtick-->


```{r table-3}
a %>% 
  bind_rows(b, c) %>% 
  kbl(booktabs = TRUE,
      col.names = c("", "n", "%"), 
      caption = "Characterization of Discontinued Participants",
      align = c("l", "c", "c"),
      digits = 1,
      longtable = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>%
  row_spec(row = 0, align = "c", italic = TRUE) %>%
  row_spec(row = 3, bold = TRUE) %>% 
  row_spec(row = 8, bold = TRUE) %>% 
  row_spec(row = 10, bold = TRUE) %>% 
  row_spec(row = 15, bold = TRUE) %>% 
  row_spec(row = 17, bold = TRUE) %>% 
  row_spec(row = 19, bold = TRUE) %>% 
  pack_rows("Eligible and consented participants discontinued prior to completing enrollment$^a$", 1, 6, bold = FALSE, escape = FALSE) %>% 
  pack_rows("Enrolled participants discontinued prior to first month follow-up$^b$", 7, 13, bold = FALSE, escape = FALSE) %>% 
  pack_rows("Enrolled participants discontinued after the first month follow-up$^c$", 14, 22, bold = FALSE, escape = FALSE) %>% 
  footnote(general=footnote_table_3, alphabet = c(footnote_table_3a, footnote_table_3b, footnote_table_3c), threeparttable = TRUE, escape = FALSE)
```
<!-- General footnotes are not preceded by anything, alphabet by a letter, you can also specify symbol=c(). Anything in the c() will appear on separate lines with the specified initial marker-->
\newpage

## Self-reported Acceptability

```{r one sample t tests}
# interference
int_sleep <- broom::tidy(lm(sleep_interfere ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sleep_interfere, na.rm = TRUE),
         d = mean(data_last$sleep_interfere, na.rm = TRUE)/sd)
int_audio <- broom::tidy(lm(audio_checkin_interfere ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$audio_checkin_interfere, na.rm = TRUE),
         d = mean(data_last$audio_checkin_interfere, na.rm = TRUE)/sd)
int_ema <- broom::tidy(lm(daily_survey_interfere ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$daily_survey_interfere, na.rm = TRUE),
         d = mean(data_last$daily_survey_interfere, na.rm = TRUE)/sd)

# dislike
dis_sleep <- broom::tidy(lm(sleep_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sleep_dislike, na.rm = TRUE),
         d = mean(data_last$sleep_dislike, na.rm = TRUE)/sd)
dis_audio <- broom::tidy(lm(audio_checkin_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$audio_checkin_dislike, na.rm = TRUE),
         d = mean(data_last$audio_checkin_dislike, na.rm = TRUE)/sd)
dis_ema <- broom::tidy(lm(daily_survey_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$daily_survey_dislike, na.rm = TRUE),
         d = mean(data_last$daily_survey_dislike, na.rm = TRUE)/sd)
dis_geolocation <- broom::tidy(lm(location_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$location_dislike, na.rm = TRUE),
         d = mean(data_last$location_dislike, na.rm = TRUE)/sd)
dis_logs <- broom::tidy(lm(all_logs_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$all_logs_dislike, na.rm = TRUE),
         d = mean(data_last$all_logs_dislike, na.rm = TRUE)/sd)
dis_text_content <- broom::tidy(lm(sms_content_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sms_content_dislike, na.rm = TRUE),
         d = mean(data_last$sms_content_dislike, na.rm = TRUE)/sd)

# willingness
use_audio <- broom::tidy(lm(audio_checkin_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$audio_checkin_1year, na.rm = TRUE),
         d = mean(data_last$audio_checkin_1year, na.rm = TRUE)/sd)
use_sleep <- broom::tidy(lm(sleep_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sleep_1year, na.rm = TRUE),
         d = mean(data_last$sleep_1year, na.rm = TRUE)/sd)
use_ema <- broom::tidy(lm(daily_survey_4_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$daily_survey_4_1year, na.rm = TRUE),
         d = mean(data_last$daily_survey_4_1year, na.rm = TRUE)/sd)
use_geolocation <- broom::tidy(lm(location_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$location_1year, na.rm = TRUE),
         d = mean(data_last$location_1year, na.rm = TRUE)/sd)
use_logs <- broom::tidy(lm(all_logs_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$all_logs_1year, na.rm = TRUE),
         d = mean(data_last$all_logs_1year, na.rm = TRUE)/sd)
use_text_content <- broom::tidy(lm(sms_content_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sms_content_1year, na.rm = TRUE),
         d = mean(data_last$sms_content_1year, na.rm = TRUE)/sd)
```



```{r active effort t test}
data_dislike <- data_last %>% 
  # get subject level means for active and passive measures
  group_by(subid) %>% 
  summarise(Active = mean(c(daily_survey_dislike, audio_checkin_dislike), na.rm = TRUE),
            Passive = mean(c(location_dislike, all_logs_dislike, sms_content_dislike), na.rm = TRUE),
            dislike_diff = Passive - Active) 

model_dislike <- lm(dislike_diff ~ 1, data = data_dislike)

data_willingness <- data_last %>% 
  # get subject level means for active and passive measures
  group_by(subid) %>% 
  summarise(Active = mean(c(daily_survey_4_1year, audio_checkin_1year), na.rm = TRUE),
            Passive = mean(c(location_1year, all_logs_1year, sms_content_1year), na.rm = TRUE),
            willingness_diff = Passive - Active) 

model_willingness <- lm(willingness_diff ~ 1, data = data_willingness)
```


### Interference

Figure 5 shows the distribution of participant responses to the self-reported acceptability item about interference. Responses are grouped by personal sensing data stream and the amount of active effort required to collect it. One sample t-tests revealed that each mean interference score (depicted as the solid red line) was significantly more acceptable than 0 (gray dashed line indicating undecided). Table 4 reports the summary statistics for each one sample t-test and pairwise correlations between personal sensing data streams. An ICC (type 3) showed that, on average, interference ratings were moderately consistent across the data streams, ICC = .42, 95% CI = [.31 - .53].

### Dislike.

Figure 6 shows the distribution of participant responses to the self-reported acceptability item about dislike by personal sensing data stream and amount of active effort required to collect it. One sample t-tests revealed that each mean dislike score was significantly more acceptable than 0. Table 5 reports the summary statistics for each one sample t-test and pairwise correlations between personal sensing data streams. An ICC (type 3) showed that, on average, the dislike ratings were moderately consistent across the data streams, ICC = .42, 95% CI = [.35 - .48].

We also assessed the effect of active effort on dislike ratings (Figure 7). We conducted a paired samples t-test to compare the average dislike for active (audio check-in, EMA) vs. passive (geolocation, cellular communication logs, text message content) methods. Participants did not significantly differ in their dislike of active vs. passive methods, $t(153)$ = `r round(broom::tidy(summary(model_dislike))$statistic, 2)`, $p$ = `r round(broom::tidy(summary(model_dislike))$p.value, 2)`, $d$ = `r round((mean(data_dislike$Passive) - mean(data_dislike$Active)) / sd(data_dislike$dislike_diff), 2)`. 

### Willingness to Use for 1 Year

Figure 8 shows the distribution of participant responses to the self-reported acceptability item about willingness to use for 1 year for each personal sensing data stream. One sample t-tests revealed that each mean willingness score was significantly more acceptable than 0. Table 6 reports the summary statistics for each one sample t-test and pairwise correlations between personal sensing data streams. An ICC (type 3) showed that, on average, the willingness ratings were moderately consistent across the data streams, ICC = .52, 95% CI = [.46 - .58].

We also assessed the effect of active effort on willingness ratings (Figure 9). We conducted a paired samples t-test of average the average willingness to use for 1 year for active (audio check-in, EMA) vs. passive (geolocation, cellular communication logs, text message content) signals. Participants reported higher acceptability with respect to willingness for passive data streams ($M$ = `r round(mean(data_willingness$Passive), 1)`, $SD$ = `r round(sd(data_willingness$Passive), 1)`) relative to active data streams ($M$ = `r round(mean(data_willingness$Active), 1)`, $SD$ = `r round(sd(data_willingness$Active), 1)`), $t(153)$ = `r round(broom::tidy(summary(model_willingness))$statistic, 2)`, $p$ = `r round(broom::tidy(summary(model_willingness))$p.value, 2)`, $d$ = `r round((mean(data_willingness$Passive) - mean(data_willingness$Active)) / sd(data_willingness$willingness_diff), 2)`. 


```{r}
footnote_table_4 <- "Initial columns indicate bivariate correlations among data streams. Final columns represent the number of participants (N), mean and standard deviation (M, SD), t-statistic (t) and Cohen’s d Effect size (d) for the one sample t-tests against 0 (undecided).  Higher values represent higher acceptability.  Active methods are displayed in red and passive methods are displayed in blue."
footnote_table_4a <- "p < .05"
```

```{r table-4}
corrplot_int <- data_last %>% 
  select(`Audio Check-in` = audio_checkin_interfere,
         `EMA` = daily_survey_interfere,
         `Sleep Quality` = sleep_interfere) %>%
  corx(triangle = "lower",
       stars = c(0.05),
       method = "pearson",
       describe = c(`$M$` = mean, `$SD$` = sd))

corrplot_int <- as_tibble(corrplot_int, rownames = " ")
corrplot_int[corrplot_int == " - " ] <- "--"

corrplot_int <- corrplot_int %>% 
  mutate(`$N$` = c(154, 154, 87)) %>% 
  mutate(`$t$` = c(str_c(round(int_audio$statistic, 2), "*"), 
                   str_c(round(int_ema$statistic, 2), "*"), 
                   str_c(round(int_sleep$statistic, 2), "*"))) %>% 
  mutate(`$d$` = (c(round(int_audio$d, 2), round(int_ema$d, 2), round(int_sleep$d, 2)))) %>% 
  select(` `, `1`, `2`, `$N$`, `$M$`, `$SD$`, everything())

corrplot_int %>% 
  kbl(align = c("l", "c", "c", "c", "c", "c", "c", "c"),
      caption = "Bivariate and Univariate Statistics for Interference by Personal Sensing Data Stream",
      booktabs = TRUE,
      escape = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>% 
  pack_rows("Active", 1, 3) %>% 
  row_spec(1:3, background = "lightred") %>%
  footnote(general = footnote_table_4, symbol = footnote_table_4a, threeparttable = TRUE, escape = FALSE)
```



```{r}
footnote_table_5 <- "Initial columns indicate bivariate correlations among data streams. Final columns represent the number of participants (N), mean and standard deviation (M, SD), t-statistic (t) and Cohen’s d Effect size (d) for the one sample t-tests against 0 (undecided).  Higher values represent higher acceptability. Active methods are displayed in red and passive methods are displayed in blue."
footnote_table_5a <- "p < .05"
```

```{r table-5}
corrplot_dis <- data_last %>% 
  select(`Audio Check-in` = audio_checkin_dislike,
         `EMA` = daily_survey_dislike,
         `Sleep Quality` = sleep_dislike,
         `Geolocation` = location_dislike,
         `Cellular Communication Logs` = all_logs_dislike,
         `Text Message Content` = sms_content_dislike) %>% 
  corx(triangle = "lower",
       stars = c(0.05),
       method = "pearson",
       describe = c(`$M$` = mean, `$SD$` = sd))

corrplot_dis <- as_tibble(corrplot_dis, rownames = " ")
corrplot_dis[corrplot_dis== " - " ] <- "--"

corrplot_dis <- corrplot_dis %>% 
  mutate(`$N$` = c(154, 154, 87, 154, 154, 154)) %>% 
  mutate(`$t$` = c(str_c(round(dis_audio$statistic, 2), "*"), 
                   str_c(round(dis_ema$statistic, 2), "*"), 
                   str_c(round(dis_sleep$statistic, 2), "*"),
                   str_c(round(dis_geolocation$statistic, 2), "*"), 
                   str_c(round(dis_logs$statistic, 2), "*"), 
                   str_c(round(dis_text_content$statistic, 2), "*"))) %>% 
  mutate(`$d$` = (c(round(dis_audio$d, 2), round(dis_ema$d, 2), round(dis_sleep$d, 2),
                    round(dis_geolocation$d, 2), round(dis_logs$d, 2), round(dis_text_content$d, 2)))) %>% 
  select(` `, `1`, `2`, `3`, `4`, `5`, `$N$`, `$M$`, `$SD$`, everything())

corrplot_dis %>% 
  kbl(align = c("l", "c", "c", "c", "c", "c", "c", "c"),
      caption = "Bivariate and Univariate Statistics for Dislike by Personal Sensing Data Stream",
      booktabs = TRUE,
      escape = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position", "scale_down")) %>% 
  pack_rows("Active", 1, 3) %>%
  pack_rows("Passive", 4, 6) %>% 
  row_spec(1:3, background = "lightred") %>%
  row_spec(4:6, background = "lightblue") %>%
  footnote(general = footnote_table_5, symbol = footnote_table_5a, threeparttable = TRUE, escape = FALSE)
```

```{r}
footnote_table_6 <- "Initial columns indicate bivariate correlations among data streams. Final columns represent the number of participants (N), mean and standard deviation (M, SD), t-statistic (t) and Cohen’s d Effect size (d) for the one sample t-tests against 0 (undecided).  Higher values represent higher acceptability. Active methods are displayed in red and passive methods are displayed in blue."
footnote_table_6a <- "p < .05"
```  


```{r table-6}
corrplot_will <- data_last %>% 
  select(`Audio Check-in` = audio_checkin_1year,
         `EMA` = daily_survey_4_1year,
         `Sleep Quality` = sleep_1year,
         `Geolocation` = location_1year,
         `Cellular Communication Logs` = all_logs_1year,
         `Text Message Content` = sms_content_1year) %>% 
  corx(triangle = "lower",
       stars = c(0.05),
       method = "pearson",
       describe = c(`$M$` = mean, `$SD$` = sd))

corrplot_will <- as_tibble(corrplot_will, rownames = " ")
corrplot_will[corrplot_will== " - " ] <- "--"

corrplot_will <- corrplot_will %>% 
  mutate(`$N$` = c(154, 154, 87, 154, 154, 154)) %>% 
  mutate(`$t$` = c(str_c(round(use_audio$statistic, 2), "*"),
                   str_c(round(use_ema$statistic, 2), "*"), 
                   str_c(round(use_sleep$statistic, 2), "*"),
                   str_c(round(use_geolocation$statistic, 2), "*"), 
                   str_c(round(use_logs$statistic, 2), "*"), 
                   str_c(round(use_text_content$statistic, 2), "*"))) %>% 
  mutate(`$d$` = (c(round(use_audio$d, 2), round(use_ema$d, 2), round(use_sleep$d, 2),
                    round(use_geolocation$d, 2), round(use_logs$d, 2), round(use_text_content$d, 2)))) %>% 
  select(` `, `1`, `2`, `3`, `4`, `5`, `$N$`, `$M$`, `$SD$`, everything())

corrplot_will %>% 
  kbl(align = c("l", "c", "c", "c", "c", "c", "c", "c", "c", "c"),
      caption = "Bivariate and Univariate Statistics for Willingness to Use for 1 Year by Personal Sensing Data Stream",
      booktabs = TRUE,
      escape = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position", "scale_down")) %>% 
  pack_rows("Active", 1, 3) %>% 
  pack_rows("Passive", 4, 6) %>% 
  row_spec(1:3, background = "lightred") %>%
  row_spec(4:6, background = "lightblue") %>%
  footnote(general = footnote_table_6, symbol = footnote_table_6a, threeparttable = TRUE, escape = FALSE)
```

\newpage


# Discussion

This study evaluated the acceptability of active and passive personal sensing methods for a variety of raw data streams and associated methods.  To this end, we assessed participants’ subjective perceptions of each sensing method and their choices/behaviors about both their participation in the study and their provision of raw data streams for each method.  We focused on participants with moderate to severe alcohol use disorder because they might have been expected to be less willing to share sensitive, private information due to the stigma associated with their disorder [@kilianStigmatizationPeopleAlcohol2021 <!--Could also possibly cite @barryStigmaDiscriminationTreatment2014 - this ref is about drug addiction stigma vs other mental illness, not sure if we also want to include it or just stick to the review on alcohol stigma-->].  However, if these sensing methods were acceptable to them, highly promising opportunities are now emerging to address their unmet treatment needs [@substanceabuseandmentalhealthservicesadministration] with technological solutions that include digital therapeutics combined with personal sensing [@tedxtalksMentalHealthcareOur2022].  We believe that the results from this study warrant cautious optimism that individuals with AUD will accept the use of personal sensing.


## Behavioral Indices of Acceptability

It appears that a large majority of individuals with AUD are indeed willing to provide these sensitive, personally sensed raw data streams based on their behavioral choices regarding consent, enrollment, and opt-in for data collection in this study.  All but one of the individuals (191/192; 99.5%) who were eligible to participate consented to the personal sensing procedures.  Most of these individuals also returned one week later to formally enroll in the study and begin to provide these data (169/191; 88%).  Furthermore, all (169/169; 100%) of the participants who enrolled in the study explicitly opted-in to provide the three arguably most sensitive passive data streams - geolocation, cellular communication logs, and text message content.  

These consent, enrollment, and opt-in numbers could be considered upper- and lower-bound estimates of the percentage of individuals who are willing to provide these raw data streams.  The very high percentage for consent may overestimate willingness because some of these individuals may have reconsidered their initial decision on further reflection such that they did not return for the next study visit to enroll formally.  However, the still quite high enrollment percentage may underestimate willingness to provide these data because some attrition was expected between consent and enrollment visits due to the instability associated with the early stages of recovery for alcohol use disorder.  In fact, table 3 indicates that almost half of the participants who consented but did not enroll may have done so for reasons other than their willingness to provide these raw data streams (e.g., health concerns, no transportation to lab, made repeated attempts to reschedule before discontinuing). <!--should we list comments explicitly like we do later for other examples?--> <!-- JOHN: we don't have the reasons for discontinuation in the participants own words. These are extracted from staff notes-->

<!--need to find a better way to describe the combined subsamples of discontinued and those who did not compete-->
<!--no longer interested is included in personal sensing reasons in results.   Decide and be consistent-->
Most enrolled participants were also able to sustain their commitment to provide these sensed data streams over time.  More than 91% (154/169) provided at least one month of sensed data and a large majority (133/169; 79%) provided all three months.  As with enrollment statistics, these numbers also likely underestimate participants ability to sustain personal sensing because many of the participants who discontinued or did not complete the study reported reasons to stop their participation that were unrelated to personal sensing (e.g., family crisis, relapse, moved out of state).  However, some participants (N = 4) did explicitly report reasons that appeared related to personal sensing (e.g., study demands too burdensome <!--example quotes?  In this instance, I dont think so?--> <!--John - I agree in this case no quotes since we don't know exactly what the participant said and what was staff interpretation-->).  And others who stop participating may have been influenced by their experiences with personal sensing without formally reporting those concerns.  

Participants who enrolled but then discontinued because of the personal sensing methods may have been influenced more by issues related to the burden associated with active sensing rather than more general issues related to data sensitivity/privacy.  Participants concerned about sharing passively sensed private information such as their moment by moment location or cellular communications would likely have had these concerns from the beginning such that they would not have consented, enrolled, and then opted-in to provide these sensitive data.   However, the burden associated with active sensing (e.g., 4x daily EMA, daily audio check-ins) may not have been clear to them until they tried to sustain those methods over time.

<!--should we consider that a method could be acceptable to a participant but they still may not comply at high levels?-->
Participants compliance with personal sensing procedures provides another behavioral window through which to judge the acceptability of these methods.  As noted previously, all participants opted-in to provide the raw data streams associated with geolocation, cellular communication logs, and text message content.  No further action was required of participants to achieve full compliance because data collection was completely passive for these raw data streams. As such, issues of assessment burden have little impact on the acceptability of passive sensing methods.  Acceptability of passive sensing methods is likely most strongly influenced by issues of data sensitivity and privacy. 

Assessment burden may play a larger role in both the acceptability of active sensing methods and participant compliance with the associated procedures.  Nonetheless, participants displayed relatively high compliance (81% of EMAs completed) with the 4x daily EMA.  This is notable because our study duration of 3 months was substantially longer than typical studies using EMA, which often last only 2 - 4 weeks [@jonesComplianceEcologicalMomentary2019; @wrzusEcologicalMomentaryAssessment2022 <!--JOHN: These are 2 meta-analysis that both support the average length of EMA studies are short.-->].  This increases confidence in this feasibility of this active sensing method for research and clinical applications that require longer monitoring periods.  Of course, this level of compliance may be contingent on the measurement parameters used in our study (4x daily survey of 7 - 10 items).  In fact, even higher compliance may have been observed if measurement was limited to 1 EMA per day given that 94% of our participants completed at least one of the four EMAs every day.  <!--could comment on the self-report data on 1x surveys here but seems confusing given that this section focuses on behavior and we havent yet discussed self-report--> 

We included a second <!--We also called sleep active but we dont say much about it here in this section--> and more novel daily active sensing method in this study, audio check-ins.  These audio check-ins have high potential as a rich source of information about participants' daily experiences.  Natural language processing of transcripts of their check-ins can provide a novel window into their thoughts [@tausczikPsychologicalMeaningWords2010; @tackmanDepressionNegativeEmotionality2019; @jacobucciUseTextbasedResponses2021; @lowNaturalLanguageProcessing2020]. <!--JOHN: It might be worthwhile to note that this type of data stream may offer more personal and candid disclosures of emotions and feelings than other data collection methods (e.g., due to open-ended nature, sense of annonymity, etc.). There are some interesting examples and cites of this in  capturing emotional candor section of @cottinghamCapturingEmotionAudio2020-->  Analyses of the acoustic characteristics of their check-ins may yield independent measures of their affective state [@beloualiAcousticLanguageAnalysis2021; @faurholt-jepsenVoiceAnalysisObjective2016].    

Unfortunately, overall participant compliance with the daily audio check-ins was relatively low (55% of check-ins completed) and 2% of the sample did not complete any check-ins throughout their entire study period. Participants' free-response evaluations of this method highlighted some concerns that could be addressed in future uses to increase compliance (e.g., timing of the check-ins, technical issues with recording and sending check-ins, use of the same prompt for all check-ins).  However, privacy issues related to recording the audio check-in were also reported by many participants and may present an inherent challenge to the use of this method.  As examples of this concern (see <!--see supplemental table ??--> for all evaluations), participants reported:     

> "It was often difficult to find privacy to respond."

> "Sometimes I wouldn't be able to do it because I was with people too much which was frustrating."

> "Only inconvenient when I already was at work and needed a quiet/private place to speak."

> "It takes time out of your day where you have to completely switch locations just so you can do it in private. I don't like that people could hear me and the topic wherever and whenever so I stopped using it."

> "I also have to keep privacy in mind since it's not something I'd like to complete within earshot of others."

> "The only negative part about the daily check in was my mom, who I live and work with, listening in while I was talking. I would have to find a time when she was preoccupied or away from me to talk freely."

Despite these issues, many other participants valued and believed they benefited from recording these daily audio check-ins.  For example:

> "I loved this part! It was like journaling kind of where i would discover things that were hidden in my subconscious. I have a recording app on my phone and i will continue doing the voice check in as a form of checking in with myself."

> "This was my favorite part of the study. It helped me to set a good intention towards my recovery."

> "To be able to verbalize my feelings was the most best therapy."
 
> "I found it very helpful to reflect on how my day was in general when it came to my urges and cravings."
 
> "I think it was most important for personal reflection throughout the day on my recovery process."
 
> "It helped me realize my emotions and I had something to answer to which helped keep me on track."
 
> "It's a good way to center yourself and focus on what you are feeling that day."

Consistent with this somewhat polarized evaluation of the audio check-ins, a more nuanced consideration of distribution for compliance across participants suggested it was somewhat bi-modal <!--if we say this, we need a supporting figure-->.  Participants tended to either comply well or very poorly with this method. We will build on these observations about divergent preferences later when we discuss sensing systems that could accommodate different sets of sensing methods and raw data streams across individuals.


## Subjective Perceptions of Acceptability

Participants explicit self-report of their perceptions about the acceptability of these personal sensing methods were generally consistent with their behavior as described above.  Specifically, on average, participants rated all of the sensing methods more favorable than the neutral mid-point ("undecided") of the rating scales for all three dimensions we evaluated - interference, dislike, and willingness to use for a year.  These self-report data combined with our behavioral measures to suggest that all of these sensing methods can be considered for use with the majority of individuals with alcohol use disorder.  

Despite the aggregate positive perceptions of the full sample, non-trivial percentages of participants did report individual ratings that were more negative than the neutral mid-point across sensing methods and specific self-report items .  <!--Kendra, can you get us a markup of the percentages of the two low values from rating scale for each method and item to see if I missed anything here?--> <!--JOHN: See burden channel for markdown titled ana_acceptability_negative--> For example, approximately 20% of participants agreed or strongly agreed that the audio check-ins interfered with their daily activities.  Approximately 20% of participants agreed or strongly agreed that they disliked both the audio check-ins and providing access to the content of their text messages.  And approximately 20% of participants disagreed or strongly disagreed that they would be willing to use our sensing methods for audio check-ins, ecological momentary assessments, and text message content <!--does this get close to 20%--> <!--JOHN: not sure what you are asking here. But I think the markdown of percentages will answer this!--> for 1 year to help their recovery.  This suggests that there is still need to improve each of these sensing methods to make them more acceptable to a larger percentage of individuals.  The free response evaluations of each method (see supplemental table <!--INSERT TABLE NUMBER-->) provide a starting point to address participant concerns.  That said, our participants did generally opt-in and comply with our sensing methods despite reporting these concerns.  Therefore, it is not clear yet at what threshold these concerns will translate to barriers for use or compliance with these methods.  

Participants self-reported perceptions were only moderately consistent across the different sensing methods.  This can be seen in the moderate ICCs (and bivariate correlations) across methods for each self-report item.  In other words, it is not true that high dislike ratings for one sensing method by a specific participant strongly indicated that this same participant would also dislike the other sensing methods.  This is also true for ratings of interference and willingness to use for a year items.  Participants could dislike (or be unwilling to use, etc) one method but not others.  To the degree to which concerns are method specific, opportunities may exist to tailor sensing systems to user preferences.  In other words, participants could opt-out of methods they deemed unacceptable but provide those other sensing methods that were acceptable to them.  Algorithms <!--I think we need to be clearer in the paper about examples of what sensed data can be used for - e.g., risk prediction--> that use sensed data could then be developed for different combinations of available raw data streams.  Participants could be educated that personalized algorithms will likely perform better if given access to more raw data streams to encourage them to use a high threshold to opt-out.  However, allowing them to opt-out of some methods may increase the number of participants who will agree to provide sensed data. <!--reference back to the audio check-in here?-->

Not surprisingly, there was some evidence that participants found passive sensing methods to be more acceptable than active sensing methods overall.  Specifically, mean ratings for willingness to use for a year were significantly higher for passive vs. active sensing methods.  However, the magnitude of this effect was small and mean willingness was significant greater than the neutral mid-point for both active and passive methods.  In addition, there was no difference in mean dislike ratings for active vs. passive methods.  These comparisons between active and passive methods increase our confidence somewhat that the selective use of active measures, when necessary, may be acceptable to participants for relatively long periods.  Of course, we cannot speculate strongly beyond three months from this study.  

Some sensing methods (e.g., EMA, audio check-ins) will always require active input from users but other methods may become more passive with further technological advances.  For example, our sensing of sleep quality in this study made use of an early version of the Beddit sleep monitor that required participants to actively log when they entered and exited their bed during each period of sleep.   However, later versions of the Beddit detect periods of sleep automatically. Similarly, we discontinued sensing of physiology with the Empatica E4 in an early phase of our study because participants had to manually connect the wristband each night to a tablet to upload their data.  This proved too burdensome and complex for most participants.  However, the current version of the Empatica E4 claims to have improved automatic bluetooth streaming of the data to the cloud, which if robust, would greatly reduce the burden associated with physiology sensing.
<!--Discuss hypothetical about 1x per day, the caveat hypothetical?-->



## Limitations, Key Issues, and Future Directions

The overall acceptability of personal sensing to research participants and patients is likely a function of both the perceived costs and benefits for those individuals [@pavlouConsumerAcceptanceElectronic2003; @schnallTrustPerceivedRisk2015; @atienzaConsumerAttitudesPerceptions2015].  However, we focused on measuring only perceived costs (e.g., privacy, burden) associated with personal sensing because the benefits to participants from the sensed data collected in this research study were minimal.  Participants were provided with modest financial incentives to complete the EMAs ($25/month) and to provide access to the two passively sensed raw data streams ($10/month separately for geolocation and cellular communications logs) <!--JOHN: I am not sure that $10 is accurate for cellular communication logs, I am waiting to hear back from Susan about this.-->.  These sensed data streams were not used to provide any clinical benefit to participants' recovery in our study.

Monetary incentives are commonly used in research to provide a more favorable cost/benefit ratio surrounding specific methods or overall participation.  Such monetary incentives are commonplace and recommended when using active personal sensing methods like EMA <!--REFS--><!-- Still working on: @vachonComplianceRetentionExperience2019 and @morrenComplianceMomentaryPain2009 show higher compliance rates with higher incentives. Though this is not a consistent correlation. Looking for more of a recommendation. Maybe something by Shiffman?-->.  However, the incentives  to provide access to passively sensed geolocation and cellular communications in our study may have contributed to the  acceptance of these methods and the success we had collecting those sensitive data from participants.

Monetary incentives to increase the acceptability of personal sensing do not need to be limited to their use in research settings.  Incentives can also be used as part of treatment or continuing care in clinical settings.  For example, the use of monetary incentives or equivalents (e.g., prizes) as part of a contingency management program is well established to support abstinence from alcohol or other drugs  [@prendergastContingencyManagementTreatment2006; @ginleyLongtermEfficacyContingency2021].  If personal sensing proved useful for the treatment or ongoing support of patient's recovery, similar incentives could be established to encourage patients to provide these sensed data.  

Incentives may be less necessary in clinical settings when more direct clinical benefits from personal sensing are available.  For example, research has suggested that privacy concerns associated with personal sensing may be reduced if participants perceive that they will benefit from the sensed data[@atienzaConsumerAttitudesPerceptions2015; @klasnjaExploringPrivacyConcerns2009]. There was some evidence for this perspective in the free response comments from our participants as well.

> "As long as my personal information was secure and only shared by study staff, and its collection could help in my recovery, I'm comfortable with it."

> "I felt all right having my location tracked. If it were used in a way to keep me from relapsing my feeling about it would be even more positive."

<!--JOHN: Here are some possible additional comments. Not sure which fit best, so I put here to discuss.-->
> "Neutral about this. Was okay to have done in the context of the study or for an app that would help me stay sober."

> "I don’t mind being tracked since I appreciate the purpose of it."

> "Highly personal but if it helps okay."

> "I'm a somewhat typical introverted, slightly paranoid, grump, so the idea of being tracked is automatically negative. That being said, if it's something that helps people that can be a really good thing."

> "It seems this was more to aid with the study research than to help with my recovery. If i could see a way it would be prioritized to help me, i would be more willing to have them tracked longer. Because of the way its use was communicated to me, it didn't seem to improve accountability."

Given this, the acceptability of personal sensing may higher than observed in our study if the sensing system was implemented as part of their treatment or continuing care during their recovery.  Digital therapeutics are particularly well-positioned to use sensed data to select, personalize, or time the delivery of interventions and others supports to improve clinical outcomes. 




### Acceptability also affected by trust

Trust affects concerns about privacy

who gets the data matters

* Research setting comes with higher trust?
* patients keep data and results may also work
* third party access may work but business model matters - can't sell data, cant affect care or insurance
	
population matters
* AUD is highly stigmatized but still not illegal behavior RE OUD.
* But did have high level of experience with other substances
* Sample not racially/ethnically diverse.  Trust might be lower still in groups that experience systemic racism




> "Invasive in theory, no problems actually because of confidentiality."

> "I trusted the study group to not use my personal information for any other use."



It is not surprising that trust plays an integral role in a person's acceptance of these methods. We know from the literature who people are sharing their personal sensing data with matters [@atienzaConsumerAttitudesPerceptions2015; @riegerPsychiatryOutpatientsWillingness2019; @rendinaPrivacyTrustData2018; @nicholasRoleDataType2019; @prasadExposingPrivacyConcerns2011]. Focus group studies suggest that people are most comfortable sharing data anonymously with researchers and with their doctor, and least comfortable sharing information with family members, electronic health record databases, and third party apps and websites [@rendinaPrivacyTrustData2018; @nicholasRoleDataType2019; @prasadExposingPrivacyConcerns2011]. This suggests trust is most closely related to trust that the information will be kept confidential.








```Individuals may experience privacy differently in their day to day lives. People belonging to historically marginalized groups may find it more difficult to achieve privacy in their daily life [@marwickPrivacyMarginsUnderstanding2018]. Relatedly, our participants are in recovery for AUD. For some, there may be high costs (e.g., legal consequences, or loss of job, relationships, or health insurance) if information related to their recovery (e.g., frequent lapses) gets in the wrong hands. As a result, personal experiences may affect how much one values privacy and as a result whether one would be willing to provide sensitive data streams.```


```Sensitivity of information may be an especially salient concern when working with clinical populations. Public opinion often holds a stigmatizing view of mental health disorders [@schomerusStigmaAlcoholDependence2011; @barryStigmaDiscriminationTreatment2014; @overtonStigmaMentalIllness2008; @parcesepePublicStigmaMental2013]. As a result, people may fear that their employer or peers will become aware of their diagnosis. These fears can become further compounded when illicit factors like drug use are involved. A negative opinion by neighbors or consequences at work are often cited reasons for not seeking treatment for substance use [@centerforbehavioralhealthstatisticsandquality2019NationalSurvey; @stringerStigmaBarrierSubstance2018]. Thus, the type and sensitivity of data that a personal sensing method collects may be an important contributor to the acceptability of that measure.```

## Beyond Acceptability

Of course, user acceptance of personal sensing methods is necessary but not sufficient for more expansive use of these methods in research and clinical implementations.  There are a variety of other key issues that may facilitate or present barriers to wider use of personal sensing.  These include cost and accessibility, stability over time, and the utility of personal sensing relative to other more traditional methods.  

The smartphone itself is arguably the best available sensing system today.  Today's smartphone contain numerous sensors and other raw data streams that can be used for personal sensing. In our study, we took advantage of GPS and other location services to track geolocation and the microphone for daily check-ins.  We accessed smartphone call and text message logs for communications meta-data and message content.  The smartphone also provided a convenient platform to collect ecological momentary assessments.  

Smartphones also provide a relatively accessible platform for personal sensing.  Despite their high cost, 85% of adults in the US already own a smartphone.  Equally important, this level of ownership is relatively consistent across race/ethnicity, geographic regions (e.g., urban, suburban, rural) and income level [@pewresearchcenterMobileFactSheet2021].  In fact, only 11 of the eligible participants for our study did not already own a  contemporary smartphone.  This is notable given that individuals with AUD may have been expected to present with more barriers to smartphone ownership that then general population.

Personal sensing can also be done with wearable or other sensors outside of the smartphone.  We used Empatica and Beddit systems to sense physiology and sleep, respectively.  The use of watches (e.g., Apple Watch; <!--REF--> <!--JOHN: Do you want me to put reference for apple watch technology or articles with people using apple watches?-->) and wristbands (e.g., Fitbit; <!--REF--> <!--Same question here-->) for sensing activity and some physiology is also increasing [@doryabIdentifyingBehavioralPhenotypes2019; @stevensonUsingEcologicalMomentary2021]<!--JOHN: Here I cite studies using watches for sensing. Let me know if instead you wanted a cite of how use of watches is becoming more prevalent. Also @liaoFutureWearableTechnologies2019 discusses how wearables can improve health care generally. May be a relvant cite?-->.  However, some of these systems can be expensive and none have been adopted widely enough to assume that most users will already own one.  For research applications, this limitation can be overcome by providing the hardware to participants as needed.  However, for clinical implementations, this may either limit or increase the cost to scale the sensing system to large numbers of patients.  

Both research and clinical applications of sensing systems require some guarantee that the hardware and software will remain available and supported for the duration of the intended use.  Unfortunately, there are currently high levels of churn among the companies that support these systems given the rapid innovation occurring at this time. We collected data for approximately 2.5 years between 2017 -- 2019.  During this time, Apple bought the company that developed the Beddit Sleep Monitor and discontinued support for previous users.  Apple re-introduced the sleep sensing system for iPhone users in late 2018, but discontinued it again in early 2022.  For these reasons, we were able to collect sleep sensing data on less than half of our research participants.   

During this same data collection period, there was also churn in the software that we used for sensing geolocation.  We used the Moves app at the start of the study, but needed to switch to use FollowMee when Facebook acquired the company that developed Moves and discontinued its support.  However, this software churn was less disruptive because both apps relied on smartphone sensors to acquire the raw geolocation data stream.  This suggests yet another reason to prefer systems that make use of generic smartphone sensors rather than propriety hardware. 

Of course, even if these sensing systems are affordable, accessible, and stable, the use of personal sensing will only increase if the measures derived from the raw data streams prove useful.  The use of ecological momentary assessments to sense mood, psychiatric symptoms, and other key mental health constructs has been well-established [@ebner-priemerEcologicalMomentaryAssessment2009; @shiffmanEcologicalMomentaryAssessment2009; @shiffmanEcologicalMomentaryAssessment2008; @sedano-capdevilaUseEcologicalMomentary2021] and in situ, momentary measurement of these constructs is difficult at best with other more traditional (self-report) approaches.  However, the validity and utility of more innovative passive sensing approaches (e.g., location and cellular communications)   has yet to be confirmed.  Proof of concept studies are rapidly accruing but larger N, rigorous studies are needed.


## Conclusions

Possible.  Acceptable to many.  Smart Digital Therapeutics 


```Cut from intro: The ability to predict the presence or absence of these constructs prospectively can also open the door for more efficient monitoring. Current treatment approaches lack the necessary resources required for long-term monitoring of symptom onset and overall mental health stability. Yet detecting a symptom lapse early on could have benefits for both an patient's prognosis [@mcglashanEarlyDetectionIntervention1996; @scottCanWePredict1992] and overall cost [@kesslerGlobalBurdenMental2009; @wangTelephoneScreeningOutreach2007]. Additionally, mental health disorders are not static states [@nelsonMovingStaticDynamic2017]. Someone might be stable for months or years before re-experiencing symptoms [@hayesChangeNotAlways2007; @witkiewitzModelingComplexityPosttreatment2007]. Integrating personal sensing methods into clinical settings may be one way to improve the reach of treatment resources and may allow for intervention prior to a full symptom relapse.```






# Remaining unused sections from Kendra. Consider including  ideas/text in main text above



We determined it would not be appropriate to group methods by where they fall on the high-low sensitivity continuum. Sensitivity cannot be defined by the personal sensing method alone. It is highly contextual depending on factors such as characteristics and preferences of the person providing their data, who is receiving the data (e.g., researcher, clinician, health insurance), and what it is being used for [@ackermanPrivacyIssuesHumanComputer2008]. It is also dynamic so that a certain set of circumstances may be acceptable for sharing sensitive information at one point (e.g., when someone is stable in their recovery), but not at a later point (e.g., when someone is in the middle of a series of lapses or a relapse).




### Benefits

```@bessenyeiComfortabilityPassiveCollection2021 - "Those who received mental health treatment liked the health and the communication features more than those who didn't. They were also more willing to share their data with their doctor than those who didn't receive treatment. Finally, they indicated higher intentions to accept app permissions and to use mobile apps for health, perceived more benefit and control and had more privacy related experience than the non-treated group."```




### Trust

```Situation matters - Ben-Zeev et al., 2016 found that 1/3rd of inpatient participants were concerned about privacy but no outpatients were.; additionally all 9 outpatients approached consented to be in study but 9 out of 20 approached inpatients declined due to concerns about tracking technology.```


## Ethics



Our participants generally felt comfortable providing these data to us. We had the strictest possible measures in place to keep their data safe and as a result they trusted us. One of these safeguards, a certificate of confidentiality, protects research participants by preventing forced disclosure of any identifying information. However, this is unique to the research context. 

Implementing personal sensing platforms in applied settings could potentially put the end user at risk [@oneilWeaponsMathDestruction2016]. Companies and third-party applications could be subpoenaed and required to disclose sensitive information to be used as ammunition in a custody case or as evidence in a criminal trial. Likewise, a company might be providing a patient's healthcare data to both their provider and their health insurance company resulting in higher insurance premiums or dropped coverage. This risk is not lost on the people being asked to provide their data and control over who can access their data and unauthorized secondary use of that information are often cited concerns [@ackermanPrivacyIssuesHumanComputer2008; @atienzaConsumerAttitudesPerceptions2015].



## Feasibility

Acceptability of personal sensing methods is only the first step in using such methods in applied settings. It must also be feasible to implement a personal sensing platform or series of personal sensing methods. In other words, it must be possible to do in a convenient, realistic, and sustainable way. Feasibility of applying personal sensing methods can be broken down into feasibility for the participant, client, or patient and feasibility for the researcher, provider, or clinician.




### Researcher, provider, or clinician

```We might also want to mention difficulty getting access to data (restricted to Androids - Bai et al., 2021; Lind et al., 2018; app not running in background 24x7 -  Bai et al., 2021; GPS tracking not always working accurately and consistently - Palmius et al., 2016)```

```Mendes et al., 2021 review - "A related issue is the predominance of solutions developed for Android OS, for which all apps have a version. This is expected as Android provides an open development platform, different from iOS, with significantly more flexibility to gather the data of interest. The divergent approaches to sensing on iOS and Android yield further issues in terms of standardization and the collection of comparable results across large cohorts, invariably with both Android and iOS users."```

```Trifan et al., 2019 review - "In the spectrum of smartphone technologies, 1 of the main challenges that can affect the health-related collection of data when developing monitoring systems is the choice of the operating system. In fact, there are some differences and difficulties in development for Android or IOS systems, the 2 most used phone operating systems worldwide. Android is currently the most popular system and has the advantage of being convenient from the programming point of view. Scanning rates of sensors are found to be superior with this operating system. Furthermore, IOS hampers third-party apps to run endlessly in background, which may make the data collection difficult. Of the selected papers, 56.7% (67/118) developed their system only for Android smartphones, 6 developed for both Android and IOS, and 45 did not provide any information about the chosen operating system."```







## Limitations and Future Directions

Future studies should use longer study durations (i.e., 1 year) and recruit nationally representative samples.

Our study also only generalizes to our specific personal sensing methods. Although we do not expect general self-reported acceptability about the types of raw data streams to change, different methods may increase or reduce user burden resulting in shifts of behavioral acceptability (i.e., compliance and retention). For example, participants were more willing to use passive personal sensing measures for 1 year compared to active measures. In clinical applications we may expect individuals to be engaging with these methods for years. Thus, it is important to test newer and more passive personal sensing methods as they become available.

Finally, acceptability is not limited to perceived costs but is an exercise of weighing costs, benefits, and trust. Future research should directly measure benefits (e.g., with self-report) and manipulate how the data is benefiting the participant (e.g., incorporating it into treatment). Trust should also be examined by applying this research to other contexts (e.g., who is receiving the data, what level of relationship exists between participant and data collector). It is critical we understand the entire framework of personal sensing acceptability if we are to implement these methods in real treatment settings.

## Conclusion



## Cuts from intro that might get included


```Two key aspects of personal sensing methods that may affect their acceptability are the sensitivity of the data being collected [@pasipanodyaPerceivedRisksAmelioration2020; @rendinaPrivacyTrustData2018], and the active effort required by the individual [@atienzaConsumerAttitudesPerceptions2015; @eiseleEffectsSamplingFrequency2020]. Importantly, these two dimensions explicitly differ from one personal sensing measure to the next. For example, text message content monitoring is completely passive, requiring no active effort from the participant, but the data are highly sensitive. On the other hand, an EMA method may ask generic questions that are not perceived to be sensitive, but the high level of effort required (e.g., lengthy questionnaire, high prompt frequency) may be too burdensome. As a result, acceptability of personal sensing methods may be best assessed at the individual level of the measure being sensed.```



```However, missing data from non-compliance is not the only way bias can be introduced into the data. Data provided irrespective of burden could result in hurried or careless response patterns likely due to study fatigue [@eiseleEffectsSamplingFrequency2020; @meadeIdentifyingCarelessResponses2012]. For example, when reimbursement is structured around compliance rates, participants may answer questions from memory or strategically answer items to avoid follow-up questions and shorten the length of the survey [@freedmanCellPhonesEcological2006].```

\newpage

# References
<div id="refs"></div>

\newpage
<!--We need to generate figure 1 by code. Also, I can't get text references to work. This hacky solution works as long as we don't need special formating in the figure caption. See: http://frederikaust.com/papaja_man/tips-and-tricks.html#text-references-->

```{r}
fig_caption_1 <- "Flowchart of participant retention over the course of the 3-month study. This figure displays retention and attrition of all eligible participants at various stages from consent though study completion.  It also display the reasons for attrition categorized as due to acceptability, other reasons, or unknown.  We present additional detail on reasons for attrition in Table 3."
```

```{r figure-1, fig.cap = fig_caption_1, out.extra = "", fig.pos="h", warning = FALSE}
include_graphics(here("burden/manuscript/figs/fig_disposition.jpg"), dpi = 150)
```

\newpage

```{r}
fig_caption_2 <- "EMA Compliance (1x Daily) over Time.\\linebreak Notes:  Mean compliance for each week on study. Overall mean compliance is depicted by the dashed line. N = 154."
```

```{r figure-2, fig.height = 3.5, fig.cap = fig_caption_2, out.extra = "", fig.pos="h"}
# function to map over
get_study_days <- function(the_subid, dates) {
  start_study <- dates %>% filter(subid == the_subid) %>% pull(start_study)
  end_study <- dates %>% filter(subid == the_subid) %>% pull(end_study)
  study_days <- tibble(subid = the_subid, study_day = seq(start_study, end_study - days(1), by = "day")) 
  return(study_days)
}

subids <- sample_fu1$subid
dates <- sample_fu1 %>% 
  select(subid, start_study, end_study)

study_dates <- subids %>% 
  map_dfr(~get_study_days(.x, dates))

ema <- ema_m %>% 
  select(subid, start_date) %>% 
  full_join(ema_l %>% select(subid, start_date), by = c("subid", "start_date")) %>% 
  mutate(start_date = date(start_date),
         subid = as.numeric(subid)) %>% 
  filter(subid %in% sample_fu1$subid)

ema_count <- ema %>%  
  count(subid, start_date) %>%
  mutate(n = if_else(n > 4, 4, as.numeric(n)))

ema_study_dates <- study_dates %>% 
  left_join(ema_count, by = c("subid", "study_day" = "start_date")) %>% 
  mutate(n = if_else(is.na(n), 0, n)) %>% 
  mutate(n_prompts = 4)

ema_study_weeks <- ema_study_dates %>% 
  group_by(subid) %>% 
  slice(1:7) %>% 
  mutate(week = 1) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(8:14) %>% 
    mutate(week = 2)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(15:21) %>% 
    mutate(week = 3)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(22:28) %>% 
    mutate(week = 4)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(29:35) %>% 
    mutate(week = 5)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(36:42) %>% 
    mutate(week = 6)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(43:49) %>% 
    mutate(week = 7)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(50:56) %>% 
    mutate(week = 8)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(57:63) %>% 
    mutate(week = 9)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(64:70) %>% 
    mutate(week = 10)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(71:77) %>% 
    mutate(week = 11)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(78:84) %>% 
    mutate(week = 12)) %>% 
  ungroup()

ema_1_week_compliance <- ema_study_weeks %>%
  mutate(n = if_else(n > 1, 1, n),
         n_prompts = 1) %>% 
  group_by(subid, week) %>% 
  summarize(sum_n = sum(n), sum_prompts = sum(n_prompts), .groups = "rowwise") %>% 
  mutate(compliance = sum_n/sum_prompts) %>% 
  ungroup()

ema_1_week_compliance %>% 
  group_by(week) %>% 
  summarize(mean_compliance = mean(compliance)) %>% 
  ggplot(aes(x = week, y = mean_compliance)) +
  geom_point() +
  geom_line() +
  theme_classic() +
  scale_x_continuous(name = "Week", 
                     breaks = seq(1, 12, 1)) +
  scale_y_continuous(name = "Compliance", 
                     breaks = seq(0, 1, .1), 
                     limits = c(0, 1)) +
  theme(legend.position = "none",
        text = element_text(size = 12)) +
  geom_hline(aes(yintercept = mean(mean_compliance)), linetype = "dashed", size = .3)
```


\newpage

```{r}
fig_caption_3 <- "EMA Compliance (4x Daily) over Time.\\linebreak Notes:  Mean compliance for each week on study. Overall mean compliance is depicted by the dashed line. N = 154."
```

```{r figure-3, fig.height = 3.5, fig.cap = fig_caption_3, out.extra = "", fig.pos="h"}
ema_4_week_compliance <- ema_study_weeks %>% 
  group_by(subid, week) %>% 
  summarize(sum_n = sum(n), sum_prompts = sum(n_prompts), .groups = "rowwise") %>% 
  mutate(compliance = sum_n/sum_prompts) %>% 
  ungroup()

ema_4_week_compliance %>% 
  group_by(week) %>% 
  summarize(mean_compliance = mean(compliance)) %>% 
  ggplot(aes(x = week, y = mean_compliance)) +
  geom_point() +
  geom_line() +
  theme_classic() +
  scale_x_continuous(name = "Week", 
                     breaks = seq(1, 12, 1)) +
  scale_y_continuous(name = "Compliance", 
                     breaks = seq(0, 1, .1), 
                     limits = c(0, 1)) +
 theme(legend.position = "none",
        text = element_text(size = 12)) +
  geom_hline(aes(yintercept = mean(mean_compliance)), linetype = "dashed", size = .3)
```

\newpage

```{r}
fig_caption_4 <- "Audio Check-in Compliance over Time.\\linebreak Notes:  Mean compliance for each week on study. Overall mean compliance is depicted by the dashed line. N = 154."
```

```{r figure-4, fig.height = 3.5, fig.cap = fig_caption_4, out.extra = "", fig.pos="h"}
audio <- audio %>% 
  filter(subid %in% sample_fu1$subid)

audio_count <- audio %>%  
  count(subid, date) %>%
  mutate(n = if_else(n > 1, 1, as.numeric(n)))

audio_study_dates <- study_dates %>% 
  left_join(audio_count, by = c("subid", "study_day" = "date")) %>% 
  mutate(n = if_else(is.na(n), 0, n)) %>% 
  mutate(n_prompts = 1)

mean_audio <- audio_study_dates %>% 
  summarize(n_total = sum(n), prompt_total = sum(n_prompts)) %>% 
  mutate(mean = n_total/prompt_total)

audio_study_weeks <- audio_study_dates %>% 
  group_by(subid) %>% 
  slice(1:7) %>% 
  mutate(week = 1) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(8:14) %>% 
    mutate(week = 2)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(15:21) %>% 
    mutate(week = 3)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(22:28) %>% 
    mutate(week = 4)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(29:35) %>% 
    mutate(week = 5)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(36:42) %>% 
    mutate(week = 6)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(43:49) %>% 
    mutate(week = 7)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(50:56) %>% 
    mutate(week = 8)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(57:63) %>% 
    mutate(week = 9)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(64:70) %>% 
    mutate(week = 10)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(71:77) %>% 
    mutate(week = 11)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(78:84) %>% 
    mutate(week = 12)) %>% 
  ungroup()

audio_week_compliance <- audio_study_weeks %>% 
  group_by(subid, week) %>% 
  summarize(sum_n = sum(n), sum_prompts = sum(n_prompts), .groups = "rowwise") %>% 
  mutate(compliance = sum_n/sum_prompts) %>% 
  ungroup()

audio_week_compliance %>% 
  group_by(week) %>% 
  summarize(mean_compliance = mean(compliance)) %>% 
  ggplot(aes(x = week, y = mean_compliance)) +
  geom_point() +
  geom_line() +
  theme_classic() +
  scale_x_continuous(name = "Week", 
                     breaks = seq(1, 12, 1)) +
  scale_y_continuous(name = "Compliance", 
                     breaks = seq(0, 1, .1), 
                     limits = c(0, 1)) +
  theme(legend.position = "none",
        text = element_text(size = 12)) +
  geom_hline(aes(yintercept = mean(mean_compliance)), linetype = "dashed", size = .4)
```

\newpage

```{r}
fig_caption_5 <- "Interference Ratings by Personal Sensing Data Stream.\\linebreak Notes:  Mean responses to [Personal sensing method name] interfered with my daily activities. X-axes are ordered to display higher acceptability on the right side.  N = 154 for all data streams except sleep  quality (N = 87). Solid red line represents the mean and dashed black line represents the neutral midpoint (undecided). All raw data streams had a mean significantly higher than the neutral midpoint.  Interference ratings were only collected for active methods."

```


```{r figure-5, fig.height = 3.5, fig.cap = fig_caption_5, out.extra = "", fig.pos="h"}
interference_plot_data <- data_last %>% 
  select(contains("interfere")) %>%   
  pivot_longer(everything(), "measure", values_drop_na = TRUE) %>% 
  mutate(measure = factor(measure, 
                          levels = c("audio_checkin_interfere", "daily_survey_interfere", 
                                     "sleep_interfere"),
                          labels = c("Audio Check-in", "EMA", "Sleep Quality"))) %>% 
  mutate(value = factor(value, levels = c(-2:2), labels = c("Strongly agree", "Agree", "Undecided", "Disagree", "Strongly disagree"))) %>% 
  mutate(active = case_when(measure == "Audio Check-in" ~ "Active",
                            measure == "EMA" ~ "Active",
                            measure == "Sleep Quality" ~ "Active")) 

interference_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#FFDEDE") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = "Proportion",
       x = NULL) +
  theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575") +
  geom_vline(aes(xintercept = means), interference_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#b44343") 
```


\newpage
```{r}
fig_caption_6 <- "Dislike Ratings by Personal Sensing Data Stream.\\linebreak  Notes:  Mean responses to I disliked [Personal sensing method name]. X-axes are ordered to display higher acceptability on the right side.  N = 154 for all data streams except sleep quality (N = 87). Solid red or blue line represents the mean and dashed line represents the neutral midpoint (undecided). All raw data streams had a mean significantly higher than the neutral midpoint.  Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-6, fig.height = 6, fig.cap = fig_caption_6, out.extra = "", fig.pos="h"}
dislike_plot_data <- data_last %>% 
  select(contains("dislike")) %>%  
  pivot_longer(everything(), "measure", values_drop_na = TRUE) %>% 
   mutate(measure = factor(measure, 
                          levels = c("audio_checkin_dislike", "daily_survey_dislike", "sleep_dislike",
                                     "location_dislike", "all_logs_dislike", "sms_content_dislike"),
                          labels = c("Audio Check-in", "EMA", "Sleep Quality",
                                     "Geolocation", "Cellular Communication Logs", "Text Message Content"))) %>% 
  mutate(value = factor(value, levels = c(-2:2), labels = c("Strongly agree", "Agree", "Undecided", "Disagree", "Strongly disagree"))) %>% 
  mutate(active = case_when(measure == "Audio Check-in" ~ "Active",
                            measure == "EMA" ~ "Active",
                            measure == "Sleep Quality" ~ "Active",
                            measure == "Geolocation" ~ "Passive",
                            measure == "Cellular Communication Logs" ~ "Passive",
                            measure == "Text Message Content" ~ "Passive")) 

active_dis <- dislike_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#FFDEDE") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
  theme(legend.position = "none",
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        text = element_text(size = 12)) +
  ylim(0, .65) +
  geom_vline(aes(xintercept = means), dislike_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#b44343") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575") +
  theme(text = element_text(size = 12))


passive_dis <- dislike_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#DBF8FF") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = means), dislike_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>%  
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#05667b") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575") 

dislike_plot <- wrap_plots(active_dis, passive_dis, ncol = 1)

dislike_plot  %>% 
  add_global_label(Ylab = "                   Proportion",
                   Ygap = .02
)
```


\newpage
```{r}
fig_caption_7 <- "Average Dislike by Active vs. Passive Methods.\\linebreak  Notes:  X-axes are ordered to display higher acceptability on the right side.  Active methods (displayed in red) represent an average of audio check-in and EMA.  Passive methods (displayed in blue) represent an average of geolocation, cellular communication logs and text message content.  Solid red or blue line represents the mean and dashed line represents the neutral midpoint (undecided).  Participants did not different significantly in their dislike of active vs. passive methods. N= 154. Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-7, fig.width = 5, fig.cap = fig_caption_7, out.extra = "", fig.pos="h"}
# transform dataframe of active/passive means to long
data_dislike_long <- data_dislike %>% 
  pivot_longer(cols = c(Active, Passive), names_to = "effort", values_to = "dislike") %>% 
  select(-dislike_diff)

data_dislike_long %>% 
  # bin continuous means, keep numeric
  mutate(dislike_binned = as.numeric(cut(dislike, breaks = c(-2.5, -1.5, -0.5, 0.5, 1.5, 2.5)))) %>% 
  # change 1 to 5 scale to -2 to 2
  mutate(dislike_binned = dislike_binned - 3) %>% 
  ggplot(aes(x = dislike_binned, y = ..prop.., group = effort, fill = effort)) +
  geom_bar(color = "black") +
  facet_wrap(~ effort, ncol = 1) +
  theme_classic() +
  labs(y = "Proportion",
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2),
                   labels = c("Strongly agree", "Agree", "Undecided", "Disagree", "Strongly disagree")) +
  scale_fill_manual(values = c("#FFDEDE", "#DBF8FF")) +
  geom_vline(aes(xintercept = m), data_dislike_long %>% 
               group_by(effort) %>% 
               summarise(m = mean(dislike)), size = .705, color = c("#b44343", "#05667b")) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", size = .4, color = "#787575")
```


\newpage

```{r}
fig_caption_8 <- "Willingness to Use for 1 Year Ratings by Personal Sensing Data Stream.\\linebreak  Notes:  Mean responses to I would be willing to use [Personal sensing method name] for 1 year to help with my recovery. X-axes are ordered to display higher acceptability on the right side.  N = 154 for all data streams except sleep monitoring (N = 87). Solid blue or red line represents the mean and dashed line represents the neutral midpoint (undecided). All raw data streams had a mean significantly higher than the neutral midpoint.  Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-8, fig.height = 6, fig.cap = fig_caption_8, out.extra = "", fig.pos="h"}
willingness_plot_data <- data_last %>% 
  select(contains("1year")) %>%  
  pivot_longer(everything(), "measure", values_drop_na = TRUE) %>% 
  mutate(measure = factor(measure, 
                          levels = c("audio_checkin_1year", "daily_survey_4_1year", 
                                     "daily_survey_1_1year", "sleep_1year", "location_1year", 
                                     "all_logs_1year", "sms_content_1year"),
                          labels = c("Audio Check-in", "EMA", "Daily Survey (x1)<i><sup>a</sup></i>",
                                     "Sleep Quality", "Geolocation", "Cellular Communication Logs", 
                                     "Text Message Content"))) %>% 
  mutate(value = factor(value, levels = c(-2:2), labels = c("Strongly disagree", "Disagree", "Undecided", "Agree", "Strongly agree"))) %>% 
  mutate(active = case_when(measure == "Audio Check-in" ~ "Active",
                            measure == "EMA" ~ "Active",
                            measure == "Sleep Quality" ~ "Active",
                            measure == "Geolocation" ~ "Passive",
                            measure == "Cellular Communication Logs" ~ "Passive",
                            measure == "Text Message Content" ~ "Passive")) 

willingness_active <- willingness_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#FFDEDE") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
  theme(legend.position = "none",
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        text = element_text(size = 12),
        strip.text = element_markdown()) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = means), willingness_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#b44343") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575")


willingness_passive <- willingness_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#DBF8FF") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = means), willingness_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>%  
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#05667b") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575")

willingness_final_plot <- wrap_plots(willingness_active, willingness_passive, ncol = 1)

willingness_final_plot  %>% 
  add_global_label(Ylab = "                   Proportion",
                   Ygap = .02
)
```

\newpage

```{r}
fig_caption_9 <- "Average Willingness to Continue for 1 Year by Active vs. Passive Methods.\\linebreak  Notes:  X-axes are ordered to display higher acceptability on the right side.  Active methods (displayed in red) represent an average of audio check-in and EMA.  Passive methods (displayed in blue) represent an average of geolocation, cellular communication logs and text message content.  Solid red or blue line represents the mean and dashed line represents the neutral midpoint (undecided). Participants reported on average significantly higher acceptability with respect to willingness to continue using for 1 year for passive compared to active methods. N= 154. Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-9, fig.width = 5, fig.cap = fig_caption_9, out.extra = "", fig.pos="h"}
# transform dataframe of active/passive means to long
data_willingness_long <- data_willingness %>% 
  pivot_longer(cols = c(Active, Passive), names_to = "effort", values_to = "willingness") %>% 
  select(-willingness_diff)

data_willingness_long %>% 
  # bin continuous means, keep numeric
  mutate(willingness_binned = as.numeric(cut(willingness, breaks = c(-2.5, -1.5, -0.5, 0.5, 1.5, 2.5)))) %>% 
  # change 1 to 5 scale to -2 to 2
  mutate(willingness_binned = willingness_binned - 3) %>% 
  ggplot(aes(x = willingness_binned, y = ..prop.., group = effort, fill = effort)) +
  geom_bar(color = "black") +
  facet_wrap(~ effort, ncol = 1) +
  theme_classic() +
  labs(y = "Proportion",
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2),
                   labels = c("Strongly disagree", "Disgree", "Undecided", "Agree", "Strongly agree")) +
  scale_fill_manual(values = c("#FFDEDE", "#DBF8FF")) +
  geom_vline(aes(xintercept = m), data_willingness_long %>% 
               group_by(effort) %>% 
               summarise(m = mean(willingness)), size = .705, color = c("#b44343", "#05667b")) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", size = .4, color = "#787575")
```
