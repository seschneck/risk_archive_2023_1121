---
title: "Acceptability of Personal Sensing among People with Alcohol Use Disorder"
shorttitle        : "Personal Sensing"

author: 
  - name          : "Kendra Wyant"
    affiliation   : "1"
  - name          : "Hannah Moshontz"
    affiliation   : "1"
  - name          : "Stephanie B. Ward"
    affiliation   : "1"
  - name          : "Gaylen E. Fronk"
    affiliation   : "1"
  - name          : "John J. Curtin"
    affiliation   : "1"
    corresponding : yes 
    address       : "1202 West Johnson St, Madison, WI 53706"
    email         : "jjcurtin@wisc.edu"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, University of Wisconsin - Madison"


authornote: |
  This research was supported by the National Institute of Alcohol Abuse and Alcoholism (NIAAA) of the National Institutes of Health (NIH) under award number R01 AA024391 (J Curtin).
  
  <!--Thank research staff and provide link to credit reporting system-->

abstract: |
  **Background:** Personal sensing may improve digital applications targeting patient mental health care in areas such as early screening, long-term symptom monitoring, and personalized and adaptive treatment suggestions. However, further development and use of personal sensing necessitates better understanding of its acceptability to people targeted for these mental health applications.
  **Objective:** We assessed the acceptability of both active and passive personal sensing methods in a sample of people with moderate to severe alcohol use disorder.
  **Method:** Participants (N = 154; 50% female; mean age = 41; 87% White, 97.4% Non-Hispanic) in early recovery (1 – 8 weeks of abstinence) from alcohol use disorder were recruited from the Madison, WI area to participate in a 3-month longitudinal study. Participants engaged with active (ecological momentary assessment; EMA, audio check-in, and sleep quality) and passive (geolocation, cellular communication logs, and text message content) personal sensing methods for up to 3 months. We assessed 3 behavioral indicators of acceptability: participants’ choices about their participation in the study at various stages in the study procedure (e.g., consent, enrollment), their choice to opt-in to provide data for each personal sensing method, and their compliance for a subset of the active methods (EMA, audio check-in). We also assessed 3 subjective self-report measures of acceptability (interference, dislike, and willingness to use for 1 year) for each method.
  **Results:** All but 1 of the individuals who were eligible to participate consented to the personal sensing procedures. Most of these individuals (88%) also returned 1 week later to formally enroll in the study and begin to provide these data. All participants (100%) opted-in to provide data for EMA, sleep quality, and all passive methods (geolocation, cellular communication logs, text message content). Three participants (2%) did not provide any audio check-ins while on study. The average completion rate for all requested EMAs (4X daily) was 81%. The completion rate for 1 daily EMA was 94%. The completion rate for the daily audio check-in was 55%. Aggregate participant ratings indicated all personal sensing methods to be significantly more acceptable (p < .05) compared to neutral across subjective measures of interference, dislike, and willingness to use for 1 year. Participants did not significantly differ in their dislike of active compared to passive methods (p = .23). However, participants reported a higher willingness to use passive methods for 1 year compared to active methods (p = .04).
  **Conclusions:** The results of our study suggest personal sensing methods are acceptable 1) over a longer period than has previously been assessed, 2) across active and passive methods, 3) despite the sensitivity of the data, 4) among individuals with alcohol use disorder who may have greater stigma or privacy concerns, and 5) without explicit clinical benefits to the participants. 


  
keywords          : "keywords"
wordcount         : "X"

floatsintext      : yes
figurelist        : yes
tablelist         : no
footnotelist      : yes
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
header-includes:
  - \raggedbottom
  - \usepackage{booktabs}
  - \definecolor{lightred}{RGB}{255,222,222}
  - \definecolor{lightblue}{RGB}{219,248,255}
  - \usepackage[justification=raggedright]{caption}
  - \usepackage{colortbl}


  
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk/manuscripts/burden", 
      "/Volumes/private/studydata/risk/manuscripts/burden")
    )
  })
bibliography: burden.bib
csl: journal-of-medical-internet-research.csl
---

```{r setup, include = FALSE}
library(here)
library(papaja)
library(knitr)
library(tidyverse)
library(kableExtra)
library(janitor)
library(corx)
library(patchwork)
library(ggtext)
library(vroom)
library(lubridate)

knitr::opts_chunk$set(echo = FALSE)
options(knitr.kable.NA = '')
```

```{r absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_burden <- "P:/studydata/risk/data_processed/burden"
          path_shared <- "P:/studydata/risk/data_processed/shared"},
        # IOS paths
        Darwin = {
          path_burden <- "/Volumes/private/studydata/risk/data_processed/burden"
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"}
       )
```

```{r relative paths and source}
path_ana <- "burden/ana_scripts"

source(here(path_ana, "fun_burden.R"))
```


```{r burden data}
data <- vroom::vroom(here(path_burden, "acceptability.csv"), col_types = vroom::cols())

# pull out last observation for each participant
# Last available sleep monitor may be earlier than last survey date for some due 
# to discontinuation of monitor - handle separately
data_sleep <- data %>% 
  filter(!is.na(sleep_interfere)) %>% 
  group_by(subid) %>% 
  arrange(desc(date)) %>% 
  slice(1) %>% 
  ungroup() %>% 
  select(c(subid, starts_with("sleep_")))

data_last <- data %>% 
  select(-c(starts_with("sleep_"))) %>% 
  group_by(subid) %>% 
  arrange(desc(date)) %>% 
  slice(1) %>% 
  ungroup() %>% 
  full_join(data_sleep, by = c("subid")) %>% 
  select(-c(contains("wristband"), contains("carrying_phone"))) 
```

```{r screen data}
screen <- vroom::vroom(here(path_shared, "screen.csv"), col_types = vroom::cols())

# include only participants used in analyses (n = 154) - use visit dates to determine
sample_fu1 <- vroom::vroom(here(path_shared, "visit_dates.csv"), col_types = vroom::cols()) %>% 
  filter(!(is.na(followup_1)))

screen <- screen %>% 
  filter(subid %in% sample_fu1$subid)
```

```{r disc data}
# read in notes on incomplete participants
notes_incomplete <- read_csv(file.path(path_burden, "notes_discontinue.csv"), col_types = cols()) %>% 
  filter(screen != "cancelled" & screen != "no show")
```

```{r ema and audio data}
ema_m <- vroom(file.path(path_shared, "ema_morning.csv"), col_types = vroom::cols()) %>% 
  mutate(start_date = with_tz(start_date, tzone = "America/Chicago"),
         subid = as.numeric(subid))
ema_l <- vroom(file.path(path_shared, "ema_later.csv"), col_types = vroom::cols()) %>% 
  mutate(start_date = with_tz(start_date, tzone = "America/Chicago"),
         subid = as.numeric(subid))
audio <- vroom(file.path(path_shared, "audio.csv"), col_types = vroom::cols()) %>% 
  mutate(subid = as.numeric(subid))
```

```{r fu12 and disposition data for calculating past month drug use}
fu_12 <- vroom(here(path_shared, "followup_12.csv"), col_types = vroom::cols())
disp <- vroom(here(path_burden, "disposition.csv"), col_types = vroom::cols())
```



## Personal Sensing

The World Health Organization's Global Observatory for eHealth has concluded that "the use of mobile and wireless technologies to support the achievement of health objectives has the potential to transform the face of health service delivery across the globe" [@whoglobalobservatoryforehealthMHealthNewHorizons2011]. This conclusion applies to research and care for mental health as well as other traditional health services. These opportunities are now possible in part because of rapid advances in smartphone and related mobile technologies [@majumderSmartphoneSensorsHealth2019] and high levels of smartphone access across race, socioeconomic status, geographic region, and other demographic characteristics [@pewresearchcenterMobileFactSheet2021].

Personal sensing may become an important component of these digital health advances [@healthWhatDigitalHealth2020]. Personal sensing is a method for longitudinal measurement in situ; i.e., real-world measurement that is embedded in individuals' day to day lives [@mohrPersonalSensingUnderstanding2017;  @klasnjaExploringPrivacyConcerns2009;  @huckvaleClinicalDigitalPhenotyping2019]. Raw data streams are collected by smartphones, wearable sensors, or other smart devices. These raw data streams can consist of self-reports or more novel data streams such as geolocations, cellular communications, social media activity, or physiology. Subsequent processing can extract psychiatric or health relevant measures of thoughts, feelings, behavior, and even interpersonal interactions.

Ecological momentary assessment (EMA), a personal sensing method that collects brief self-reports about momentary states multiple times per day, has been used for many years in short-term longitudinal studies of psychiatric disorders. For example, EMA research on substance use disorders has identified proximal causes and risk factors for drug craving and relapse [@morgensternEcologicalMomentaryAssessment2014; @fronkStressAllostasisSubstance2020; @schultzStressorelicitedSmokingCravingInpress]. It has also characterized the time course and nature of drug withdrawal [@piaseckiSmokingWithdrawalDynamics2003a; @mccarthyLifeQuittingSmoking2006]. EMA research on major depressive and bipolar disorders has identified predictors of daily mood fluctuations, documented treatment efficacy, and been paired with other human neuroscience methods to explore biological mechanisms [@aanhetrotMoodDisordersEveryday2012]. Much of this research could not have been accomplished with other measurement methods.

More recently, mental health relevant research using personal sensing of raw data streams other than self-report is emerging. This includes methods to sense geolocation [@epsteinPredictionStressDrug2020; @palmiusDetectingBipolarDepression2016; @moshontzProspectivePredictionLapses2021], cellular communications [@jacobsonDigitalBiomarkersSocial2020; @wangExaminingCorrelationDepression2021; @razaviDepressionScreeningUsing2020; @baiTrackingMonitoringMood2021; @moshontzProspectivePredictionLapses2021], sleep [@baiTrackingMonitoringMood2021], and physiology [@kleimanUsingWearablePhysiological2019; @kleimanCanPassiveMeasurement2021], as examples. These alternative personal sensing methods provide benefits and opportunities not possible with EMA. For example, many of these data streams can be sensed passively such that they have very low assessment burden. This may allow their use for long-term longitudinal monitoring of participants that would not be feasible with EMA, which requires more active effort for data collection. 

Personal sensing is a powerful tool for mental health research[@shiffmanEcologicalMomentaryAssessment2008]. These data are inherently longitudinal, which allows observation of the temporal ordering for putative etiologic mechanisms and their effects. Longitudinal measurement is also critical for many mental health constructs that display meaningful, and often frequent, temporal variation within person (e.g., psychiatric symptoms). Measures based on personal sensing data generally have high ecological validity because they are collected in situ.  Personal sensing measures also have low retrospective bias because they are often collected in real-time. Furthermore, personal sensing can derive measures from raw data streams (e.g., in situ behavior, physiology, interpersonal interactions) that are difficult or even impossible to obtain through other traditional research measurement methods.

Personal sensing may have even higher value in the future for mental health applications that target patient mental health care than it does for research[@huckvaleClinicalDigitalPhenotyping2019; @onnelaHarnessingSmartphoneBasedDigital2016; @torousRealizingPotentialMobile2015]. Data collected by personal sensing methods may be used for preliminary screening for psychiatric disorders [@eichstaedtFacebookLanguagePredicts2018; @razaviDepressionScreeningUsing2020]. These methods can also be used to monitor psychiatric symptoms or even predict future risk for symptom recurrence, relapse, or other harmful behaviors (e.g., suicide attempts)[@barnettRelapsePredictionSchizophrenia2018; @chihPredictiveModelingAddiction2014; @epsteinPredictionStressDrug2020; @jashinskyTrackingSuicideRisk2013; @jacobsonPassiveSensingPrediction2020]. Personal sensing measures or risk indicators may be shared, with patient consent, to health care providers to allow for cost-effective, targeted allocation of limited mental health resources to patients with the greatest or most urgent need [@quanbeckIntegratingAddictionTreatment2014]. Personal sensing has the potential to support precision mental health care by adapting and timing interventions based on characteristics of the patient and the moment in time [@nahum-shaniJustinTimeAdaptiveInterventions2018; @aggarwalAdvancingArtificialIntelligence2020; @kaiserObamaGivesEast2015]. To be clear, these applications of personal sensing are currently aspirational rather than available for clinical implementation today. However, clinical research is advancing us rapidly toward these goals [@moshontzProspectivePredictionLapses2021; @chihPredictiveModelingAddiction2014; @sheikhWearableEnvironmentalSmartphoneBased2021; @baiTrackingMonitoringMood2021].

Mental health research and applications with emerging, often more passively sensed, novel data streams like geolocation and cellular communications is still nascent.  This research has predominately involved "proof-of-concept" studies that typically include only healthy controls or other convenience samples rather than people with psychiatric disorders [@razaviDepressionScreeningUsing2020; @jacobsonDigitalBiomarkersSocial2020; @wangExaminingCorrelationDepression2021].  It has also often used very small sample sizes and/or short monitoring periods [@palmiusDetectingBipolarDepression2016; @jacobsonDigitalBiomarkersSocial2020; @kleimanUsingWearablePhysiological2019; @kleimanCanPassiveMeasurement2021].  Recent reviews of this emerging literature have highlighted gaps in reporting on participant exclusions, attrition, and compliance that are necessary to assess selection biases and feasibility of these more novel personal sensing methods [@deangelDigitalHealthTools2022; @ortizAppsGapsBipolar2021; @faurholt-jepsenSmartphonebasedObjectiveMonitoring2018].


## Acceptability of Personal Sensing

Further development and use of personal sensing necessitates better understanding of its acceptability to research participants and patients targeted for mental health applications. Will individuals consent to the use of personal sensing methods?  Will they opt-in to allow for passive measurement methods?  Can they sustain the behaviors necessary for active measurement methods for longer periods of time?  Do they perceive specific personal sensing methods as burdensome or dislike them?  Answers to these questions about the acceptability of personal sensing methods are central to its feasibility for both mental health research and applications.  

The acceptability of a personal sensing method may be influenced by the degree of active effort required from the participant or patient to collect the raw data (i.e., the method's assessment burden) and other factors (e.g., the sensitivity of the data collected).  As such, acceptability may vary across different personal sensing methods and comparisons across methods within the same individuals are thus warranted. Furthermore, comprehensive assessment of both behavioral measures (e.g., compliance) and subjective perceptions of acceptability may better anticipate potential issues for recruitment, consent, compliance, and attrition when they are used for either research or clinical applications.

Much of what is known about the acceptability of personal sensing is limited to EMA.  Studies that have accessed participants' perceptions of EMA methods have generally concluded that it is acceptable to participants from both non-clinical and clinical samples[@stoneIntensiveMomentaryReporting2003; @kirkExposureAssessmentCurrent2013; @ramseyFeasibilityAcceptabilitySmartphone2016; @yangFeasibilityAcceptabilitySmartphoneBased2015; @moitraFeasibilityAcceptabilityPosthospitalization2017]. Similarly, participants display moderate or better compliance with respect to response rates even with relatively high sampling density (e.g., 6-9 daily assessments)[@eiseleEffectsSamplingFrequency2020; @stoneIntensiveMomentaryReporting2003; @wenComplianceMobileEcological2017]. However, these studies generally assessed participants' perceptions and compliance over short monitoring periods (i.e., 2-6 weeks).  Less is known about the use of EMA over longer duration monitoring periods (e.g., months) as would be necessary for clinical applications.

Assessment burden may impact the relative acceptability of EMA.  EMA assessment burden can increase as more active effort is required from participants when EMA survey density (i.e., frequency of surveys) or depth (number of items/length of surveys) increases.  However, the impact of active effort due to density and depth may not be comparable.  Increased density does not appear to robustly impact perceptions or compliance [@eiseleEffectsSamplingFrequency2020; @stoneIntensiveMomentaryReporting2003; @jonesComplianceEcologicalMomentary2019; @wrzusEcologicalMomentaryAssessment2022 but see @wenComplianceMobileEcological2017].  However, greater survey depth may decrease both perceptions of acceptability and compliance  [@eiseleEffectsSamplingFrequency2020].

The duration of the EMA monitoring period may also increase assessment burden and affect acceptability but findings have been mixed.  Some studies suggest that as duration increases beyond only a few weeks, compliance decreases [@wrzusEcologicalMomentaryAssessment2022; @yangFeasibilityAcceptabilitySmartphoneBased2015; @onoWhatAffectsCompletion2019]. Yet other studies find no changes in compliance over time [@wenComplianceMobileEcological2017; @kirkExposureAssessmentCurrent2013]. Importantly, very few studies have looked at compliance over longer duration (e.g., > 6 weeks). One recent study asked people about their perceptions of EMA studies (without deploying the EMA protocol). They found shorter study duration to be associated with greater willingness to participate and a perception that the study would be more enjoyable. However, even in this hypothetical scenario the long duration condition was only 3 weeks [@smythInfluenceEcologicalMomentary2021].

The context of an EMA protocol likely moderates acceptability; existing research raises some concern about perceptions and compliance with EMA protocols in patients with substance use disorders relative to other groups.  Specifically, a recent meta-analysis confirmed decreased compliance with EMA protocols in patients with substance use disorder diagnoses vs. recreational substance users [@jonesComplianceEcologicalMomentary2019].  However, a separate meta-analysis by [@wrzusEcologicalMomentaryAssessment2022] showed that compliance rates did not differ between healthy, physically ill, mentally ill, and mixed samples, which suggests that compliance concerns may be limited to applications with patients with substance use disorders rather than all psychiatric disorders more generally.

Far less is known about participants' perceptions and compliance with more passive personal sensing methods. Some research has presented hypothetical scenarios to participants to assess their perceptions about personal sensing methods [@duncanAcceptabilitySmartphoneApplications2019; @riegerPsychiatryOutpatientsWillingness2019; @bessenyeiComfortabilityPassiveCollection2021].  Participants' willingness to share sensed data appears to vary by the data type (e.g., sleep, geolocation, social media activity).  However, it is difficult to determine how well participants' perceptions in these hypothetical scenarios would generalize to real world collection of these data. And, of course, it is impossible to measure attrition and compliance outside of explicit implementation of these sensing methods. 

Preliminary research has begun to examine perceptions and compliance during real world use of passive personal sensing methods. However, this research has generally been limited by small sample sizes [@lindEffortlessAssessmentRisk2018; @ben-zeevMobileBehavioralSensing2016], use of convenience samples (e.g., students) [@lindEffortlessAssessmentRisk2018;  @kirkExposureAssessmentCurrent2013; @rooksbyStudentPerspectivesDigital2019], short monitoring duration [@lindEffortlessAssessmentRisk2018; @kleimanUsingWearablePhysiological2019; @ben-zeevMobileBehavioralSensing2016; @raughDigitalPhenotypingAdherence2021], and coarse, incomplete, or aggregate reporting of perceptions, compliance, and related participant behaviors [@lindEffortlessAssessmentRisk2018; @ben-zeevMobileBehavioralSensing2016; @kirkExposureAssessmentCurrent2013]. These are important first efforts but more research into the feasibility of personal sensing methods is clearly warranted.    

## Study Goals

This study reports on the acceptability of both active and passive personal sensing methods in a sample of participants with moderate to severe alcohol use disorder.  These participants were enrolled early in their recovery (i.e., 1 -- 8 weeks after becoming abstinent) and followed for 3 months. We used active personal sensing methods to collect EMA, daily audio check-ins, sleep quality, and selected physiology. We used primarily passive methods to collect geolocation, cellular communications logs, and text message content. We assessed participants' choices about their participation in the study at various stages in the study procedure (e.g., consent, enrollment, data collection), their choice to opt-in to provide data associated with each personal sensing method, and their reasons for discontinuation when available. For active measures, we also assessed their compliance with providing those raw data streams for up to 3 months of their study participation. Finally, we assessed participants' subjective perceptions of the acceptability of each of these personal sensing methods, separately, by self-report. We believe these data provide an unparalleled and transparent window into the feasibility of using numerous personal sensing methods with individuals with alcohol use disorder, a highly stigmatized psychiatric disorder.


# Method

## Research Transparency

We value the principles of research transparency that are essential to the robustness and reproducibility of science [@schonbrodtVoluntaryCommitmentResearch2015]. Consequently, we maximized transparency through several complementary methods. First, we report how we determined our sample size, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]. Second, we completed a transparency checklist, which can be found in the supplement of this paper (Multimedia Appendix 1) [@aczelConsensusbasedTransparencyChecklist2019]. Third, we made the data, analysis scripts and annotated results, self-report surveys, and other study materials associated with this report publicly available [@zotero-22408].

## Participants

We collected the study data between 2017 -- 2019 as part of a larger grant-funded parent project (RO1 AA024391). The sample size was determined based on power analyses for the aims of that project. We used all available participants for this study. 

We recruited participants in early recovery (1 -- 8 weeks of abstinence) from alcohol use disorder in Madison, Wisconsin, USA, to participate in a 3-month longitudinal study. Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required that participants:

1.  were 18 years of age or older,
2.  were able to write and read in English,
3.  had at least moderate alcohol use disorder (\>= 4 DSM-5 symptoms ^[We measured DSM-5 symptoms with a self-report survey administered to participants during the screening visit.]),
4.  were abstinent from alcohol for at least 1 week but no longer than 2 months,
5.  were willing to use a single smartphone (their personal phone or 1 provided by us) while enrolled in the study.

We also excluded participants if they exhibited severe symptoms of psychosis or paranoia ^[Psychosis and paranoia were defined as scores greater than 2.2 or 2.8, respectively, on the psychosis or paranoia scales of the on the Symptom Checklist – 90 (SCL-90) [@derogatisSCL90OutpatientPsychiatric1973].]. All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

We assessed eligibility and exclusion criteria using a brief phone screen followed by a more detailed in person screening visit. One hundred ninety-two participants met criteria for enrollment. Of these participants, 191 consented to participate in the study at the screening session and 169 subsequently enrolled in the study at the enrollment visit which occurred approximately 1 week later. Fifteen participants discontinued prior to the first follow-up visit at 1 month. The remaining 154 participants provided study measures for 1 (N = 14), 2 (N = 7) or 3 (N = 133) months. We provide a study participation flow chart in Figure 1.

## Procedure

Participants completed 5 study visits over the course of approximately 3 months. Participants first attended a screening visit where we determined eligibility, obtained informed consent, and collected self-report measures of individual differences (e.g., demographics, alcohol use history). We scheduled eligible and consented participants to enroll in the study approximately 1 week later. During this enrollment visit, we collected additional self-report and interview measures. Participants completed 3 additional follow-up visits that occurred about every 30 days. We collected self-report and interview measures and downloaded cellular communications logs (text message and phone call) at these visits. Finally, we collected various raw data streams (e.g., geolocation, cellular communication logs, EMA) using personal sensing to monitor participants throughout the 3-month study period. A full description of the procedure and data collected at each visit can be found at the study's OSF page [@zotero-22408].

## Personal Sensing

Personal sensing methods can be coarsely classified as active or passive. Active personal sensing requires active effort from the participant to provide the raw data streams whereas passive personal sensing data are collected automatically (either asynchronously or continuously) with little to no effort required by the participant. Our study obtained several active signals that varied somewhat in the amount of effort required by the participant. Specifically, we used active methods to collect EMA, daily audio check-ins, sleep quality, and selected physiology. We used primarily passive methods to collect geolocation, cellular communications logs, and text message content. More detail about each raw data stream collected by personal sensing is provided below.

### Audio Check-in

Participants recorded a diary-style audio response on their smartphone to an open-ended prompt each day following a reminder from us that was sent via text message. They responded to the prompt ("How are you feeling about your recovery today?"), which stayed the same throughout the entire study. We instructed them that their responses should be approximately 15-30 seconds in duration. These recordings were sent to us by text message.

### EMA

Participants completed a brief (7 -- 10 questions) EMA 4 times each day following reminders from us that were sent by text message. These text messages included a link to a Qualtrics survey that was optimized for completion on their smartphone. All 4 EMAs included items that asked about any alcohol use that had not yet been reported, current affective state (pleasantness and arousal), greatest urge to drink alcohol since the last EMA, any pleasant or positive events and any hassles or stressful events that occurred since the last EMA, any exposure to risky situations (i.e., people, places, or things) since the last EMA. The first EMA each day asked an additional 3 questions about how likely participants were to encounter a risky situation, encounter a stressful event, and drink alcohol in the upcoming week. The first and last EMAs of the day were scheduled within 1 hour of participants' typical wake and sleep times. The other 2 EMAs were each scheduled randomly within the first and second halves of the participants' typical day. All EMAs were separated from each other by at least 1 hour.

### Sleep Quality

We collected information about participants' sleep duration, timing, and overall quality with a Beddit sleep monitor (Beddit Oy Inc., Espoo, Finland) that was placed in their beds and connected to their smartphones. We used an early version of the sleep monitor that required participants to actively start and stop the monitor when they entered and exited their bed each night and morning, respectively. These data are available for only 87 participants because Beddit Oy was acquired by Apple Inc. during data collection for this study. Apple discontinued cloud support for data collection with the sleep monitor in November 2018, which prevented its further use for our remaining participants.

### Physiology

We continuously monitored participants' physiology (heart rate, electrodermal activity, skin temperature) using an early version of the Empatica E4 wristband monitor (by Empatica Inc., Boston, MA). However, this early version did not adequately support Bluetooth streaming of data to the cloud. Instead, participants had to manually connect the wristband each night to a tablet we provided to upload their data. This and other software bugs made use of the wristband too complicated for many participants. Therefore, we discontinued use of the wristband after we collected data from 9 participants. Given this small sample size, we did not include the wristband in our primary analyses. We do provide self-reported acceptability ratings for this signal from this small sample in Multimedia Appendix 3 (Figure S1).

### Geolocation

We continuously collected participants' moment-by-moment geolocation using location services on their smartphones in combination with commercial software that accessed these geolocation data and saved them in the cloud. At the start of the study, we used the Moves app (developed by ProtoGeo Oy, Helsinki, Finland). However, Facebook acquired ProtoGeo Oy and shut down use of the Moves app in July 2018. At this point, we switched to using the FollowMee GPS tracking mobile app (FollowMee LLC, Murphy, TX). Measurement of geolocation required only initial installation of the app by the participants. Subsequent measurement and transfer of the data to the cloud was completed automatically with no input or effort by the participant. Both apps allowed participants to temporarily disable location sharing if they deemed it necessary for short periods of time.

### Cellular Communication Logs

We collected cellular communication logs that include meta-data about smartphone communications involving both text messages and phone calls. For each communication entry, these logs include the phone number of the other party, the type of call or message (i.e., incoming, outgoing, missed, rejected), the name of the party if listed in the phone contacts, the date and time the message or call occurred, whether the log entry was read (text messages only), and the duration of the call (voice calls only). These data are saved passively on the phone with no additional input or effort on the part of the participant. We downloaded these logs from participants' phones at each 1-month follow-up visit. Participants were informed that they could delete any text message or voice call log entries prior to the download if they desired. 

### Text Message Content 

We also collected the message content from participants' text messages on their smartphone. As with the logs, content from individual text messages is saved passively on the phone with no additional input or effort on the part of the participant. We downloaded text message content at each 1-month follow-up visit and participants could delete text messages prior to the download. Note that we did not have a parallel method to gain access to phone call content. Thus, we had meta-data from communication logs for both text messages and phone calls but had the content of the communication only for text messages.

## Measures

### Individual Differences

We collected demographic information and information relevant to participants' alcohol use and DSM-5 alcohol use disorder symptoms at the screening visit. ^[Additional variables were measured as part of the parent project aims. We share the full surveys on OSF.] 


### Behavioral Measures of Acceptability

Coarse assessment of the acceptability of the personal sensing methods can be made based on participants' behaviors. Specifically, we assessed 3 categories of behavior. First, we assessed participants' choices about their participation in the study at various stages in the study procedure (e.g., consent, enrollment, data collection) and their reasons for discontinuation when available. Second, we assessed their choice to opt-in to provide data associated with each personal sensing method. Participants were allowed to participate in the study without opting-in to any specific personal sensing method other than EMA. Instead, they were paid a monthly bonus for each sensing method they chose to opt-in to that ranged from \$10-\$25. Finally, for a subset of the active measures (EMA, audio check-in), we assessed their behavioral compliance for up to 3 months of study participation.

### Self-reported Measures of Acceptability

To assess participants' subjective experience of the acceptability of the personal sensing methods in this study, each month they rated each method on 3 acceptability relevant dimensions (see Multimedia Appendix 2). Specifically, participants were asked to indicate how much they agreed with each of the following 3 statements on a 5-point bipolar scale (strongly disagree, disagree, undecided, agree, strongly agree) for the personal sensing signals ^[Participants provided ratings of dislike and willingness separately for text message and phone call logs. However, participants’ ratings for each item were highly correlated across the 2 logs (r = 0.83 for dislike; r = 0.79 for willingness). Furthermore, permissions for API access to text messages and phone call logs are linked for both Android and iOS operating systems such that researchers and app developers get access to both if permission is granted. Therefore, we decided to combine ratings of dislike and willingness for each of these 2 logs.]:


1.  [Personal sensing method name] interfered with my daily activities.
2.  I disliked [Personal sensing method name].
3.  I would be willing to use [Personal sensing method name] for 1 year to help with my recovery.

The interference item (item 1) was collected only for the active methods because the passive methods require no effort and therefore cannot interfere with daily activities. Dislike and willingness to use for 1 year (items 2 & 3, respectively) were collected for all methods.

## Data Analytic Strategy

We conducted all analyses in R version 4.1.1 [@rcoreteamLanguageEnvironmentStatistical2021] using RStudio [@rstudioteamRStudioIntegratedDevelopment2020] and the tidyverse ecosystem of packages [@wickhamWelcomeTidyverse2019]. 

### Behavioral Measures of Acceptability

We provide descriptive data on participants' choices about their participation in the study at various stages in the study procedure (e.g., consent, enrollment, data collection). We provide both coarse and more granular tabulation of their reasons for discontinuation when available. We report the percentages of participants who opted-in to provide us with the raw data streams we collected via personal sensing. We also report compliance measures for 2 of the active personal sensing methods (EMA and audio check-in). Formal measures of compliance could not be calculated for geolocation, cellular communication logs, text message content, and sleep quality because it was not possible to distinguish between low volumes of data due to compliance (e.g., deleting phone calls or messages, turning off location services on the phone, failing to start sleep monitoring at bedtime) and valid reasons (no calls made during the day, no movement, erratic sleep patterns).

### Self-reported Measures of Acceptability

Participants responded to the 3 self-report items related to acceptability (interference, dislike, and willingness to use for 1 year) on a 5-point bipolar scale (strongly disagree, disagree, undecided, agree, strongly agree). We retained these ordinal labels for visual display of these data in figures but ordered the labels such that higher scores represent greater acceptability (i.e., strongly agree for willingness to use for 1 year and strongly disagree for interference and dislike). For analyses, we re-coded these items to a numeric scale ranging from -2 to 2 with 0 representing the neutral (undecided) midpoint and higher scores representing greater acceptability.

Participants responded to these items at each monthly follow-up visit. Therefore, participants had up to 3 responses for each item depending on when they ended their participation. We analyzed their last available response in our primary analyses to allow us to include all participants and to represent their final perception of each personal sensing signal. However, mean responses across each time point remained relatively constant for all signals (see Figure S2 in Multimedia Appendix 3).

To detect mean perceptions of the personal sensing signals that diverge from neutral (i.e., mean responses to any items that are different from 0/undecided), we conducted one sample t-tests for the 3 self-report items for each personal sensing signal. To examine relative perceptions of the signals, we compared perceptions of the active vs. passive categories of signals using within-sample t-tests for dislike and willingness to use for 1 year ^[Participants did not provide ratings of interference for passive signals so the comparisons of active vs. passive categories were limited to dislike and willingness to use for 1 year. Also, due to high proportion of missing data for the sleep monitor, we excluded this signal from these analyses and the intra-class correlations described next.]. We also report pairwise comparisons among all personal sensing signals using within-sample t-tests for each of the 3 self-report items in Table S1 in Multimedia Appendix 3.

Finally, we conducted 2 analyses to examine the consistency of perceptions across personal sensing signals (e.g., do participants who dislike 1 signal also dislike the other signals?). First, we calculated bivariate correlations among the personal sensing signals for each item. Second, we calculated intraclass correlations (single, case 3 [@shroutIntraclassCorrelationsUses1979]) separately for each item to quantify agreement in participants' perceptions across the signals.

# Results

## Participant Characteristics

A total of 154 participants completed at least 1 monthly follow-up visit and provided self-report acceptability ratings for interference, dislike, and willingness to use for 1 year. These participants serve as our primary sample for our analyses. Table 1 presents demographic information for these participants. Table 2 characterizes information relevant to alcohol use for these participants. We compared demographics and alcohol use information for participants who were included in the analyses vs. eligible participants who did not provide study measures (i.e., did not enroll or discontinued prior to the first month follow-up; N = 36 ^[We are missing demographic data for 1 participant who consented but did not subsequently enroll in the study.]) and found no significant differences (see  Table S2 in Multimedia Appendix 3 for more detail on these analyses).

\newpage

```{r table-1-code}
dem <- screen %>% 
  summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
            SD = as.character(round(sd(dem_1, na.rm = TRUE), 1))) %>% 
  mutate(var = "Age",
         n = as.numeric(""),
         perc = as.numeric("")) %>% 
  select(var, n, perc, everything()) %>% 
  full_join(screen %>% 
  select(var = dem_2) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_3) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("American Indian/Alaska Native", "Asian", "Black/African American",
                           "White/Caucasian", "Other/Multiracial")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_4) %>% 
  mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
                         TRUE ~ "Yes"),
         var = fct_relevel(factor(var, c("Yes", "No")))) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_5) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Less than high school or GED degree", "High school or GED", 
                           "Some college", "2-Year degree", "College degree", "Advanced degree")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_6, dem_6_1) %>% 
  mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
                         dem_6_1 == "Part-time" ~ "Employed part-time",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Employed full-time", "Employed part-time", "Full-time student",
                           "Homemaker", "Disabled", "Retired", "Unemployed", 
                           "Temporarily laid off, sick leave, or maternity leave",
                           "Other, not otherwise specified")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  summarise(mean = as.character(round(mean(dem_7, na.rm = TRUE), 0)),
            SD = as.character(round(sd(dem_7, na.rm = TRUE), 0))) %>% 
  mutate(var = "Personal Income",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>% 
  full_join(screen %>% 
  select(var = dem_8) %>% 
  mutate(var = case_when(var == "Never Married" ~ "Never married",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Never married", "Married", "Divorced", "Separated",
                           "Widowed")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))
```


```{r table-1}
dem %>% 
  kbl(booktabs = TRUE,
      caption = "Demographics",
      col.names = c("", "n", "%", "M", "SD"),
      align = c("l", "c", "c", "c", "c"),
      digits = 1,
      longtable = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>% 
  row_spec(row = 0, align = "c", italic = TRUE) %>% 
  pack_rows("Sex", 2, 3, bold = FALSE) %>% 
  pack_rows("Race", 4, 8, bold = FALSE) %>%
  pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) %>%
  pack_rows("Education", 11, 16, bold = FALSE) %>%
  pack_rows("Employment", 17, 25, bold = FALSE) %>%
  pack_rows("Marital Status", 27, 31, bold = FALSE) %>% 
  footnote("N = 154")
```

\newpage

```{r table-2-code}
auh <- screen %>%
  summarise(mean = mean(auh_1, na.rm = TRUE),
            SD = sd(auh_1, na.rm = TRUE)) %>%
  mutate(var = "Age of first drink",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_2, na.rm = TRUE),
            SD = sd(auh_2, na.rm = TRUE)) %>%
  mutate(var = "Age of regular drinking",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_3, na.rm = TRUE),
            SD = sd(auh_3, na.rm = TRUE)) %>%
  mutate(var = "Age at which drinking became problematic",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_4, na.rm = TRUE),
            SD = sd(auh_4, na.rm = TRUE)) %>%
  mutate(var = "Age of first quit attempt",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_5, na.rm = TRUE),
            SD = sd(auh_5, na.rm = TRUE)) %>%
  mutate(var = "Number of Quit Attempts",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  select(var = auh_6_1) %>%
  mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ mos.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_2) %>%
  mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 mos.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_3) %>%
  mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_4) %>%
  mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_5) %>%
  mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_6) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_7) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_7) %>%
  mutate(var = fct_relevel(factor(var, c("Yes", "No")))) %>%
  group_by(var) %>%
  summarise(n = n()) %>%
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) %>%
  rowwise() %>%
  # calculate dsm5 score by adding up dsm5_1 through dsm5_11
  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7,
                            dsm5_8, dsm5_9, dsm5_10, dsm5_11))) %>%
  ungroup() %>%
  summarise(mean = mean(dsm5_total),
            SD = sd(dsm5_total)) %>%
  mutate(var = "DSM-5 Alcohol Use Disorder Symptom Count",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  select(var = assist_1_1) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_2) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Cannabis (marijuana, pot, grass, hash, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_3) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Cocaine (coke, crack, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_4) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_5) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Inhalants (nitrous, glue, petrol, paint thinner, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_6) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_7) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_8) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Opioids (heroin, morphine, methadone, codeine, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc"))




# add assist past month drug use data
assist_fu_1 <- fu_12 %>% 
  filter(subid %in% screen$subid) %>% 
  group_by(subid) %>% 
  slice(1) %>% 
  select(subid,
         tobacco_fu1 = assist_2_1,
         cannabis_fu1 = assist_2_2,
         cocaine_fu1 = assist_2_3,
         amphetamine_fu1 = assist_2_4,
         inhalants_fu1 = assist_2_5,
         sedatives_fu1 = assist_2_6,
         hallucinogens_fu1 = assist_2_7,
         opioids_fu1 = assist_2_8,
         other_fu1 = assist_2_9) %>% 
  ungroup()

assist_fu_2 <- fu_12 %>% 
  filter(subid %in% screen$subid) %>% 
  filter(subid %in% subset(disp, last_visit != "followup_1")$subid) %>% 
  group_by(subid) %>%
  arrange(desc(start_date)) %>% 
  slice(1) %>% 
  select(subid,
         tobacco_fu2 = assist_2_1,
         cannabis_fu2 = assist_2_2,
         cocaine_fu2 = assist_2_3,
         amphetamine_fu2 = assist_2_4,
         inhalants_fu2 = assist_2_5,
         sedatives_fu2 = assist_2_6,
         hallucinogens_fu2 = assist_2_7,
         opioids_fu2 = assist_2_8,
         other_fu2 = assist_2_9) %>% 
  ungroup()


assist <- assist_fu_1 %>% 
  left_join(assist_fu_2, by = "subid") %>%
  mutate(across(tobacco_fu1:other_fu2, ~if_else(.x == "Never" | is.na(.x), 0, 1)),
         tobacco = if_else(tobacco_fu1 == 1 | tobacco_fu2 == 1, "Yes", "No"),
         cannabis = if_else(cannabis_fu1 == 1 | cannabis_fu2 == 1, "Yes", "No"),
         cocaine = if_else(cocaine_fu1 == 1 | cocaine_fu2 == 1, "Yes", "No"),
         amphetamine = if_else(amphetamine_fu1 == 1 | amphetamine_fu2 == 1, "Yes", "No"),
         inhalants = if_else(inhalants_fu1 == 1 | inhalants_fu2 == 1, "Yes", "No"),
         sedatives = if_else(sedatives_fu1 == 1 | sedatives_fu2 == 1, "Yes", "No"),
         hallucinogens = if_else(hallucinogens_fu1 == 1 | hallucinogens_fu2 == 1, "Yes", "No"),
         opioids = if_else(opioids_fu1 == 1 | opioids_fu2 == 1, "Yes", "No"),
         other = if_else(other_fu1 == 1 | other_fu2 == 1, "Yes", "No")) %>% 
  select(subid, tobacco:other)


auh <- auh %>% 
  full_join(assist %>%
  select(var = tobacco) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(assist %>%
  select(var = cannabis) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Cannabis (marijuana, pot, grass, hash, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(assist %>%
  select(var = cocaine) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Cocaine (coke, crack, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(assist %>%
  select(var = amphetamine) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(assist %>%
  select(var = inhalants) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Inhalants (nitrous, glue, petrol, paint thinner, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(assist %>%
  select(var = sedatives) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(assist %>%
  select(var = hallucinogens) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(assist %>%
  select(var = opioids) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Opioids (heroin, morphine, methadone, codeine, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc"))
```

```{r table-2}
auh %>% 
  kbl(booktabs = TRUE,
      caption = "Alcohol Related Characteristics for the Sample",
      col.names = c("", "n", "%", "M", "SD"),
      align = c("l", "c", "c", "c", "c"),
      digits = 1,
      longtable = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>% 
  row_spec(row = 0, align = "c", italic = TRUE) %>% 
  pack_rows("Alcohol Use Disorder Milestones", 1, 4, bold = FALSE) %>%    
  pack_rows("Types of Treatment (Can choose more than 1)", 6, 12, bold = FALSE) %>%
  pack_rows("Received Medication for Alcohol Use Disorder", 13, 14, bold = FALSE) %>%
  pack_rows("Lifetime Drug Use", 16, 23, bold = FALSE) %>%
  pack_rows("Current Drug Use$^a$", 24, 30, bold = FALSE, escape = FALSE) %>%
  footnote("N = 154", general_title = "") %>% 
  footnote(alphabet = "Current refers to past month drug use reported at follow-up visits 1 or 2", alphabet_title = "Note: ")
```

\newpage

## Behavioral Measures of Acceptability

### Participation

Figure 1 shows participant attrition/discontinuation at each phase of the study. Of the 192 eligible participants at screening, only 1 did not consent after hearing the details of the study. Enrollment occurred during a second visit 1 week later. A total of 169 participants completed enrollment.

Study phases where we lost eligible participants are highlighted in red in Figure 1. In addition, we coarsely tabulated participants stated reasons for discontinuation as due to acceptability, other reasons, or unknown in this figure. Eleven participants (5.7%) were lost due to acceptability-relevant causes (e.g., no longer interested, non-compliance with data, or citing study demands as too burdensome). Other reasons for discontinuation not related to the acceptability of the signals include circumstances such as moving or no longer wishing to abstain from alcohol. It should be noted that 31 participants (16.1%) were lost to follow-up such that we had no information about their reasons for discontinuation. We provide more granular tabulation of these reasons for discontinuation in Table 3. <!-- Denominator for above percentages is 192-->

### Opt-In and Compliance

All participants (100%) opted-in to provide data for EMA, sleep quality, and all passive personal sensing methods (geolocation, cellular communication logs, text message content). Three participants (2%) did not provide any audio check-ins while on study. 

Daily compliance rates were relatively high for EMA such that 94% of participants completed at least 1 of the 4 EMAs every day. On average, participants completed 3.2 EMAs every day. The overall compliance rate for all requested EMAs was 81%. Participants' completion rate for the audio check-in was 55% (Figure S4 in Multimedia Appendix 3 contains more information on this distribution). That is, of their total days on study, participants completed an audio check-in on approximately half of them. Figure 2 shows mean weekly compliance with each of these methods for each week on study. In Multimedia Appendix 3 we also report compliance for participants who completed the 3-month study compared to those who dropped out prior to completion (Figure S3 in Multimedia Appendix 3).



\newpage

```{r table-3-code}
notes_incomplete <- notes_incomplete %>%
  filter(screen != "no consent") %>%
  mutate(characterize = case_when(characterize == "no_transportation" ~ "No longer has transportation",
                                  characterize == "not_sober" ~ "No longer sober or no longer wishes to abstain from alcohol",
                                  characterize == "rescheduled" ~ "Rescheduled multiple times before cancelling/no showing",
                                  characterize == "treatment" ~ "No longer sober or no longer wishes to abstain from alcohol",
                                  characterize == "compliance" ~ "Noncompliance with providing data",
                                  characterize == "lapse" ~ "No longer sober or no longer wishes to abstain from alcohol",
                                  characterize == "cell_service" ~ "Cell service shut off",
                                  characterize == "move" ~ "Moved out of state",
                                  characterize == "study_demands" ~ "Cited study demands as too burdensome",
                                  characterize == "Unreachable" ~ "Unknown",
                                  characterize == "Unspecified concerns from staff" ~ "Staff concerns",
                                  characterize == "inappropriate behavior" ~ "Staff concerns",
                                  characterize == "mental health" ~ "Mental health concerns",
                                  characterize == "no aud" ~ "Does not meet criteria for moderate or severe alcohol use disorder",
                                  characterize == "phone" ~ "Ineligible phone",
                                  is.na(characterize) ~ "Unknown",
                                  TRUE ~ characterize))
```



```{r table-3-code-2}
a <- notes_incomplete %>% 
  filter(screen != "ineligible") %>% 
  filter(intake != "complete" | is.na(intake)) %>% 
  filter(intake != "ineligible" | is.na(intake)) %>% 
  group_by(characterize) %>% 
  summarise(n = n()) %>% 
  mutate(percent = n/sum(n)*100) %>% 
  adorn_totals("row") 

b <- notes_incomplete %>% 
  filter(screen != "ineligible") %>% 
  filter(completed_1month == "no") %>% 
  filter(intake == "complete") %>% 
  group_by(characterize) %>% 
  summarise(n = n()) %>% 
  mutate(percent = n/sum(n)*100) %>% 
  adorn_totals("row") 

c <- notes_incomplete %>% 
  filter(screen != "ineligible") %>% 
  filter(completed_1month == "yes") %>% 
  group_by(characterize) %>% 
  summarise(n = n()) %>% 
  mutate(percent = n/sum(n)*100) %>% 
  adorn_totals("row") 

footnote_table_3 <- "Bolded rows depict explicit acceptability-related discontinuation." 
footnote_table_3a <- "These participants are labeled as `Not Enrolled' in Figure 1." 
footnote_table_3b <- "These participants are labeled as `Discontinued' in Figure 1." 
footnote_table_3c <- "These participants are labeled as `Participated through 1st month follow-up' or `Participated through 2nd month follow-up' in Figure 1."
```


```{r table-3}
a %>% 
  bind_rows(b, c) %>% 
  kbl(booktabs = TRUE,
      col.names = c("", "n", "%"), 
      caption = "Characterization of Discontinued Participants",
      align = c("l", "c", "c"),
      digits = 1,
      longtable = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>%
  row_spec(row = 0, align = "c", italic = TRUE) %>%
  row_spec(row = 3, bold = TRUE) %>% 
  row_spec(row = 8, bold = TRUE) %>% 
  row_spec(row = 10, bold = TRUE) %>% 
  row_spec(row = 15, bold = TRUE) %>% 
  row_spec(row = 17, bold = TRUE) %>% 
  row_spec(row = 19, bold = TRUE) %>% 
  pack_rows("Eligible and consented participants discontinued prior to completing enrollment$^a$", 1, 6, bold = FALSE, escape = FALSE) %>% 
  pack_rows("Enrolled participants discontinued prior to first month follow-up$^b$", 7, 13, bold = FALSE, escape = FALSE) %>% 
  pack_rows("Enrolled participants discontinued after the first month follow-up$^c$", 14, 22, bold = FALSE, escape = FALSE) %>% 
  footnote(general=footnote_table_3, alphabet = c(footnote_table_3a, footnote_table_3b, footnote_table_3c), threeparttable = TRUE, escape = FALSE)
```

\newpage

## Self-reported Acceptability

```{r one sample t tests}
# interference
int_sleep <- broom::tidy(lm(sleep_interfere ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sleep_interfere, na.rm = TRUE),
         d = mean(data_last$sleep_interfere, na.rm = TRUE)/sd)
int_audio <- broom::tidy(lm(audio_checkin_interfere ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$audio_checkin_interfere, na.rm = TRUE),
         d = mean(data_last$audio_checkin_interfere, na.rm = TRUE)/sd)
int_ema <- broom::tidy(lm(daily_survey_interfere ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$daily_survey_interfere, na.rm = TRUE),
         d = mean(data_last$daily_survey_interfere, na.rm = TRUE)/sd)

# dislike
dis_sleep <- broom::tidy(lm(sleep_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sleep_dislike, na.rm = TRUE),
         d = mean(data_last$sleep_dislike, na.rm = TRUE)/sd)
dis_audio <- broom::tidy(lm(audio_checkin_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$audio_checkin_dislike, na.rm = TRUE),
         d = mean(data_last$audio_checkin_dislike, na.rm = TRUE)/sd)
dis_ema <- broom::tidy(lm(daily_survey_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$daily_survey_dislike, na.rm = TRUE),
         d = mean(data_last$daily_survey_dislike, na.rm = TRUE)/sd)
dis_geolocation <- broom::tidy(lm(location_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$location_dislike, na.rm = TRUE),
         d = mean(data_last$location_dislike, na.rm = TRUE)/sd)
dis_logs <- broom::tidy(lm(all_logs_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$all_logs_dislike, na.rm = TRUE),
         d = mean(data_last$all_logs_dislike, na.rm = TRUE)/sd)
dis_text_content <- broom::tidy(lm(sms_content_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sms_content_dislike, na.rm = TRUE),
         d = mean(data_last$sms_content_dislike, na.rm = TRUE)/sd)

# willingness
use_audio <- broom::tidy(lm(audio_checkin_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$audio_checkin_1year, na.rm = TRUE),
         d = mean(data_last$audio_checkin_1year, na.rm = TRUE)/sd)
use_sleep <- broom::tidy(lm(sleep_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sleep_1year, na.rm = TRUE),
         d = mean(data_last$sleep_1year, na.rm = TRUE)/sd)
use_ema <- broom::tidy(lm(daily_survey_4_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$daily_survey_4_1year, na.rm = TRUE),
         d = mean(data_last$daily_survey_4_1year, na.rm = TRUE)/sd)
use_geolocation <- broom::tidy(lm(location_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$location_1year, na.rm = TRUE),
         d = mean(data_last$location_1year, na.rm = TRUE)/sd)
use_logs <- broom::tidy(lm(all_logs_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$all_logs_1year, na.rm = TRUE),
         d = mean(data_last$all_logs_1year, na.rm = TRUE)/sd)
use_text_content <- broom::tidy(lm(sms_content_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sms_content_1year, na.rm = TRUE),
         d = mean(data_last$sms_content_1year, na.rm = TRUE)/sd)
```



```{r active effort t test}
data_dislike <- data_last %>% 
  # get subject level means for active and passive measures
  group_by(subid) %>% 
  summarise(Active = mean(c(daily_survey_dislike, audio_checkin_dislike), na.rm = TRUE),
            Passive = mean(c(location_dislike, all_logs_dislike, sms_content_dislike), na.rm = TRUE),
            dislike_diff = Passive - Active) 

model_dislike <- lm(dislike_diff ~ 1, data = data_dislike)

data_willingness <- data_last %>% 
  # get subject level means for active and passive measures
  group_by(subid) %>% 
  summarise(Active = mean(c(daily_survey_4_1year, audio_checkin_1year), na.rm = TRUE),
            Passive = mean(c(location_1year, all_logs_1year, sms_content_1year), na.rm = TRUE),
            willingness_diff = Passive - Active) 

model_willingness <- lm(willingness_diff ~ 1, data = data_willingness)
```


### Interference

Figure 3 shows the distribution of participant responses to the self-reported acceptability item about interference. Responses are grouped by personal sensing data stream and the amount of active effort required to collect it. One sample t-tests revealed that each mean interference score (depicted as the solid red line) was significantly more acceptable than 0 (gray dashed line indicating undecided). Table 4 reports the summary statistics for each one sample t-test and pairwise correlations between personal sensing data streams. An ICC (type 3) showed that, on average, interference ratings were moderately consistent across the data streams, ICC = .42, 95% CI = [.31 - .53].

### Dislike.

Figure 4 shows the distribution of participant responses to the self-reported acceptability item about dislike by personal sensing data stream and amount of active effort required to collect it. One sample t-tests revealed that each mean dislike score was significantly more acceptable than 0. Table 5 reports the summary statistics for each one sample t-test and pairwise correlations between personal sensing data streams. An ICC (type 3) showed that, on average, the dislike ratings were moderately consistent across the data streams, ICC = .42, 95% CI = [.35 - .48].

We also assessed the effect of active effort on dislike ratings (Figure 5). We conducted a paired samples t-test to compare the average dislike for active (audio check-in, EMA) vs. passive (geolocation, cellular communication logs, text message content) methods. Participants did not significantly differ in their dislike of active vs. passive methods, $t(153)$ = `r round(broom::tidy(summary(model_dislike))$statistic, 2)`, $P$ = `r round(broom::tidy(summary(model_dislike))$p.value, 2)`, $d$ = `r round((mean(data_dislike$Passive) - mean(data_dislike$Active)) / sd(data_dislike$dislike_diff), 2)`. 

### Willingness to Use for 1 Year

Figure 6 shows the distribution of participant responses to the self-reported acceptability item about willingness to use for 1 year for each personal sensing data stream (Figure S5 in Multimedia Appendix 3 contains additional information about willingness to use a 1X daily EMA method for 1 year). One sample t-tests revealed that each mean willingness score was significantly more acceptable than 0. Table 6 reports the summary statistics for each one sample t-test and pairwise correlations between personal sensing data streams. An ICC (type 3) showed that, on average, the willingness ratings were moderately consistent across the data streams, ICC = .52, 95% CI = [.46 - .58].

We also assessed the effect of active effort on willingness ratings (Figure 7). We conducted a paired samples t-test of the average willingness to use for 1 year for active (audio check-in, EMA) vs. passive (geolocation, cellular communication logs, text message content) signals. Participants reported higher acceptability with respect to willingness for passive data streams ($M$ = `r round(mean(data_willingness$Passive), 1)`, $SD$ = `r round(sd(data_willingness$Passive), 1)`) relative to active data streams ($M$ = `r round(mean(data_willingness$Active), 1)`, $SD$ = `r round(sd(data_willingness$Active), 1)`), $t(153)$ = `r round(broom::tidy(summary(model_willingness))$statistic, 2)`, $P$ = `r round(broom::tidy(summary(model_willingness))$p.value, 2)`, $d$ = `r round((mean(data_willingness$Passive) - mean(data_willingness$Active)) / sd(data_willingness$willingness_diff), 2)`. 


```{r}
footnote_table_4 <- "Initial columns indicate bivariate correlations among data streams. Final columns represent the number of participants ($N$), mean and standard deviation ($M$, $SD$), t-statistic ($t$) and Cohen’s d Effect size ($d$) for the one sample t-tests against 0 (undecided).  Higher values represent higher acceptability.  Active methods are displayed in red."
footnote_table_4a <- "$p$ < .05"
```

```{r table-4}
corrplot_int <- data_last %>% 
  select(`Audio Check-in` = audio_checkin_interfere,
         `EMA` = daily_survey_interfere,
         `Sleep Quality` = sleep_interfere) %>%
  corx(triangle = "lower",
       stars = c(0.05),
       method = "pearson",
       describe = c(`$M$` = mean, `$SD$` = sd))

corrplot_int <- as_tibble(corrplot_int, rownames = " ")
corrplot_int[corrplot_int == " - " ] <- "--"

corrplot_int <- corrplot_int %>% 
  mutate(`$N$` = c(154, 154, 87)) %>% 
  mutate(`$t$` = c(str_c(round(int_audio$statistic, 2), "*"), 
                   str_c(round(int_ema$statistic, 2), "*"), 
                   str_c(round(int_sleep$statistic, 2), "*"))) %>% 
  mutate(`$d$` = (c(round(int_audio$d, 2), round(int_ema$d, 2), round(int_sleep$d, 2)))) %>% 
  select(` `, `1`, `2`, `$N$`, `$M$`, `$SD$`, everything())

corrplot_int %>% 
  kbl(align = c("l", "c", "c", "c", "c", "c", "c", "c"),
      caption = "Bivariate and Univariate Statistics for Interference by Personal Sensing Data Stream",
      booktabs = TRUE,
      escape = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>% 
  pack_rows("Active", 1, 3) %>% 
  row_spec(1:3, background = "lightred") %>%
  footnote(general = footnote_table_4, symbol = footnote_table_4a, threeparttable = TRUE, escape = FALSE)
```



```{r}
footnote_table_5 <- "Initial columns indicate bivariate correlations among data streams. Final columns represent the number of participants ($N$), mean and standard deviation ($M$, $SD$), t-statistic ($t$) and Cohen’s d Effect size ($d$) for the one sample t-tests against 0 (undecided).  Higher values represent higher acceptability. Active methods are displayed in red and passive methods are displayed in blue."
footnote_table_5a <- "$p$ < .05"
```

```{r table-5}
corrplot_dis <- data_last %>% 
  select(`Audio Check-in` = audio_checkin_dislike,
         `EMA` = daily_survey_dislike,
         `Sleep Quality` = sleep_dislike,
         `Geolocation` = location_dislike,
         `Cellular Communication Logs` = all_logs_dislike,
         `Text Message Content` = sms_content_dislike) %>% 
  corx(triangle = "lower",
       stars = c(0.05),
       method = "pearson",
       describe = c(`$M$` = mean, `$SD$` = sd))

corrplot_dis <- as_tibble(corrplot_dis, rownames = " ")
corrplot_dis[corrplot_dis== " - " ] <- "--"

corrplot_dis <- corrplot_dis %>% 
  mutate(`$N$` = c(154, 154, 87, 154, 154, 154)) %>% 
  mutate(`$t$` = c(str_c(round(dis_audio$statistic, 2), "*"), 
                   str_c(round(dis_ema$statistic, 2), "*"), 
                   str_c(round(dis_sleep$statistic, 2), "*"),
                   str_c(round(dis_geolocation$statistic, 2), "*"), 
                   str_c(round(dis_logs$statistic, 2), "*"), 
                   str_c(round(dis_text_content$statistic, 2), "*"))) %>% 
  mutate(`$d$` = (c(round(dis_audio$d, 2), round(dis_ema$d, 2), round(dis_sleep$d, 2),
                    round(dis_geolocation$d, 2), round(dis_logs$d, 2), round(dis_text_content$d, 2)))) %>% 
  select(` `, `1`, `2`, `3`, `4`, `5`, `$N$`, `$M$`, `$SD$`, everything())

corrplot_dis %>% 
  kbl(align = c("l", "c", "c", "c", "c", "c", "c", "c"),
      caption = "Bivariate and Univariate Statistics for Dislike by Personal Sensing Data Stream",
      booktabs = TRUE,
      escape = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position", "scale_down")) %>% 
  pack_rows("Active", 1, 3) %>%
  pack_rows("Passive", 4, 6) %>% 
  row_spec(1:3, background = "lightred") %>%
  row_spec(4:6, background = "lightblue") %>%
  footnote(general = footnote_table_5, symbol = footnote_table_5a, threeparttable = TRUE, escape = FALSE)
```

```{r}
footnote_table_6 <- "Initial columns indicate bivariate correlations among data streams. Final columns represent the number of participants ($N$), mean and standard deviation ($M$, $SD$), t-statistic ($t$) and Cohen’s d Effect size ($d$) for the one sample t-tests against 0 (undecided).  Higher values represent higher acceptability. Active methods are displayed in red and passive methods are displayed in blue."
footnote_table_6a <- "$p$ < .05"
```  


```{r table-6}
corrplot_will <- data_last %>% 
  select(`Audio Check-in` = audio_checkin_1year,
         `EMA` = daily_survey_4_1year,
         `Sleep Quality` = sleep_1year,
         `Geolocation` = location_1year,
         `Cellular Communication Logs` = all_logs_1year,
         `Text Message Content` = sms_content_1year) %>% 
  corx(triangle = "lower",
       stars = c(0.05),
       method = "pearson",
       describe = c(`$M$` = mean, `$SD$` = sd))

corrplot_will <- as_tibble(corrplot_will, rownames = " ")
corrplot_will[corrplot_will== " - " ] <- "--"

corrplot_will <- corrplot_will %>% 
  mutate(`$N$` = c(154, 154, 87, 154, 154, 154)) %>% 
  mutate(`$t$` = c(str_c(round(use_audio$statistic, 2), "*"),
                   str_c(round(use_ema$statistic, 2), "*"), 
                   str_c(round(use_sleep$statistic, 2), "*"),
                   str_c(round(use_geolocation$statistic, 2), "*"), 
                   str_c(round(use_logs$statistic, 2), "*"), 
                   str_c(round(use_text_content$statistic, 2), "*"))) %>% 
  mutate(`$d$` = (c(round(use_audio$d, 2), round(use_ema$d, 2), round(use_sleep$d, 2),
                    round(use_geolocation$d, 2), round(use_logs$d, 2), round(use_text_content$d, 2)))) %>% 
  select(` `, `1`, `2`, `3`, `4`, `5`, `$N$`, `$M$`, `$SD$`, everything())

corrplot_will %>% 
  kbl(align = c("l", "c", "c", "c", "c", "c", "c", "c", "c", "c"),
      caption = "Bivariate and Univariate Statistics for Willingness to Use for 1 Year by Personal Sensing Data Stream",
      booktabs = TRUE,
      escape = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position", "scale_down")) %>% 
  pack_rows("Active", 1, 3) %>% 
  pack_rows("Passive", 4, 6) %>% 
  row_spec(1:3, background = "lightred") %>%
  row_spec(4:6, background = "lightblue") %>%
  footnote(general = footnote_table_6, symbol = footnote_table_6a, threeparttable = TRUE, escape = FALSE)
```

\newpage


# Discussion

This study evaluated the acceptability of active and passive personal sensing methods for a variety of raw data streams and associated methods.  To this end, we assessed participants’ choices/behaviors about both participating in the study and providing raw data streams for each method and their subjective perceptions of each sensing method.  We focused on participants with moderate to severe alcohol use disorder because they might have been expected to be less willing to share sensitive, private information due to the stigma associated with their disorder [@kilianStigmatizationPeopleAlcohol2021].  However, if these sensing methods were acceptable to them, highly promising opportunities are now emerging to address their largely unmet treatment needs [@substanceabuseandmentalhealthservicesadministrationKeySubstanceUse2020] with technological solutions that include digital therapeutics combined with personal sensing [@tedxtalksMentalHealthcareOur2022].  We organize our discussion around 7 key conclusions from our analyses.


## 1. Individuals with alcohol use disorder will generally accept the use of personal sensing methods 

Based on our sample, it appears individuals with alcohol use disorder are indeed willing to provide these sensitive, personally sensed raw data streams based on their behavioral choices regarding consent, enrollment, and opt-in for data collection in this study.  All but 1 of the individuals (191/192; 99.5%) who were eligible to participate consented to the personal sensing procedures.  Most of these individuals also returned 1 week later to formally enroll in the study and begin to provide these data (169/191; 88%).  Furthermore, all (169/169; 100%) of the participants who enrolled in the study explicitly opted-in to provide the 3 arguably most sensitive passive data streams - geolocation, cellular communication logs, and text message content.  

These consent, enrollment, and opt-in numbers could be considered upper- and lower-bound estimates of the percentage of individuals who are willing to provide these raw data streams in a research setting.  The very high percentage for consent may overestimate willingness because some of these individuals may have reconsidered their initial decision on further reflection such that they did not return for the next study visit to enroll formally.  However, the still quite high enrollment percentage may underestimate willingness to provide these data because some attrition was expected between consent and enrollment visits due to the instability associated with the early stages of recovery from alcohol use disorder.  In fact, table 3 indicates that almost half of the participants who consented but did not enroll may have done so for reasons other than their willingness to provide these raw data streams (e.g., health concerns, no transportation to lab, made repeated attempts to reschedule before discontinuing).

Participants' explicit self-report of their perceptions about the acceptability of these personal sensing methods were also generally consistent with their behavior.  Specifically, on average, participants rated all of the sensing methods more favorable than the neutral mid-point ("undecided") of the rating scales for all 3 dimensions we evaluated - interference, dislike, and willingness to use for 1 year.  These self-report data combined with our behavioral measures to suggest that all of these sensing methods can be considered for use with the majority of individuals with alcohol use disorder.  

Despite the aggregate positive perceptions of the full sample, non-trivial percentages of participants did report individual ratings that were more negative than the neutral mid-point across sensing methods and specific self-report items. For example, 18% of participants agreed or strongly agreed that the audio check-ins interfered with their daily activities.  Approximately 25% of participants agreed or strongly agreed that they disliked both the audio check-ins (27%) and providing access to the content of their text messages (22%).  And approximately 20% of participants disagreed or strongly disagreed that they would be willing to use our sensing methods for audio check-ins (16%), EMA (23%), and text message content (15%) for 1 year to help their recovery.  This suggests that there is still need to improve each of these sensing methods to make them more acceptable to a larger percentage of individuals.  The free response evaluations of each method (Tables S3 -- S7 in Multimedia Appendix 3) provide a starting point to address participant concerns.  That said, our participants did generally opt-in and comply with our sensing methods despite reporting these concerns.  Therefore, it is not clear yet at what threshold these concerns will translate to barriers for use or compliance with these methods.  

## 2.  Individuals can sustain the use of personal sensing for relatively long periods

Most enrolled participants were also able to sustain their commitment to provide these sensed data streams over time.  More than 91% (154/169) provided at least 1 month of sensed data and a large majority (133/169; 79%) provided data for all 3 months. As with enrollment statistics, these numbers also likely underestimate participants' ability to sustain personal sensing because many of the participants who discontinued or did not complete the study reported reasons to stop their participation that were unrelated to personal sensing (e.g., family crisis, relapse, moved out of state).  However, some participants (N = 4) did explicitly report reasons that appeared related to personal sensing (e.g., study demands too burdensome).  Additionally, others who stopped participating may have been influenced by their experiences with personal sensing without formally reporting those concerns.  

Participants who enrolled but then discontinued because of the personal sensing methods may have been influenced more by issues related to the burden associated with active sensing rather than more general issues related to data sensitivity/privacy.  Participants concerned about sharing passively sensed private information such as their moment by moment location or cellular communications would likely have had these concerns from the beginning such that they would not have consented, enrolled, and then opted-in to provide these sensitive data.  However, the burden associated with active sensing (e.g., 4x daily EMA, daily audio check-ins) may not have been clear to them until they tried to sustain those methods over time. 

In our sample of participants, we saw evidence that many of our participants hardly thought about the passively sensed data streams. For example:  

> "I forgot I was being tracked so it was not a big deal to me."

> "I went about my days as i normally would and never thought about it."

> "I forgot this was even happening most of the time. It did not interfere with my life."

On the other hand, some participants reported more discontent with the actively sensed data streams as time progressed.

> "I didnt mind the surveys but after the second month i felt like they became repetitive and I didn't feel like I got much utility from them."

> "Became overwhelming over time."

> "I did well with my recovery on my own and the surveys were more of a burden at the end."

Existing research assessing acceptability of sensing methods has been limited by short durations of monitoring, with very few studies extending beyond 6 weeks [@kleimanUsingWearablePhysiological2019; @ben-zeevMobileBehavioralSensing2016; @raughDigitalPhenotypingAdherence2021]. Additionally, compliance has been shown in some studies to decrease after only a few weeks [@wrzusEcologicalMomentaryAssessment2022; @yangFeasibilityAcceptabilitySmartphoneBased2015; @onoWhatAffectsCompletion2019]. The present study demonstrates that individuals can sustain their commitment to providing personally sensed data over time with limited drop-off. These findings suggest personal sensing methods may be viable in clinical settings, where consistent, sustained monitoring would be necessary. Given this promise, future research should expand to longer durations to assess self-reported and behavioral acceptability beyond 3 months. Our group is exploring this directly by employing personal sensing monitoring with individuals with opioid use disorder for a full year [@moshontzProspectivePredictionLapses2021]. Methods that permit long-term monitoring are particularly important for clinical applications for individuals with substance use disorders, who require lifelong care that can adapt as their risk for relapse and corresponding recovery needs fluctuate over time.



## 3. Some types of active personal sensing methods are generally acceptable and sustainable
	
Assessment burden may be expected to play a role in both the acceptability of active sensing methods and participant compliance with the associated procedures.  Nonetheless, participants displayed relatively high compliance (on average 81% of EMAs completed) with the 4x daily EMA.  This is notable because our study duration of 3 months was substantially longer than typical studies using EMA, which often last only 2 - 4 weeks [@jonesComplianceEcologicalMomentary2019; @wrzusEcologicalMomentaryAssessment2022].  This increases confidence in the feasibility of this active sensing method for research and clinical applications that require longer monitoring periods.  Of course, this level of compliance may be contingent on the measurement parameters used in our study (4x daily survey of 7 - 10 items).  In fact, even higher compliance may have been observed if measurement was limited to 1 EMA per day given that 94% of our participants completed at least 1 of the 4 EMAs every day. Participants were also significantly more likely to report a willingness to use a 1X daily EMA compared to 4X daily EMA for 1 year. However, we must interpret these findings cautiously. Participant self-reports to a 1X daily EMA method are not based on experience since they were expected to comply with the 4X daily EMA. 

From free-response comments, we see evidence that many of our participants had no issues with the 4X daily EMA and some even enjoyed the frequent prompts. For example:

> "They were quick and easy to complete."

> "I had no issues for the daily surveys. I felt that it kept me in check and reminder to not drink. I would not change it."

> "I liked the structure that it added to my day. For other activities,a being deliberate about setting aside time for things has helped me maintain changes to my life for the better. The surveys were no exception."

Still, other participants suggested less frequent prompts would be more practical. For example:

> "I felt it was an annoyance, another thing i had to do on top of everything else when it was so frequent. I found one per day is tolerable by days end not a set time. I am just too scattered with how my brain works."

> "One daily would be great."

> "Great tool for self check in. Not necessary to do four per day."
	
There was some evidence that participants found passive sensing methods to be more acceptable than active sensing methods overall.  Specifically, mean ratings for willingness to use for 1 year were significantly higher for passive vs. active sensing methods.  However, the magnitude of this effect was small, and mean willingness was significantly greater than the neutral mid-point for both active and passive methods.  In addition, there was no difference in mean dislike ratings for active vs. passive methods.  Thus, differences between acceptability of active and passive methods were small, inconsistent, and unlikely to be clinically meaningful. These comparisons between active and passive methods increase our confidence somewhat that the selective use of active measures, when necessary, may be acceptable to participants for relatively long periods.  Of course, we cannot speculate strongly beyond 3 months from this study.

Some sensing methods (e.g., EMA, audio check-ins) will always require active input from users but other methods may become more passive with further technological advances.  For example, our sensing of sleep quality in this study made use of an early version of the Beddit sleep monitor that required participants to actively log when they entered and exited their bed during each period of sleep.   However, later versions of the Beddit detect periods of sleep automatically. Similarly, we discontinued sensing of physiology with the Empatica E4 in an early phase of our study because participants had to manually connect the wristband each night to a tablet to upload their data.  This proved too burdensome and complex for most participants.  However, the current version of the Empatica E4 claims to have improved automatic bluetooth streaming of the data to the cloud, which if robust, would greatly reduce the burden associated with physiology sensing.

The acceptability of active sensing methods holds great clinical utility. Active personal sensing methods such as EMA offer unique insight into patient experiences, thoughts, and feelings that cannot always be captured accurately or comprehensively by passive methods. Self-report EMA in particular seems likely to maintain a role in risk monitoring and other, similar clinical applications. Thus, we were encouraged to find that even with relatively high active burden of 4X daily surveys, EMA was acceptable to participants as assessed via self-report and behavioral compliance. 


## 4. Important individual differences in subjective perceptions exist both within and across personal sensing methods.

We included a second and more novel daily active sensing method in this study, audio check-ins.  These audio check-ins have high potential as a rich source of information about participants' daily experiences.  Natural language processing of transcripts of their check-ins can provide a novel window into their thoughts [@tausczikPsychologicalMeaningWords2010; @tackmanDepressionNegativeEmotionality2019; @jacobucciUseTextbasedResponses2021; @lowNaturalLanguageProcessing2020]. These audio check-ins provided participants the opportunity to share more openly and candidly (i.e., without close-ended questions) their thoughts, feelings, and progress towards recovery without being limited to researcher-selected prompts. Analyses of the acoustic characteristics of their check-ins may yield independent measures of their affective state [@beloualiAcousticLanguageAnalysis2021; @faurholt-jepsenVoiceAnalysisObjective2016], including the potential for measuring affect outside of a participant's conscious awareness. 

Unfortunately, overall participant compliance with the daily audio check-ins was relatively low (on average 55% of audio check-ins completed) and 2% of the sample did not complete any check-ins throughout their entire study period. Participants' free-response evaluations of this method highlighted some concerns that could be addressed in the future to increase compliance (e.g., timing of the check-ins, technical issues with recording and sending check-ins, use of the same prompt for all check-ins).  However, privacy issues related to recording the audio check-in were also reported by many participants.  As examples of this concern, participants reported:     

> "It takes time out of your day where you have to completely switch locations just so you can do it in private. I don't like that people could hear me and the topic wherever and whenever so I stopped using it."

> "I also have to keep privacy in mind since it's not something I'd like to complete within earshot of others."

> "The only negative part about the daily check in was my mom, who I live and work with, listening in while I was talking. I would have to find a time when she was preoccupied or away from me to talk freely."

These privacy concerns represent an inherent challenge to using this method as implemented, but accommodations could be made to gather some if not all of the same information. For example, using less frequent prompting with wider time completion windows (i.e., a weekly audio check-in) may increase individuals' ability to find a private moment. Additionally, allowing individuals to type their response as an alternative completion method could assuage concerns. This alternative would prevent acoustic analysis, but it would still permit natural language processing of open-ended responses. These accommodations could encourage greater compliance among those who completed few or no audio check-ins as well as individuals who missed check-ins sporadically due to privacy concerns.

Finding ways to assuage privacy concerns and accommodate individual preferences may be useful as many other participants valued and believed they benefited from recording these daily audio check-ins.  For example:

> "I loved this part! It was like journaling kind of where i would discover things that were hidden in my subconscious. I have a recording app on my phone and i will continue doing the voice check in as a form of checking in with myself."

> "This was my favorite part of the study. It helped me to set a good intention towards my recovery."

> "I liked the daily check in. It gave me a way to vent without being judged."

Consistent with this somewhat polarized evaluation of the audio check-ins, a more nuanced consideration of distribution for compliance across participants suggested it was somewhat bi-modal.  Participants tended to either comply well or very poorly with this method.	
	
More broadly, participants' self-reported perceptions were only moderately consistent across the different sensing methods.  This can be seen in the moderate ICCs (and bivariate correlations) across methods for each self-report item.  In other words, high dislike ratings for 1 sensing method by a specific participant did not strongly indicate that this same participant would also dislike the other sensing methods.  This is also true for ratings of interference and willingness to use for 1 year items.  Participants could dislike (or be unwilling to use, etc.) 1 method but not others.  To the degree to which concerns are method-specific, opportunities may exist to tailor sensing systems to user preferences.  In other words, participants could opt-out of methods they deemed unacceptable but provide those other sensing methods that were acceptable to them.  For example, our behavioral compliance data suggest that some participants would not have completed the study if daily audio check-ins were required, yet they were willing to provide data via other personal sensing methods. Algorithms that use sensed data for clinical applications could then be developed for different combinations of available raw data streams.  Participants could be educated that personalized algorithms will likely perform better if given access to more raw data streams. This education will allow them to make an informed choice as to the threshold they set for themselves to opt-out and the potential consequences of not providing that data source.  However, allowing them to opt-out of some methods may increase the number of participants who will agree to provide sensed data. 
	
## 5.  Benefits likely matter

The overall acceptability of personal sensing to research participants and patients is likely a function of both the perceived costs and benefits for those individuals [@pavlouConsumerAcceptanceElectronic2003; @schnallTrustPerceivedRisk2015; @atienzaConsumerAttitudesPerceptions2015].  However, we focused on measuring only perceived costs (e.g., privacy, burden) associated with personal sensing because the benefits to participants from the sensed data collected in this research study were minimal.  Participants were provided with modest financial incentives to complete the EMAs ($25/month) and to provide access to the 2 passively sensed raw data streams ($10/month for geolocation and $15/month for cellular communication logs).  These sensed data streams were not used to provide any clinical benefit to participants' recovery in our study.  

Monetary incentives are commonly used in research to provide a more favorable cost/benefit ratio surrounding specific methods or overall participation.  Such monetary incentives are commonplace and recommended when using active personal sensing methods like EMA [@parkinsonDesigningUsingIncentives2019].  However, the incentives to provide access to passively sensed geolocation and cellular communications in our study may have contributed to the acceptance of these methods and the success we had collecting those sensitive data from participants. This may be particularly true given the relatively low socioeconomic status of many of our participants. For example, the median personal income for our participants was $34,233, with 12% of individuals reporting current unemployment and 25% reporting an annual income below the federal poverty guidelines. 

Monetary incentives to increase the acceptability of personal sensing do not need to be limited to research settings.  Incentives can also be used as part of treatment or continuing care in clinical settings.  For example, the use of monetary incentives or equivalents (e.g., prizes) as part of a contingency management program is well established to support abstinence from alcohol or other drugs  [@prendergastContingencyManagementTreatment2006; @ginleyLongtermEfficacyContingency2021].  If personal sensing proved useful for the treatment or ongoing support of patients' recovery, similar incentives could be established to encourage patients to provide these sensed data.  

Incentives may be less necessary in clinical settings when more direct clinical benefits from personal sensing are available.  For example, research has suggested that privacy concerns associated with personal sensing may be reduced if participants perceive that they will benefit from the sensed data[@atienzaConsumerAttitudesPerceptions2015; @klasnjaExploringPrivacyConcerns2009;@bessenyeiComfortabilityPassiveCollection2021]. There was some evidence for this perspective in the free response comments from our participants as well.  For example: 


> "I felt all right having my location tracked. If it were used in a way to keep me from relapsing my feeling about it would be even more positive."

> "I'm a somewhat typical introverted, slightly paranoid, grump, so the idea of being tracked is automatically negative. That being said, if it's something that helps people that can be a really good thing."

> "It seems this was more to aid with the study research than to help with my recovery. If i could see a way it would be prioritized to help me, i would be more willing to have them tracked longer."


Given this, the acceptability of personal sensing may be higher than observed in our study if the sensing system was implemented as part of their treatment or continuing care during their recovery.  Digital therapeutics are particularly well-positioned to use sensed data to select, personalize, or time the delivery of interventions and other supports to improve clinical outcomes. Future research should evaluate the acceptability of personal sensing in contexts where its use directly benefits those providing the sensed data.  In these contexts, benefits can also be explicitly measured.  It may even be possible to manipulate the benefits from personal sensing across participants to evaluate their contribution to acceptability more rigorously.


## 6.  Trust likely matters

Trust is also likely to affect the overall acceptability of personal sensing data, which are inherently private and sensitive in nature.  Acceptability may depend on who employs personal sensing and who has access to the raw and processed data [@atienzaConsumerAttitudesPerceptions2015; @riegerPsychiatryOutpatientsWillingness2019; @rendinaPrivacyTrustData2018; @nicholasRoleDataType2019; @prasadExposingPrivacyConcerns2011].  The available evidence suggests that people are more comfortable sharing private, sensitive information with researchers and their doctor and less comfortable sharing information with family members, electronic health record databases, and third party apps and websites [@rendinaPrivacyTrustData2018; @nicholasRoleDataType2019; @prasadExposingPrivacyConcerns2011].

We saw evidence of the role of trust in the free response comments from our participants. For example:

> "I trusted the study group to not use my personal information for any other use."

> "I don't mind turning this info over and I trust the people running the study."

 
The research setting may come with relatively greater trust because of the high level of transparency regarding risks and protections associated with obtaining informed consent.  Some protections may only be feasible for research as well.  For example,  NIH funded research that collects identifiable, sensitive information is automatically  issued a Certificate of Confidentiality that prohibits disclosing this information to anyone not connected to the research except when the participant consents or in a few other limited situations. Certificates of Confidentiality can also be requested for similar research not funded by NIH.  

Our participants appeared to recognize and appreciate these protective measures. For example:

> "I don't mind this at all either, as long as I know this information is protected and anonymous."

> "Didn't bother me since its protected."

> "Just in general this feels intrusive but as part of the study it is fine!"
 
Implementations of personal sensing for treatment inside and outside of clinical care settings [@aggarwalAdvancingArtificialIntelligence2020] will need to carefully consider how to establish similar, high levels of trust.  Clinical applications of personal sensing may sit at an intersection of sharing data with doctors (with which individuals tend to be comfortable) and with electronic health record databases and apps (with which individuals tend to be less comfortable) [@nicholasRoleDataType2019; @prasadExposingPrivacyConcerns2011]. For example, it may be necessary to protect against the subpoena of sensitive information in civil and criminal proceedings.  Patients will also likely need to be assured that sensed data used for their clinical care will not also be shared with their health insurance provider with associated risks related to higher insurance premiums or dropped coverage. These issues of data access and unauthorized secondary use of otherwise private information are often cited concerns regarding personal sensing [@ackermanPrivacyIssuesHumanComputer2008; @atienzaConsumerAttitudesPerceptions2015].
	
Regardless of the setting, trust may be lower in stigmatized groups that could otherwise benefit from personal sensing.  For example, individuals with mental illness still experience substantial stigma that could impede willingness to share personal, sensitive information with researchers or clinical care providers [@schomerusStigmaAlcoholDependence2011; @barryStigmaDiscriminationTreatment2014; @overtonStigmaMentalIllness2008; @parcesepePublicStigmaMental2013].  In fact, we focused on individuals with alcohol use disorder in this study to evaluate the acceptance of personal sensing methods in a population that we expected might have barriers associated with trust.  Of course, trust may be lower still among individuals with other substance use disorders that involve drugs whose use is illegal.  That said, many of our participants reported ongoing use of drugs other than alcohol throughout the study (49% reported past month illicit drug use), as expected given high rates of poly-substance use among individuals with substance use disorders. Furthermore, we have had promising, preliminary success recruiting participants with opioid use disorder for an NIH funded study on personal sensing in this population [@zotero-22407]. This suggests that our results regarding the acceptance of personal sensing may generalize across substance use disorders.  

Trust and related privacy concerns may also be more difficult to overcome in historically marginalized groups that have experienced systemic racism and other stigma or exclusions [@marwickPrivacyMarginsUnderstanding2018].  These individuals may find it more difficult to achieve privacy in their daily lives, and they may hold very different perspectives on the costs vs. benefits of surveillance in the context of personal sensing or more generally.  Unfortunately, our sample was not diverse with respect to race and ethnicity.  Future research on personal sensing must specifically recruit for such diversity to better understand its acceptance in communities of color.  We have learned from the present study and adjusted our recruiting efforts accordingly to recruit a sample that is more diverse with respect to race, ethnicity, and geographical region for our ongoing personal sensing project with individuals with opioid use disorder. 


## 7.  Feasibility is a function of more than participant perceptions of acceptabilty

Of course, user acceptance of personal sensing methods is necessary but not sufficient to expand use of these methods in research and clinical implementations.  A variety of other key issues may facilitate or present barriers to wider use of personal sensing.  These include cost and accessibility, stability over time, and the utility of personal sensing relative to other more traditional methods.  

The smartphone itself is arguably the best available sensing system today.  Today's smartphone contains numerous sensors and other raw data streams that can be used for personal sensing. In our study, we took advantage of GPS and other location services to track geolocation, and the microphone for daily check-ins.  We accessed smartphone call and text message logs for communications meta-data and message content.  The smartphone also provided a convenient platform to collect self-report EMA.  

Smartphones also provide a relatively accessible platform for personal sensing.  Despite their high cost, 85% of adults in the US already own a smartphone.  Equally important, this level of ownership is relatively consistent across race/ethnicity, geographic regions (e.g., urban, suburban, rural) and income level [@pewresearchcenterMobileFactSheet2021].  In fact, only 11 of the eligible participants for our study did not already own a contemporary smartphone.  This is notable given that individuals with alcohol use disorder may have been expected to present with more barriers to smartphone ownership than that of the general population.  In a research setting, we were able to provide individuals with a smartphone if they did not already have one. Like monetary incentives, this practice need not be limited to research; smartphones could be provided to permit personal sensing-based clinical support. 

Personal sensing can also be done with wearable or other sensors outside of the smartphone.  We used Empatica and Beddit systems to sense physiology and sleep, respectively.  The use of watches (e.g., Apple Watch) and wristbands (e.g., Fitbit) for sensing activity and some physiology is also increasing [@doryabIdentifyingBehavioralPhenotypes2019; @stevensonUsingEcologicalMomentary2021].  However, some of these systems can be expensive, and - unlike smartphones - none have been adopted widely enough to assume that most users will already own said devices.  For research applications, this limitation can be overcome by providing the hardware to participants as needed.  Though not impossible to do the same in clinical settings, the large number of patients who would require this technology may either limit or increase the cost to scale the sensing system.  

Both research and clinical applications of sensing systems require some guarantee that the hardware and software will remain available and supported for the duration of the intended use.  Unfortunately, there are currently high levels of churn among the companies that support these systems given the rapid innovation occurring at this time. We collected data for approximately 2.5 years between 2017 -- 2019.  During this time, Apple bought the company that developed the Beddit Sleep Monitor and discontinued support for previous users.  Apple re-introduced the sleep sensing system for iPhone users in late 2018, but discontinued it again in early 2022.  For these reasons, we were able to collect sleep sensing data on fewer than half of our research participants.   

During this same data collection period, there was also churn in the software that we used for sensing geolocation.  We used the Moves app at the start of the study, but needed to switch to use FollowMee when Facebook acquired the company that developed Moves and discontinued its support.  However, this software churn was less disruptive because both apps relied on smartphone sensors to acquire the raw geolocation data stream.  This suggests yet another reason to prefer systems that make use of generic smartphone sensors rather than propriety hardware. 

High rates of churn can also affect the perceived acceptability of the software. For example, it could be inconvenient to have to adapt to frequent changing of app platforms. Additionally, software may be left unmonitored for periods of time leaving new bugs unresolved. We saw in our own sample of participants how frustrating technological issues were. For example:

> "I experienced issues with this [audio check-in] from the very beginning and discontinued early on. I was unable to get the message beyond 8-10 seconds with close to 6 attempts each day. Was not worth the time or stress for me."

> "My bigger problem with the tracker app. Was more the fact that my battery power was getting depleted so rapidly, sometimes within 2 hours post-full charge. I found myself charging my phone at least 5  times a day. And at times, this became very stressful."

> "There were problems with the sleep monitor in that I was not able to activate the monitor when I was going to sleep." 

	
## Future Directions

The present study demonstrates the acceptability of several personal sensing methods. These methods were acceptable 1) over a longer period of time than has previously been assessed, 2) across active and passive methods, 3) despite the sensitivity of the data, 4) among individuals with alcohol use disorder who may have greater privacy concerns, and 5) without explicit clinical benefits to the participants. These findings suggest personal sensing methods are poised as accessible, feasible avenues to collect data about individuals to be used for clinical applications. More work is needed to determine the predictive utility of the data that can be collected via personal sensing, but our study shows that this work will be worthwhile to pursue. 

Personal sensing is acceptable, and the technology to collect it (namely, the smartphone) is widely accessible. Personal sensing can make digital therapeutics - smartphone and web-based apps that provide mental health care - smart. These methods can personalize care for individuals such that they receive the specific interventions and supports they need at the time they need them. Smart digital therapeutics can be scaled widely to provide treatment to the overwhelming majority of individuals who do not currently receive mental health care. They can reach those who have been historically excluded from or have otherwise faced barriers to care. With personal sensing powering digital therapeutics, we are positioned for a paradigm shift in mental health care. The present study brings us 1 step closer to this goal, ensuring that the methods we hope to use to revolutionize care are acceptable to the patients who will use them.
<!--I changed every "mental healthcare" to "mental health care" in the discussion to be consistent with the intro-->

\newpage

# References
<div id="refs"></div>

\newpage

```{r}
fig_caption_1 <- "Flowchart of participant retention over the course of the 3-month study. This figure displays retention and attrition of all eligible participants at various stages from consent though study completion.  It also display the reasons for attrition categorized as due to acceptability, other reasons, or unknown.  We present additional detail on reasons for attrition in Table 3."
```

```{r figure-1, fig.cap = fig_caption_1, out.extra = "", fig.pos="h", warning = FALSE}
include_graphics(here("burden/manuscript/figs/fig_disposition.jpg"), dpi = 150)
```

\newpage

```{r}
fig_caption_2 <- "Compliance over Time for EMA (1x daily), EMA (4x daily), and Audio Check-in.\\linebreak Notes:  Mean compliance for each week on study. Mean standard error is depicted by the solid error bars. Overall mean compliance is depicted by the dashed line. N = 154."
```

```{r figure-2, fig.height = 3.5, fig.cap = fig_caption_2, out.extra = "", fig.pos="h"}
# function to map over
get_study_days <- function(the_subid, dates) {
  start_study <- dates %>% filter(subid == the_subid) %>% pull(start_study)
  end_study <- dates %>% filter(subid == the_subid) %>% pull(end_study)
  study_days <- tibble(subid = the_subid, study_day = seq(start_study, end_study - days(1), by = "day")) 
  return(study_days)
}

subids <- sample_fu1$subid
dates <- sample_fu1 %>% 
  select(subid, start_study, end_study)

study_dates <- subids %>% 
  map_dfr(~get_study_days(.x, dates))

ema <- ema_m %>% 
  select(subid, start_date) %>% 
  full_join(ema_l %>% select(subid, start_date), by = c("subid", "start_date")) %>% 
  mutate(start_date = date(start_date),
         subid = as.numeric(subid)) %>% 
  filter(subid %in% sample_fu1$subid)

ema_count <- ema %>%  
  count(subid, start_date) %>%
  mutate(n = if_else(n > 4, 4, as.numeric(n)))

ema_study_dates <- study_dates %>% 
  left_join(ema_count, by = c("subid", "study_day" = "start_date")) %>% 
  mutate(n = if_else(is.na(n), 0, n)) %>% 
  mutate(n_prompts = 4)

ema_study_weeks <- ema_study_dates %>% 
  group_by(subid) %>% 
  slice(1:7) %>% 
  mutate(week = 1) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(8:14) %>% 
    mutate(week = 2)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(15:21) %>% 
    mutate(week = 3)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(22:28) %>% 
    mutate(week = 4)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(29:35) %>% 
    mutate(week = 5)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(36:42) %>% 
    mutate(week = 6)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(43:49) %>% 
    mutate(week = 7)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(50:56) %>% 
    mutate(week = 8)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(57:63) %>% 
    mutate(week = 9)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(64:70) %>% 
    mutate(week = 10)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(71:77) %>% 
    mutate(week = 11)) %>% 
  bind_rows(ema_study_dates %>% 
    group_by(subid) %>% 
    slice(78:84) %>% 
    mutate(week = 12)) %>% 
  ungroup()

ema_1_week_compliance <- ema_study_weeks %>%
  mutate(n = if_else(n > 1, 1, n),
         n_prompts = 1) %>% 
  group_by(subid, week) %>% 
  summarize(sum_n = sum(n), sum_prompts = sum(n_prompts), .groups = "rowwise") %>% 
  mutate(compliance = sum_n/sum_prompts) %>% 
  ungroup()

ema_4_week_compliance <- ema_study_weeks %>% 
  group_by(subid, week) %>% 
  summarize(sum_n = sum(n), sum_prompts = sum(n_prompts), .groups = "rowwise") %>% 
  mutate(compliance = sum_n/sum_prompts) %>% 
  ungroup()


audio <- audio %>% 
  filter(subid %in% sample_fu1$subid)

audio_count <- audio %>%  
  count(subid, date) %>%
  mutate(n = if_else(n > 1, 1, as.numeric(n)))

audio_study_dates <- study_dates %>% 
  left_join(audio_count, by = c("subid", "study_day" = "date")) %>% 
  mutate(n = if_else(is.na(n), 0, n)) %>% 
  mutate(n_prompts = 1)

mean_audio <- audio_study_dates %>% 
  summarize(n_total = sum(n), prompt_total = sum(n_prompts)) %>% 
  mutate(mean = n_total/prompt_total)

audio_study_weeks <- audio_study_dates %>% 
  group_by(subid) %>% 
  slice(1:7) %>% 
  mutate(week = 1) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(8:14) %>% 
    mutate(week = 2)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(15:21) %>% 
    mutate(week = 3)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(22:28) %>% 
    mutate(week = 4)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(29:35) %>% 
    mutate(week = 5)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(36:42) %>% 
    mutate(week = 6)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(43:49) %>% 
    mutate(week = 7)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(50:56) %>% 
    mutate(week = 8)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(57:63) %>% 
    mutate(week = 9)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(64:70) %>% 
    mutate(week = 10)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(71:77) %>% 
    mutate(week = 11)) %>% 
  bind_rows(audio_study_dates %>% 
    group_by(subid) %>% 
    slice(78:84) %>% 
    mutate(week = 12)) %>% 
  ungroup()

audio_week_compliance <- audio_study_weeks %>% 
  group_by(subid, week) %>% 
  summarize(sum_n = sum(n), sum_prompts = sum(n_prompts), .groups = "rowwise") %>% 
  mutate(compliance = sum_n/sum_prompts) %>% 
  ungroup()


week_compliance_all <- audio_week_compliance %>% 
  group_by(week) %>% 
  summarize(mean_compliance = mean(compliance),
            n = n(),
            sd = sd(compliance)) %>% 
  mutate(se = sd/sqrt(n),
         signal = "Audio Check-in") %>% 
  bind_rows(ema_4_week_compliance %>% 
              group_by(week) %>% 
              summarize(mean_compliance = mean(compliance),
                        n = n(),
                        sd = sd(compliance)) %>% 
              mutate(se = sd/sqrt(n),
                     signal = "EMA (4x Daily)")) %>% 
  bind_rows(ema_1_week_compliance %>% 
              group_by(week) %>% 
              summarize(mean_compliance = mean(compliance),
                        n = n(),
                        sd = sd(compliance)) %>% 
              mutate(se = sd/sqrt(n),
                     signal = "EMA (1x Daily)"))

week_compliance_all %>% 
  mutate(signal = factor(signal, levels = c("EMA (1x Daily)", "EMA (4x Daily)", "Audio Check-in"))) %>% 
  group_by(week, signal) %>% 
  ggplot(aes(x = week, y = mean_compliance, group = signal, shape = signal)) +
  geom_point(size = 2) +
  geom_line() +
  geom_errorbar(aes(ymin = mean_compliance - se, ymax = mean_compliance + se), 
                width = .3, size = .3) +
  theme_classic() +
  scale_x_continuous(name = "Week", 
                     breaks = seq(1, 12, 1)) +
  scale_y_continuous(name = "Compliance", 
                     breaks = seq(0, 1, .1), 
                     limits = c(0, 1)) +
  scale_shape_manual(values = c(19, 1, 17)) +
  geom_hline(aes(yintercept = mean_compliance), week_compliance_all %>% 
               group_by(signal) %>% 
               summarize(mean_compliance = mean(mean_compliance)),
             linetype = "dashed", size = .3) +
  theme(legend.title = element_blank(),
        legend.text = element_markdown(),
        legend.position = "bottom",
        plot.caption = element_markdown(hjust = 0, size = 10, lineheight = 1.25),
        text = element_text(),
        plot.title = element_markdown(),
        plot.subtitle = element_markdown(size = 12, lineheight = 1.25)) 
```


\newpage

```{r}
fig_caption_3 <- "Interference Ratings by Personal Sensing Data Stream.\\linebreak Notes:  Mean responses to [Personal sensing method name] interfered with my daily activities. X-axes are ordered to display higher acceptability on the right side.  N = 154 for all data streams except sleep  quality (N = 87). Solid red line represents the mean and dashed black line represents the neutral midpoint (undecided). All raw data streams had a mean significantly higher than the neutral midpoint.  Interference ratings were only collected for active methods."

```


```{r figure-3, fig.height = 3.5, fig.cap = fig_caption_3, out.extra = "", fig.pos="h"}
interference_plot_data <- data_last %>% 
  select(contains("interfere")) %>%   
  pivot_longer(everything(), "measure", values_drop_na = TRUE) %>% 
  mutate(measure = factor(measure, 
                          levels = c("audio_checkin_interfere", "daily_survey_interfere", 
                                     "sleep_interfere"),
                          labels = c("Audio Check-in", "EMA", "Sleep Quality"))) %>% 
  mutate(value = factor(value, levels = c(-2:2), labels = c("Strongly agree", "Agree", "Undecided", "Disagree", "Strongly disagree"))) %>% 
  mutate(active = case_when(measure == "Audio Check-in" ~ "Active",
                            measure == "EMA" ~ "Active",
                            measure == "Sleep Quality" ~ "Active")) 

interference_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#FFDEDE") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = "Proportion",
       x = NULL) +
  theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575") +
  geom_vline(aes(xintercept = means), interference_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#b44343") 
```


\newpage
```{r}
fig_caption_4 <- "Dislike Ratings by Personal Sensing Data Stream.\\linebreak  Notes:  Mean responses to I disliked [Personal sensing method name]. X-axes are ordered to display higher acceptability on the right side.  N = 154 for all data streams except sleep quality (N = 87). Solid red or blue line represents the mean and dashed line represents the neutral midpoint (undecided). All raw data streams had a mean significantly higher than the neutral midpoint.  Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-4, fig.height = 6, fig.cap = fig_caption_4, out.extra = "", fig.pos="h"}
dislike_plot_data <- data_last %>% 
  select(contains("dislike")) %>%  
  pivot_longer(everything(), "measure", values_drop_na = TRUE) %>% 
   mutate(measure = factor(measure, 
                          levels = c("audio_checkin_dislike", "daily_survey_dislike", "sleep_dislike",
                                     "location_dislike", "all_logs_dislike", "sms_content_dislike"),
                          labels = c("Audio Check-in", "EMA", "Sleep Quality",
                                     "Geolocation", "Cellular Communication Logs", "Text Message Content"))) %>% 
  mutate(value = factor(value, levels = c(-2:2), labels = c("Strongly agree", "Agree", "Undecided", "Disagree", "Strongly disagree"))) %>% 
  mutate(active = case_when(measure == "Audio Check-in" ~ "Active",
                            measure == "EMA" ~ "Active",
                            measure == "Sleep Quality" ~ "Active",
                            measure == "Geolocation" ~ "Passive",
                            measure == "Cellular Communication Logs" ~ "Passive",
                            measure == "Text Message Content" ~ "Passive")) 

active_dis <- dislike_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#FFDEDE") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
  theme(legend.position = "none",
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        text = element_text(size = 12)) +
  ylim(0, .65) +
  geom_vline(aes(xintercept = means), dislike_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#b44343") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575") +
  theme(text = element_text(size = 12))


passive_dis <- dislike_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#DBF8FF") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = means), dislike_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>%  
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#05667b") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575") 

dislike_plot <- wrap_plots(active_dis, passive_dis, ncol = 1)

dislike_plot  %>% 
  add_global_label(Ylab = "                   Proportion",
                   Ygap = .02
)
```


\newpage
```{r}
fig_caption_5 <- "Average Dislike by Active vs. Passive Methods.\\linebreak  Notes:  X-axes are ordered to display higher acceptability on the right side.  Active methods (displayed in red) represent an average of audio check-in and EMA.  Passive methods (displayed in blue) represent an average of geolocation, cellular communication logs and text message content.  Solid red or blue line represents the mean and dashed line represents the neutral midpoint (undecided).  Participants did not different significantly in their dislike of active vs. passive methods. N= 154. Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-5, fig.width = 5, fig.cap = fig_caption_5, out.extra = "", fig.pos="h"}
# transform dataframe of active/passive means to long
data_dislike_long <- data_dislike %>% 
  pivot_longer(cols = c(Active, Passive), names_to = "effort", values_to = "dislike") %>% 
  select(-dislike_diff)

data_dislike_long %>% 
  # bin continuous means, keep numeric
  mutate(dislike_binned = as.numeric(cut(dislike, breaks = c(-2.5, -1.5, -0.5, 0.5, 1.5, 2.5)))) %>% 
  # change 1 to 5 scale to -2 to 2
  mutate(dislike_binned = dislike_binned - 3) %>% 
  ggplot(aes(x = dislike_binned, y = ..prop.., group = effort, fill = effort)) +
  geom_bar(color = "black") +
  facet_wrap(~ effort, ncol = 1) +
  theme_classic() +
  labs(y = "Proportion",
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2),
                   labels = c("Strongly agree", "Agree", "Undecided", "Disagree", "Strongly disagree")) +
  scale_fill_manual(values = c("#FFDEDE", "#DBF8FF")) +
  geom_vline(aes(xintercept = m), data_dislike_long %>% 
               group_by(effort) %>% 
               summarise(m = mean(dislike)), size = .705, color = c("#b44343", "#05667b")) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", size = .4, color = "#787575")
```


\newpage

```{r}
fig_caption_6 <- "Willingness to Use for 1 Year Ratings by Personal Sensing Data Stream.\\linebreak  Notes:  Mean responses to I would be willing to use [Personal sensing method name] for 1 year to help with my recovery. X-axes are ordered to display higher acceptability on the right side.  N = 154 for all data streams except sleep monitoring (N = 87). Solid blue or red line represents the mean and dashed line represents the neutral midpoint (undecided). All raw data streams had a mean significantly higher than the neutral midpoint.  Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-6, fig.height = 6, fig.cap = fig_caption_6, out.extra = "", fig.pos="h"}
willingness_plot_data <- data_last %>% 
  select(contains("1year")) %>%  
  pivot_longer(everything(), "measure", values_drop_na = TRUE) %>% 
  mutate(measure = factor(measure, 
                          levels = c("audio_checkin_1year", "daily_survey_4_1year", 
                                     "daily_survey_1_1year", "sleep_1year", "location_1year", 
                                     "all_logs_1year", "sms_content_1year"),
                          labels = c("Audio Check-in", "EMA", "Daily Survey (x1)<i><sup>a</sup></i>",
                                     "Sleep Quality", "Geolocation", "Cellular Communication Logs", 
                                     "Text Message Content"))) %>% 
  mutate(value = factor(value, levels = c(-2:2), labels = c("Strongly disagree", "Disagree", "Undecided", "Agree", "Strongly agree"))) %>% 
  mutate(active = case_when(measure == "Audio Check-in" ~ "Active",
                            measure == "EMA" ~ "Active",
                            measure == "Sleep Quality" ~ "Active",
                            measure == "Geolocation" ~ "Passive",
                            measure == "Cellular Communication Logs" ~ "Passive",
                            measure == "Text Message Content" ~ "Passive")) 

willingness_active <- willingness_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#FFDEDE") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
  theme(legend.position = "none",
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        text = element_text(size = 12),
        strip.text = element_markdown()) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = means), willingness_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#b44343") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575")


willingness_passive <- willingness_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#DBF8FF") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = means), willingness_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>%  
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#05667b") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575")

willingness_final_plot <- wrap_plots(willingness_active, willingness_passive, ncol = 1)

willingness_final_plot  %>% 
  add_global_label(Ylab = "                   Proportion",
                   Ygap = .02
)
```

\newpage

```{r}
fig_caption_7 <- "Average Willingness to Continue for 1 Year by Active vs. Passive Methods.\\linebreak  Notes:  X-axes are ordered to display higher acceptability on the right side.  Active methods (displayed in red) represent an average of audio check-in and EMA.  Passive methods (displayed in blue) represent an average of geolocation, cellular communication logs and text message content.  Solid red or blue line represents the mean and dashed line represents the neutral midpoint (undecided). Participants reported on average significantly higher acceptability with respect to willingness to continue using for 1 year for passive compared to active methods. N= 154. Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-7, fig.width = 5, fig.cap = fig_caption_7, out.extra = "", fig.pos="h"}
# transform dataframe of active/passive means to long
data_willingness_long <- data_willingness %>% 
  pivot_longer(cols = c(Active, Passive), names_to = "effort", values_to = "willingness") %>% 
  select(-willingness_diff)

data_willingness_long %>% 
  # bin continuous means, keep numeric
  mutate(willingness_binned = as.numeric(cut(willingness, breaks = c(-2.5, -1.5, -0.5, 0.5, 1.5, 2.5)))) %>% 
  # change 1 to 5 scale to -2 to 2
  mutate(willingness_binned = willingness_binned - 3) %>% 
  ggplot(aes(x = willingness_binned, y = ..prop.., group = effort, fill = effort)) +
  geom_bar(color = "black") +
  facet_wrap(~ effort, ncol = 1) +
  theme_classic() +
  labs(y = "Proportion",
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2),
                   labels = c("Strongly disagree", "Disgree", "Undecided", "Agree", "Strongly agree")) +
  scale_fill_manual(values = c("#FFDEDE", "#DBF8FF")) +
  geom_vline(aes(xintercept = m), data_willingness_long %>% 
               group_by(effort) %>% 
               summarise(m = mean(willingness)), size = .705, color = c("#b44343", "#05667b")) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", size = .4, color = "#787575")
```
