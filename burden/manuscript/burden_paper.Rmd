---
title: "Acceptability of Personal Sensing among People with Alcohol Use Disorder"
shorttitle        : "Personal Sensing"

author: 
  - name          : "Kendra Wyant"
    affiliation   : "1"
  - name          : "Hannah Moshontz"
    affiliation   : "1"
  - name          : "Stephanie B. Ward"
    affiliation   : "1"
  - name          : "John J. Curtin"
    affiliation   : "1"
    corresponding : yes 
    address       : "1202 West Johnson St, Madison, WI 53706"
    email         : "jjcurtin@wisc.edu"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, University of Wisconsin - Madison"


authornote: |
  Enter author note here. 
  
  Each new line herein must be indented, like this line.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

floatsintext      : yes
figurelist        : yes
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
header-includes:
  - \raggedbottom
  - \usepackage{booktabs}
  - \definecolor{lightred}{RGB}{255,222,222}
  - \definecolor{lightblue}{RGB}{219,248,255}
  - \usepackage[justification=raggedright]{caption}
  - \usepackage{colortbl}


  
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk/manuscripts/burden", 
      "/Volumes/private/studydata/risk/manuscripts/burden")
    )
  })
bibliography: burden.bib
csl: journal-of-medical-internet-research.csl
---

```{r setup, include = FALSE}
library(here)
library(papaja)
library(knitr)
library(tidyverse)
library(kableExtra)
library(janitor)
library(corx)
library(patchwork)
library(ggtext)

knitr::opts_chunk$set(echo = FALSE)
options(knitr.kable.NA = '')
```

```{r absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_burden <- "P:/studydata/risk/data_processed/burden"
          path_shared <- "P:/studydata/risk/data_processed/shared"},
        # IOS paths
        Darwin = {
          path_burden <- "/Volumes/private/studydata/risk/data_processed/burden"
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"}
       )
```

```{r relative paths and source}
path_ana <- "burden/ana_scripts"

source(here(path_ana, "fun_burden.R"))
```


```{r burden data}
data <- vroom::vroom(here(path_burden, "acceptability.csv"), col_types = vroom::cols())

# pull out last observation for each participant
# Last available sleep monitor may be earlier than last survey date for some due 
# to discontinuation of monitor - handle separately
data_sleep <- data %>% 
  filter(!is.na(sleep_interfere)) %>% 
  group_by(subid) %>% 
  arrange(desc(date)) %>% 
  slice(1) %>% 
  ungroup() %>% 
  select(c(subid, starts_with("sleep_")))

data_last <- data %>% 
  select(-c(starts_with("sleep_"))) %>% 
  group_by(subid) %>% 
  arrange(desc(date)) %>% 
  slice(1) %>% 
  ungroup() %>% 
  full_join(data_sleep, by = c("subid")) %>% 
  select(-c(contains("wristband"), contains("carrying_phone"))) 
```

```{r screen data}
screen <- vroom::vroom(here(path_shared, "screen.csv"), col_types = vroom::cols())

# include only participants used in analyses (n = 154) - use visit dates to determine
sample_fu1 <- vroom::vroom(here(path_shared, "visit_dates.csv"), col_types = vroom::cols()) %>% 
  filter(!(is.na(followup_1)))

screen <- screen %>% 
  filter(subid %in% sample_fu1$subid)
```

```{r disc data}
# read in notes on incomplete participants
notes_incomplete <- read_csv(file.path(path_burden, "notes_discontinue.csv"), col_types = cols()) %>% 
  filter(screen != "cancelled" & screen != "no show")
```

<!-- Need to decide on terminology RE mental health, mental illness, psychatric disorders.   Currently we use mental health in the context of health generally and then psychiatric disoders.   Lets make sure we use these two terms consistently-->

<!--Density, depth, and duration.  REport for EMA in method--->

<!--Should we talk always about acceptability as the top level construct.  Do we describe how it can be measured by self report or behavior and affected by a variety of factors like burden or sensitivity-->

<!-- John, I cannot seem to improve tables any further - still need to control width, change spacing (maybe single?) to fit on a single page, and get line returns in footnote to work. I have latex line breaks and it looks like it is trying to add a new line based on weird spacing in knit file but I think it might be interfering with papaja template. Additionally, I cannot seem to color the tables as I did in word. -->  

<!-- https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_pdf.pdf-->
<!-- Kendra line return in footnotes solved, see table 3 -->
<!-- Kendra we can use kable_styling(latex_options = "scale_down") if you want to shrink tables to fit on a single page. However it doesn't work on longtable/threeparttable so those would need additional formatting. Looking at JMIR's example doc, i'm not clear that this is required, as it says they'll do it during typesetting.-->

<!-- My figures are also at a standstill - outstanding issues include placement (I can not seem to alter the float parameter to get the figures to stay in the correct manuscript place) and fig.caption parsing error where figure# appears after Figure X.-->  

<!-- @Kendra re: fig placement: Although they're now moved to end, if wanted in text, use fig.pos and (out.extra="" or out.width="x%") in chunk options. It won't respect fig.pos setting unless a) at least one of those other two is specified and b) there is enough room where you want it, so you can always start with out.width="50%" just to force them where you wanted them. -->

<!-- @Kendra re: fig.caption parsing error: 2 issues here. 1) figure names cannot have spaces or underscores, I replaced them with hyphens; 2) fig.cap must be specified for it to be inserted into latex's figure workspace.-->

<!-- @Kendra re: line breaks in figure captions: use \\linebreak and the added usepackage{caption} where it specifies allowing ragged right margin. https://mirror.math.princeton.edu/pub/CTAN/macros/latex/contrib/caption/caption-eng.pdf for further documentation -->

## Personal Sensing

**The World Health Organization's Global Observatory for eHealth has concluded that "the use of mobile and wireless technologies to support the achievement of health objectives has the potential to transform the face of health service delivery across the  globe" [@whoglobalobservatoryforehealthMHealthNewHorizons2011].** This conclusion applies to research and care for mental health as well as other traditional health services. These opportunities are now possible in part because of rapid advances in smartphone and related mobile technologies [@majumderSmartphoneSensorsHealth2019] and high levels of smartphone access across race, socioeconomic status, geographic region, and other demographic characteristics [@pewresearchcenterMobileFactSheet2021].

**Personal sensing may become an important component of these digital health advances [@healthWhatDigitalHealth2020]**. Personal sensing is a method for longitudinal measurement in situ; i.e., real-world measurement that is embedded in individuals' day to day lives [@mohrPersonalSensingUnderstanding2017;  @klasnjaExploringPrivacyConcerns2009;   @huckvaleClinicalDigitalPhenotyping2019]. Raw data streams are collected by smartphones, wearable sensors, or other smart devices. These raw data streams can consist of self-reports or more novel data streams such as geolocations, cellular communications, social media activity, or physiology. Subsequent processing can extract psychiatric or health relevant measures of thoughts, feelings, behavior, and even interpersonal interactions.


**Personal sensing is a powerful tool for mental health research**. By definition, these data are inherently longitudinal, which allows for confirmation <!--word choice--> of the temporal ordering for putative etiologic mechanisms and their effects. Longitudinal measurement is also critical for many mental health constructs that display meaningful, and often frequent, temporal variation within person (e.g., psychiatric symptoms). Measures based on personal sensing data generally have high ecological validity and low retrospective bias given that these data are collected in situ and often in real-time [@shiffmanEcologicalMomentaryAssessment2008a]. Furthermore, personal sensing can derive measures from raw data streams (e.g., in situ behavior, physiology, interpersonal interactions) that are difficult or even impossible to obtain through other traditional research measurement methods.


**Personal sensing may have even higher value in the future for clinical applications that target patient mental health care [@huckvaleClinicalDigitalPhenotyping2019; @onnelaHarnessingSmartphoneBasedDigital2016; @torousRealizingPotentialMobile2015]**. Personal sensing can collect data that may be used for preliminary screening for psychiatric disorders [@eichstaedtFacebookLanguagePredicts2018; @razaviDepressionScreeningUsing2020]. These methods can also be used to monitor psychiatric symptoms or even predict future risk for symptom reoccurrence, relapse, or other harmful behaviors (e.g., suicide attempts)[@barnettRelapsePredictionSchizophrenia2018; @chihPredictiveModelingAddiction2014; @epsteinPredictionStressDrug2020; @jashinskyTrackingSuicideRisk2013; @jacobsonPassiveSensingPrediction2020]. Personal sensing measures or risk indicators may be shared with health care providers to allow for cost effective, targeted allocation of limited mental health resources to patients with greatest or most urgent need [@quanbeckIntegratingAddictionTreatment2014]. Personal sensing has the potential to support precision mental health care by adapting and timing interventions based on characteristics of the patient and the moment in time [@nahum-shaniJustinTimeAdaptiveInterventions2018; @aggarwalAdvancingArtificialIntelligence2020; @kaiserObamaGivesEast2015]. To be clear, these applications of personal sensing are currently aspirational rather than available for clinical implementation today. However, clinical research is advancing us rapidly toward these goals [@moshontzProspectivePredictionLapses2021; @chihPredictiveModelingAddiction2014; @sheikhWearableEnvironmentalSmartphoneBased2021; @baiTrackingMonitoringMood2021].

**Ecological momentary assessment (EMA), a personal sensing method that collects brief self-reports about momentary states multiple times per day, has been used for many years in short-term longitudinal studies of psychiatric disorders**. For example, EMA research on substance use disorders has identified proximal causes and risk factors for drug craving and relapse [@morgensternEcologicalMomentaryAssessment2014; @fronkStressAllostasisSubstance2020; @schultzStressorelicitedSmokingCravingInpress]. It has also characterized the time course and nature of drug withdrawal [@piaseckiSmokingWithdrawalDynamics2003a; @mccarthyLifeQuittingSmoking2006]. EMA research on major depressive and bipolar disorders has identified predictors of daily mood fluctuations, documented treatment efficacy, and been paired with other human neuroscience methods to explore biological mechanisms [@aanhetrotMoodDisordersEveryday2012]. Much of this research could not have been accomplished with other measurement methods.

**More recently, mental health relevant research using personal sensing of raw data streams other than self report is emerging**. This includes methods to sense geolocation [@epsteinPredictionStressDrug2020; @palmiusDetectingBipolarDepression2016; @moshontzProspectivePredictionLapses2021], cellular communications [@jacobsonDigitalBiomarkersSocial2020; @wangExaminingCorrelationDepression2021; @razaviDepressionScreeningUsing2020; @baiTrackingMonitoringMood2021; @moshontzProspectivePredictionLapses2021], sleep [@baiTrackingMonitoringMood2021], and physiology [@kleimanUsingWearablePhysiological2019; @kleimanCanPassiveMeasurement2021] as examples. These alternative personal sensing methods provide benefits and opportunities not possible with EMA. For example, many of these data streams can be sensed passively such that they have very low assessment burden. This may allow their use for long-term longitudinal monitoring of patients that would not be feasible with EMA, which requires more active effort for data collection. These data streams can also support measures that are not possible by self report.

**Clinical research with these alternative, often more passively sensed, data streams is still nascent**.  This research has predominately involved "proof-of-concept" studies that typically include only healthy controls or other convenience samples rather than patients with psychiatric disorders [@razaviDepressionScreeningUsing2020; @jacobsonDigitalBiomarkersSocial2020; @wangExaminingCorrelationDepression2021].  It has also often used very small sample sizes and/or short monitoring periods [@palmiusDetectingBipolarDepression2016; @jacobsonDigitalBiomarkersSocial2020; @kleimanUsingWearablePhysiological2019; @kleimanCanPassiveMeasurement2021].  Recent reviews of this emerging literature have highlighted gaps in reporting on participant exclusions, attrition, and compliance that are necessary to assess selection biases and feasibility of these more novel personal sensing methods [@deangelDigitalHealthTools2022; @ortizAppsGapsBipolar2021; @faurholt-jepsenSmartphonebasedObjectiveMonitoring2018].


## Acceptability of Personal Sensing

**Further development and use of personal sensing necessitates better understanding of its acceptability to research participants and patients targeted for clinical applications**.  The acceptability of a personal sensing method may be influenced by the degree of active effort required from the participant or patient to collect the raw data (i.e., the method's assessment burden) and other factors (e.g., the sensitivity of the data collected).  As such, acceptability may vary across different personal sensing methods and comparisons across methods within the same individuals is thus warranted. Furthermore, comprehensive assessment of both subjective perceptions and behavioral measures (e.g., compliance) of acceptability may better anticipate potential issues for recruitment, consent, compliance, and attrition when they are used for either research or clinical applications.

**Not surprisingly, much of what is known about the acceptability of personal sensing is limited to EMA**.  Studies that have accessed participants' perceptions of EMA methods have generally concluded that it is acceptable to participants from both non-clinical and clinical samples[@stoneIntensiveMomentaryReporting2003; @kirkExposureAssessmentCurrent2013; @ramseyFeasibilityAcceptabilitySmartphone2016; @yangFeasibilityAcceptabilitySmartphoneBased2015; @moitraFeasibilityAcceptabilityPosthospitalization2017]. Similarly, participants display moderate or better compliance with respect to response rates even with relatively high sampling density (e.g., 6-9 daily assessments)[@eiseleEffectsSamplingFrequency2020; @stoneIntensiveMomentaryReporting2003; @wenComplianceMobileEcological2017]. However, these studies generally assessed participants' perceptions and compliance over short monitoring periods (i.e., 2-6 weeks).  Less is known about the use of EMA over longer duration monitoring periods (e.g., months) as would be necessary for clinical applications.

**Assessment burden may impact the relative acceptability of EMA**.  EMA assessment burden can increase as more active effort is required from participants when EMA survey density (i.e., frequency of surveys) or depth (number of items/length of surveys) increases.  However, the impact of active effort due to density and depth may not be comparable.  Increased density does not appear to robustly impact perceptions or compliance [@eiseleEffectsSamplingFrequency2020; @stoneIntensiveMomentaryReporting2003; @jonesComplianceEcologicalMomentary2019; @wrzusEcologicalMomentaryAssessment2022 but see @wenComplianceMobileEcological2017].  However, greater survey depth may decrease both perceptions of acceptability and compliance  [@eiseleEffectsSamplingFrequency2020].  Notably, these studies were again generally limited to shorter duration monitoring periods (i.e, <= 6 weeks).

**The duration of the EMA monitoring period may also increase assessment burden and affect acceptability but findings have been mixed**.  Some studies suggest that as duration increases beyond only a few weeks, compliance decreases [@wrzusEcologicalMomentaryAssessment2022; @yangFeasibilityAcceptabilitySmartphoneBased2015]. However, other studies find no changes in compliance over time [@wenComplianceMobileEcological2017; @kirkExposureAssessmentCurrent2013]. Importantly, very few studies have looked at compliance over longer duration (> 6 weeks). <!--Kendra, did any studies look at perceptions rather than just compliance?-->

**Existing research also raises some concern about perceptions and compliance with EMA protocols in patients with substance use disorders relative to other groups**.  Specifically, a recent meta-analysis confirmed decreased compliance with EMA protocols in patients with substance use disorder diagnoses vs. recreational substance users [@jonesComplianceEcologicalMomentary2019].  However,  a separate meta-analysis by [@wrzusEcologicalMomentaryAssessment2022] showed that
compliance rates did not differ between healthy, physically ill, mentally ill, and mixed samples, which suggests that compliance concerns may be limited to applications with patients with substance use disorders rather than all psychiatric disorders more generally.

**Far less is known about participants' perceptions and compliance with more passive personal sensing methods**. Some research has presented hypothetical scenarios to participants to assess their perceptions about personal sensing methods [@duncanAcceptabilitySmartphoneApplications2019; @riegerPsychiatryOutpatientsWillingness2019; @bessenyeiComfortabilityPassiveCollection2021].  Participants' willingness to share sensed data appears to vary by the data type (e.g., sleep, geolocation, social media activity).  However, it is difficult to determine how well participants' perceptions in these hypothetical scenarios would generalize to real world collection of these data. And, of course, it is impossible to measure attrition and compliance outside of explicit implementation of these sensing methods. 

**Preliminary research has begun to examine perceptions and compliance during real world use of passive personal sensing methods**. However, this research has generally been limited by small sample sizes [@lindEffortlessAssessmentRisk2018; @ben-zeevMobileBehavioralSensing2016], use of convenience samples (e.g., students) [@lindEffortlessAssessmentRisk2018;  @kirkExposureAssessmentCurrent2013; @rooksbyStudentPerspectivesDigital2019], short monitoring durations [@lindEffortlessAssessmentRisk2018; @kleimanUsingWearablePhysiological2019; @ben-zeevMobileBehavioralSensing2016; @raughDigitalPhenotypingAdherence2021], and coarse, incomplete or aggregate reporting of perceptions, compliance and related participant behaviors[@lindEffortlessAssessmentRisk2018; @ben-zeevMobileBehavioralSensing2016; @kirkExposureAssessmentCurrent2013]. These are important first efforts but more research into the feasibility of personal sensing methods is clearly warranted.    

## The Current Study

**The current study reports on the acceptability of both active and passive personal sensing methods in a sample of patients with moderate to severe alcohol use disorder**.  These participants were enrolled early in their recovery (i.e., 1 - 8 weeks after becoming abstinent) and followed for 3 months. We used active personal sensing methods to collect ecological momentary assessments, daily audio check-ins, sleep quality, and selected physiology. We used primarily passive methods to collect moment-by-moment geolocation, cellular communications logs, and text message content. We assessed participants' subjective perceptions of the acceptability each of these personal sensing methods, separately, by self report.  We also assessed participants' choices about their participation in the study at various stages in the study procedure (e.g., consent, enrollment, data collection), their choice to opt-in to provide data associated with each personal sensing method, and their reasons for discontinuation when available. Finally, for active measures, we assess their compliance with providing those raw data streams for up to three months of their study participation. We believe these data provide an unparalleled and transparent window into the feasibility of using numerous personal sensing methods with individuals with alcohol use disorder, a highly stigmatized psychiatric disorder.


# Method

## Research Transparency

We value the principles of research transparency that are essential to the robustness and reproducibility of science [@schonbrodtVoluntaryCommitmentResearch2015]. Consequently, we maximized transparency through several complementary methods. First, we reported how we determined our sample size, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]. Second, we completed a transparency checklist, which can be found in the supplement of this paper (Multimedia Appendix 1) [@aczelConsensusbasedTransparencyChecklist2019]. Third, we made the data, analysis scripts and annotated results, self-report surveys, and other study materials associated with this report publicly available [<https://osf.io/cjsvk> ].

## Participants

We recruited participants in early recovery (1 -- 8 weeks of abstinence) from AUD within the Madison area to participate in a three-month longitudinal study. Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required that participants:

1.  were 18 years of age or older,
2.  were able to write and read in English,
3.  had at least moderate AUD (\> 4 DSM-5 AUD symptoms ),
4.  were abstinent from alcohol for at least one week but no longer than two months,
5.  were willing to use a single smartphone (their personal phone or one provided by us) while enrolled in the study.

We also excluded participants if they exhibited severe symptoms of psychosis or paranoia[REF<!--KENDRA get SCL-90 citation and footnote here-->. All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

We assessed eligibility and exclusion criteria using a brief phone screen followed by a more detailed in person screening visit. One hundred and ninety-two participants met criteria for enrollment. Of these participants, 191 consented to participate in the study at the screening session and 169 subsequently enrolled in the study at the enrollment visit which occurred approximately one week later. Fifteen participants discontinued prior to the first follow-up visit at one month. The remaining 154 participants provided study measures for one (N = 14), two (N = 7) or three (N = 133) months. We provide a study participation flow chart in Figure 1.

## Procedure

We collected the study data between 2017 -- 2019 as part of a larger grant-funded parent project (RO1 AA024391). The sample size was determined based on power analyses for the aims of that project. We used all available participants for this study. Participants completed five study visits over the course of approximately three months. Participants first attended a screening visit where we determined eligibility, obtained informed consent, and collected self-report measures of individual differences (e.g., demographics, alcohol use history). We scheduled eligible and consented participants to enroll in the study approximately one week later. During this enrollment visit, we collected additional self-report and interview measures. Participants completed an additional three follow-up visits that occurred about every 30 days. We collected self-report and interview measures and downloaded cellular communications logs (text message and phone call) at these visits. Finally, we collected various raw data streams (e.g., geolocation, cellular communication logs, ecological momentary assessments) using personal sensing to monitor participants throughout the three-month study period. A full description of the procedure and data collected at each visit can be found at the study's OSF page (<https://osf.io/cjsvk>).

## Personal Sensing

Personal sensing methods can be coarsely classified as active or passive. Active personal sensing requires active effort from the participant to provide the raw data streams whereas passive personal sensing data are collected automatically (either asynchronously or continuously) with little to no effort required by the participant. Our study obtained several active signals that varied somewhat in the amount of effort required by the participant. Specifically, we used active methods to collect ecological momentary assessments, daily audio check-ins, sleep quality, and selected physiology. We used primarily passive methods to collect moment-by-moment geolocation, cellular communications logs, and text message content. More detail about each raw data stream collected by personal sensing is provided below.

### Audio Check-in

Participants recorded a diary-style audio response on their smartphone to an open-ended prompt each day following a reminder from us that was sent via text message. They responded to the prompt ("How are you feeling about your recovery today?"), which stayed the same throughout the entire study. We instructed them that their responses should be approximately 15-30 seconds in duration. These recordings were sent to us by text message.

### Ecological Momentary Assessments (EMA)

Participants completed a brief EMA four times each day following reminders from us that were sent by text message. These text messages included a link to a Qualtrics survey that was optimized for completion on their smartphone. All four EMAs included items that asked about any alcohol use that had not yet been reported, current affective state (pleasantness and arousal), greatest urge to drink alcohol since the last EMA, any pleasant or positive events and any hassles or stressful events that occurred since the last EMA, any exposure to risky situations (i.e., people, places, or things) since the last EMA. The first EMA each day asked an additional three questions about how likely participants were to encounter a risky situation, encounter a stressful event, and drink alcohol in the upcoming week. The first and last EMAs of the day were scheduled within one hour of participants' typical wake and sleep times. The other two EMAs were each scheduled randomly within the first and second halves of the participants' typical day. All EMAs were separated from each other by at least one hour.

### Sleep Quality

We collected information about participants' sleep duration, timing, and overall quality with a Beddit sleep monitor (Beddit Oy Inc., Espoo, Finland) that was placed in their beds and connected to their smartphones. We used an early version of the sleep monitor that required participants to actively start and stop the monitor when they entered and exited their bed each night and morning, respectively. These data are available on only approximately 87 participants because Beddit Oy was acquired by Apple Inc. during data collection for this study. Apple discontinued cloud support for data collection with the sleep monitor in November 2018, which prevented its further use for our remaining participants.

### Physiology

We continuously monitored participants' physiology (heart rate, electrodermal activity, skin temperature) using an early version of the Empatica E4 wristband monitor (by Empatica Inc., Boston, MA). However, this early version did not adequately support Bluetooth streaming of data to the cloud. Instead, participants had to manually connect the wristband each night to a tablet we provided to upload their data. This and other software bugs made use of the wristband too complicated for many participants. Therefore, we discontinued use of the wristband after collected data from nine participants. Given this small sample size, we did not include the wristband in our primary analyses. We do provide self-reported acceptability ratings for this signal from this small sample in Multimedia Appendix 2 (Figure S1).

### Geolocation

We continuously collected participants' moment-by-moment geolocation using location services on their smartphones in combination with commercial software that accessed these geolocation data and saved them in the cloud. At the start of the study, we used the Moves app (developed by ProtoGeo Oy, Helsinki, Finland). However, Facebook acquired ProtoGeo Oy and shut down use of the Moves app in July 2018. At this point, we switched to using the FollowMee GPS tracking mobile app (FollowMee LLC, Murphy, TX). Measurement of geolocation required only initial installation of the app by the participants. Subsequent measurement and transfer of the data to the cloud was completed automatically with no input or effort by the participant. Both apps allowed participants to temporarily disable location sharing if they deemed it necessary for short periods of time.

### Cellular Communication Logs

We collected cellular communication logs that include meta-data about smartphone communications involving both text messages and phone calls. For each communication entry, these logs include the phone number of the other party, the type of call or message (i.e., incoming, outgoing, missed, rejected), the name of the party if listed in the phone contacts, the date and time the message or call occurred, whether the log entry was read (text messages only), and the duration of the call (voice calls only). These data are saved passively on the phone with no additional input or effort on the part of the participant. We downloaded these logs from participants' phones at each one-month follow-up visit. Participants were informed that they could delete any text message or voice call log entries prior to the download if they desired. Text Message Content. We also collected the message content from participants' text messages on their smartphone. As with the logs, content from individual text messages is saved passively on the phone with no additional input or effort on the part of the participant. We downloaded text message content at each one-month follow-up visit and participants could delete text messages prior to the download. Note that we did not have a parallel method to gain access to phone call content. Thus, we had meta-data from communication logs for both text messages and phone calls but had content of the communication only for text messages.

## Measures

### Individual Differences

We collected demographic information and information relevant to participants' alcohol use and DSM-5 AUD symptoms at the screening visit (see surveys in Multimedia Appendix 1).

### Behavioral Acceptability

Coarse assessment of the acceptability of the personal sensing methods can be made based on participants' behaviors. Specifically, we assessed three categories of behavior. First, we assessed participants' choices about their participation in the study at various stages in the study procedure (e.g., consent, enrollment, data collection) and their reasons for discontinuation when available. Second, we assessed their choice to opt-in to provide data associated with each personal sensing method. Participants were allowed to participate in the study without opting-in to any specific personal sensing method other than EMA. Instead, they were paid a monthly bonus for each sensing method they chose to opt-in to that ranged from \$10-\$25. Finally, for a subset of the active measures (EMAs, daily audio check-ins), we assess their compliance with providing those raw data streams for up to three months of study participation.

### Self-reported Acceptability

To assess participants' subjective experience of the acceptability of the personal sensing methods in this study, they rated each method on three acceptability relevant dimensions (see Acceptability Survey in Multimedia Appendix 1). Specifically, participants were asked to indicate how much they agreed with each of the following three statements on a five-point bipolar scale (strongly disagree, disagree, undecided, agree, strongly agree) for the personal sensing signals:

1.  [Personal sensing method name] interfered with my daily activities.
2.  I disliked [Personal sensing method name].
3.  I would be willing to use [Personal sensing method name] for one year to help with my recovery.

The interference item was collected only for the active methods because they passive methods require no effort and therefore cannot interfere with daily activities. Dislike and willingness to use for one year were collected for all methods.

## Data Analytic Strategy

We conducted all analyses in R version 4.0.3 [@rcoreteamLanguageEnvironmentStatistical2021] using RStudio [@rstudioteamRStudioIntegratedDevelopment2020] and the tidyverse ecosystem of packages [@wickhamWelcomeTidyverse2019]. <!-- Update R version-->

### Behavioral Acceptability

We provide descriptive data on participants' choices about their participation in the study at various stages in the study procedure (e.g., consent, enrollment, data collection). We provide both coarse and more granular tabulation of their reasons for discontinuation when available. We report the percentages of participants who opted-in to provide us with the raw data streams we collected via personal sensing. We also report compliance measures for two of the active personal sensing methods (EMAs and daily audio check-ins). Formal measures of compliance could not be calculated for geolocation, cellular communication log, and sleep quality because it was not possible to distinguish between missing data due to compliance (e.g., deleting phone calls or messages, turning off location services on the phone, failing to start sleep monitoring at bedtime) and valid reasons (no calls made during the day, no movement, erratic sleep patterns)

### Self-reported Acceptability

Participants responded to the three self-report items related to acceptability (interference, dislike, and willingness to use for one year) on a five-point bipolar scale (strongly disagree, disagree, undecided, agree, strongly agree). We retained these ordinal labels for visual display of these data in figures but ordered the labels such that higher scores represent greater acceptability (i.e., strongly agree for willingness to use for one year and strongly disagree for interference and dislike. For analyses, we re-coded these items to a numeric scale ranging from -2 to 2 with 0 representing the neutral (undecided) midpoint and higher scores representing greater acceptability.

Participants responded to these items at each monthly follow-up visit. Therefore, participants had up to three responses for each item depending on when they ended their participation. We analyzed their last available response in our primary analyses to allow us to include all participants and to represent their final perception of each personal sensing signal. However, mean responses across each time point remained relatively constant for all signals (see Figure S2 in Multimedia Appendix 2).

To detect polarized perceptions of the personal sensing signals (i.e., mean responses to any items that are different from 0/undecided), we conducted one-sample t-tests for the three self-report items for each personal sensing signal. To examine relative perceptions of the signals, we compared perceptions of the active vs. passive categories of signals using within-sample t-tests for dislike and willingness to use for one year . We also report pairwise comparisons among all personal sensing signals using within-sample t-tests for each of the three self-report items in Table S2 in Multimedia Appendix 2.

Finally, we conducted two analyses to examine the consistency of perceptions across personal sensing signals (e.g., do participants who dislike one signal also dislike the other signals?). First, we calculated bivariate correlations among the personal sensing signals for each item. Second, we calculated intraclass correlations (single, case 3 [@shroutIntraclassCorrelationsUses1979]) separately for each item to quantify aggregate agreement in participant perceptions across the signals.

# Results

## Participant Characteristics

One hundred and fifty-four participants completed at least one monthly follow-up visit and provided self-report acceptability ratings for interference, dislike, and willingness to use for one year. These participants serve as our primary sample for our analyses. Table 1 presents demographic information these participants. Table 2 characterizes alcohol use and AUD-relevant information for these participants. We compared demographics and AUD information for participants who were included in the analyses vs. eligible participants who did not provide study measures (i.e., did not enroll or discontinued prior to the first month follow-up; N = 36 ) and found no significant differences (see Multimedia Appendix 2 for more detail on these analyses).

\newpage

```{r table-1-code}
dem <- screen %>% 
  summarise(mean = as.character(round(mean(dem_1, na.rm = TRUE), 1)),
            SD = as.character(round(sd(dem_1, na.rm = TRUE), 1))) %>% 
  mutate(var = "Age",
         n = as.numeric(""),
         perc = as.numeric("")) %>% 
  select(var, n, perc, everything()) %>% 
  full_join(screen %>% 
  select(var = dem_2) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_3) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("American Indian/Alaska Native", "Asian", "Black/African American",
                           "White/Caucasian", "Other/Multiracial")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_4) %>% 
  mutate(var = case_when(var == "No, I am not of Hispanic, Latino, or Spanish origin" ~ "No",
                         TRUE ~ "Yes"),
         var = fct_relevel(factor(var, c("Yes", "No")))) %>% 
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_5) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Less than high school or GED degree", "High school or GED", 
                           "Some college", "2-Year degree", "College degree", "Advanced degree")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  select(var = dem_6, dem_6_1) %>% 
  mutate(var = case_when(dem_6_1 == "Full-time" ~ "Employed full-time",
                         dem_6_1 == "Part-time" ~ "Employed part-time",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Employed full-time", "Employed part-time", "Full-time student",
                           "Homemaker", "Disabled", "Retired", "Unemployed", 
                           "Temporarily laid off, sick leave, or maternity leave",
                           "Other, not otherwise specified")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>% 
  full_join(screen %>% 
  summarise(mean = as.character(round(mean(dem_7, na.rm = TRUE), 0)),
            SD = as.character(round(sd(dem_7, na.rm = TRUE), 0))) %>% 
  mutate(var = "Income",
        n = as.numeric(""),
        perc = as.numeric("")) %>% 
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>% 
  full_join(screen %>% 
  select(var = dem_8) %>% 
  mutate(var = case_when(var == "Never Married" ~ "Never married",
                         TRUE ~ var)) %>% 
  mutate(var = fct_relevel(factor(var, 
                         c("Never married", "Married", "Divorced", "Separated",
                           "Widowed")))) %>%
  group_by(var) %>% 
  summarise(n = n()) %>% 
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc"))
```


```{r table-1}
dem %>% 
  kbl(booktabs = TRUE,
      caption = "Demographics",
      col.names = c("", "n", "%", "M", "SD"),
      align = c("l", "c", "c", "c", "c"),
      digits = 1,
      longtable = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>% 
  row_spec(row = 0, align = "c", italic = TRUE) %>% 
  pack_rows("Sex", 2, 3, bold = FALSE) %>% 
  pack_rows("Race", 4, 8, bold = FALSE) %>%
  pack_rows("Hispanic, Latino, or Spanish Origin", 9, 10, bold = FALSE) %>%
  pack_rows("Education", 11, 16, bold = FALSE) %>%
  pack_rows("Employment", 17, 25, bold = FALSE) %>%
  pack_rows("Marital Status", 27, 31, bold = FALSE) %>% 
  footnote("N = 154")
```

\newpage

```{r table-2-code}
auh <- screen %>%
  summarise(mean = mean(auh_1, na.rm = TRUE),
            SD = sd(auh_1, na.rm = TRUE)) %>%
  mutate(var = "Age of first drink",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_2, na.rm = TRUE),
            SD = sd(auh_2, na.rm = TRUE)) %>%
  mutate(var = "Age of regular drinking",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_3, na.rm = TRUE),
            SD = sd(auh_3, na.rm = TRUE)) %>%
  mutate(var = "Age at which drinking became problematic",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_4, na.rm = TRUE),
            SD = sd(auh_4, na.rm = TRUE)) %>%
  mutate(var = "Age of first quit attempt",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  summarise(mean = mean(auh_5, na.rm = TRUE),
            SD = sd(auh_5, na.rm = TRUE)) %>%
  mutate(var = "Number of Quit Attempts",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  select(var = auh_6_1) %>%
  mutate(var = case_when(var == "Long-Term Residential Treatment (more than 6 months)" ~ "Long-term residential (6+ mos.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_2) %>%
  mutate(var = case_when(var == "Short-Term Residential Treatment (less than 6 months)" ~ "Short-term residential (< 6 mos.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_3) %>%
  mutate(var = case_when(var == "Outpatient Treatment" ~ "Outpatient",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_4) %>%
  mutate(var = case_when(var == "Individual Counseling" ~ "Individual counseling",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_5) %>%
  mutate(var = case_when(var == "Group Counseling" ~ "Group counseling",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_6) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_6_7) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = auh_7) %>%
  mutate(var = fct_relevel(factor(var, c("Yes", "No")))) %>%
  group_by(var) %>%
  summarise(n = n()) %>%
  mutate(perc = (n / sum(n)) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  mutate(across(dsm5_1:dsm5_11, ~ recode(., "No" = 0, "Yes" = 1))) %>%
  rowwise() %>%
  # calculate dsm5 score by adding up dsm5_1 through dsm5_11
  mutate(dsm5_total = sum(c(dsm5_1, dsm5_2, dsm5_3, dsm5_4, dsm5_5, dsm5_6, dsm5_7,
                            dsm5_8, dsm5_9, dsm5_10, dsm5_11))) %>%
  ungroup() %>%
  summarise(mean = mean(dsm5_total),
            SD = sd(dsm5_total)) %>%
  mutate(var = "AUD DSM-5 Symptom Count",
        n = as.numeric(""),
        perc = as.numeric("")) %>%
  select(var, n, perc, everything()), by = c("var", "n", "perc", "mean", "SD")) %>%
  full_join(screen %>%
  select(var = assist_1_1) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Tobacco products (cigarettes, chewing tobacco, cigars, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_2) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Cannabis (marijuana, pot, grass, hash, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_3) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Cocaine (coke, crack, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_4) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Amphetamine type stimulants (speed, diet pills, ecstasy, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_5) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Inhalants (nitrous, glue, petrol, paint thinner, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_6) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Sedatives or sleeping pills (Valium, Serepax, Rohypnol, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_7) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Hallucinogens (LSD, acid, mushrooms, PCP, Special K, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc")) %>%
  full_join(screen %>%
  select(var = assist_1_8) %>%
  filter(var == "Yes") %>%
  mutate(var = case_when(var == "Yes" ~ "Opioids (heroin, morphine, methadone, codeine, etc.)",
                         TRUE ~ var)) %>%
  group_by(var) %>%
  drop_na() %>%
  summarise(n = n()) %>%
  mutate(perc = (n / 154) * 100), by = c("var", "n", "perc"))
```

```{r table-2}
auh %>% 
  kbl(booktabs = TRUE,
      caption = "Alcohol Related Characteristics for the Sample",
      col.names = c("", "n", "%", "M", "SD"),
      align = c("l", "c", "c", "c", "c"),
      digits = 1,
      longtable = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>% 
  row_spec(row = 0, align = "c", italic = TRUE) %>% 
  pack_rows("AUD Milestones", 1, 4, bold = FALSE) %>%   #Kendra is this supposed to be 1-5?
  pack_rows("Types of Treatment (Can choose more than 1)", 6, 12, bold = FALSE) %>%
  pack_rows("Received Medication for AUD", 13, 14, bold = FALSE) %>%
  pack_rows("Lifetime Drug Use", 16, 23, bold = FALSE) %>%
  footnote("N = 154")
```

\newpage

## Behavioral Measures of Acceptability

### Participation

Figure 1 shows participant attrition/discontinuation at each phase of the study. Of the 192 eligible participants at screening, only one did not consent after hearing the details of the study. Enrollment occurred during a second visit one week later. A total of 169 participants completed enrollment.

Study phases where we lost eligible participants are highlighted in red in Figure 1. In addition, we coarsely tabulated participants stated reasons for discontinuation as due to acceptability, other reasons, or unknown in this figure. Eleven participants (5.7%) were lost due to acceptability-relevant causes (e.g., no longer interested, non-compliance with data, or citing study demands as too burdensome). Other reasons for discontinuation not related to the acceptability of the signals include circumstances such as moving or no longer wishing to abstain from alcohol. It should be noted that thirty-one participants (16.1%) were lost to follow-up such that we had no information about their reasons for discontinuation. We provide more granular tabulation of these reasons for discontinuation in Table 3. <!-- Denominator for above percentages is 192-->

### Opt-In and Compliance

All participants (100%) opted-in to provide data for all personal sensing methods. Daily compliance rates were relatively high for EMAs such that 94% of participants completed at least one of the four EMAs every day. On average, participants completed 3.1 EMAs every day. The overall compliance rate for all requested EMAs was 77%. Participants' completion rate for the daily audio check-in was 53%. That is, of their total days on study, participants completed an audio check-in on approximately half of them.

\newpage

```{r table-3-code}
notes_incomplete <- notes_incomplete %>%
  filter(screen != "no consent") %>%
  mutate(characterize = case_when(characterize == "no_transportation" ~ "No longer has transportation",
                                  characterize == "not_sober" ~ "No longer sober or no longer wishes to abstain from alcohol",
                                  characterize == "rescheduled" ~ "Rescheduled multiple times before cancelling/no showing",
                                  characterize == "treatment" ~ "No longer sober or no longer wishes to abstain from alcohol",
                                  characterize == "compliance" ~ "Noncompliance with providing data",
                                  characterize == "lapse" ~ "No longer sober or no longer wishes to abstain from alcohol",
                                  characterize == "cell_service" ~ "Cell service shut off",
                                  characterize == "move" ~ "Moved out of state",
                                  characterize == "study_demands" ~ "Cited study demands as too burdensome",
                                  characterize == "Unreachable" ~ "Unknown",
                                  characterize == "Unspecified concerns from staff" ~ "Staff concerns",
                                  characterize == "inappropriate behavior" ~ "Staff concerns",
                                  characterize == "mental health" ~ "Mental health concerns",
                                  characterize == "no aud" ~ "Does not meet criteria for moderate or severe AUD",
                                  characterize == "phone" ~ "Ineligible phone",
                                  is.na(characterize) ~ "Unknown",
                                  TRUE ~ characterize))
```



```{r table-3-code-2}
a <- notes_incomplete %>% 
  filter(screen != "ineligible") %>% 
  filter(intake != "complete" | is.na(intake)) %>% 
  filter(intake != "ineligible" | is.na(intake)) %>% 
  group_by(characterize) %>% 
  summarise(n = n()) %>% 
  mutate(percent = n/sum(n)) %>% 
  adorn_totals("row") 

b <- notes_incomplete %>% 
  filter(screen != "ineligible") %>% 
  filter(completed_1month == "no") %>% 
  filter(intake == "complete") %>% 
  group_by(characterize) %>% 
  summarise(n = n()) %>% 
  mutate(percent = n/sum(n)) %>% 
  adorn_totals("row") 

c <- notes_incomplete %>% 
  filter(screen != "ineligible") %>% 
  filter(completed_1month == "yes") %>% 
  group_by(characterize) %>% 
  summarise(n = n()) %>% 
  mutate(percent = n/sum(n)) %>% 
  adorn_totals("row") 

footnote_table_3 <- "Bolded rows depict acceptability-related discontinuation." 
footnote_table_3a <- "These participants are labeled as `Not Enrolled' in Figure 1." 
footnote_table_3b <- "These participants are labeled as `Discontinued' in Figure 1." 
footnote_table_3c <- "These participants are labeled as `Participated through 1st month follow-up' or `Participated through 2nd month follow-up' in Figure 1."
```
<!-- Kendra two notes here. First, linebreaks don't work in footnotes, but you can have separate footnotes that appear on separate lines. See below for syntax. Also, I fixed the open single quote not rendering correctly by replacing it with a backtick-->


```{r table-3}
a %>% 
  bind_rows(b, c) %>% 
  kbl(booktabs = TRUE,
      col.names = c("", "n", "%"), 
      caption = "Characterization of Discontinued Participants",
      align = c("l", "c", "c"),
      digits = 2,
      longtable = TRUE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>%
  row_spec(row = 0, align = "c", italic = TRUE) %>%
  row_spec(row = 3, bold = TRUE) %>% 
  row_spec(row = 8, bold = TRUE) %>% 
  row_spec(row = 10, bold = TRUE) %>% 
  row_spec(row = 15, bold = TRUE) %>% 
  row_spec(row = 17, bold = TRUE) %>% 
  row_spec(row = 19, bold = TRUE) %>% 
  pack_rows("Eligible and consented participants discontinued prior to completing enrollment$^a$", 1, 6, bold = FALSE, escape = FALSE) %>% 
  pack_rows("Enrolled participants discontinued prior to first month follow-up$^b$", 7, 13, bold = FALSE, escape = FALSE) %>% 
  pack_rows("Enrolled participants discontinued after the first month follow-up$^c$", 14, 22, bold = FALSE, escape = FALSE) %>% 
  footnote(general=footnote_table_3, alphabet = c(footnote_table_3a, footnote_table_3b, footnote_table_3c), threeparttable = TRUE, escape = FALSE)
```
<!-- General footnotes are not preceded by anything, alphabet by a letter, you can also specify symbol=c(). Anything in the c() will appear on separate lines with the specified initial marker-->
\newpage

## Self-reported Acceptability

```{r one sample t tests}
# interference
int_sleep <- broom::tidy(lm(sleep_interfere ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sleep_interfere, na.rm = TRUE),
         d = mean(data_last$sleep_interfere, na.rm = TRUE)/sd)
int_audio <- broom::tidy(lm(audio_checkin_interfere ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$audio_checkin_interfere, na.rm = TRUE),
         d = mean(data_last$audio_checkin_interfere, na.rm = TRUE)/sd)
int_ema <- broom::tidy(lm(daily_survey_interfere ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$daily_survey_interfere, na.rm = TRUE),
         d = mean(data_last$daily_survey_interfere, na.rm = TRUE)/sd)

# dislike
dis_sleep <- broom::tidy(lm(sleep_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sleep_dislike, na.rm = TRUE),
         d = mean(data_last$sleep_dislike, na.rm = TRUE)/sd)
dis_audio <- broom::tidy(lm(audio_checkin_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$audio_checkin_dislike, na.rm = TRUE),
         d = mean(data_last$audio_checkin_dislike, na.rm = TRUE)/sd)
dis_ema <- broom::tidy(lm(daily_survey_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$daily_survey_dislike, na.rm = TRUE),
         d = mean(data_last$daily_survey_dislike, na.rm = TRUE)/sd)
dis_geolocation <- broom::tidy(lm(location_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$location_dislike, na.rm = TRUE),
         d = mean(data_last$location_dislike, na.rm = TRUE)/sd)
dis_logs <- broom::tidy(lm(all_logs_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$all_logs_dislike, na.rm = TRUE),
         d = mean(data_last$all_logs_dislike, na.rm = TRUE)/sd)
dis_text_content <- broom::tidy(lm(sms_content_dislike ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sms_content_dislike, na.rm = TRUE),
         d = mean(data_last$sms_content_dislike, na.rm = TRUE)/sd)

# willingness
use_audio <- broom::tidy(lm(audio_checkin_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$audio_checkin_1year, na.rm = TRUE),
         d = mean(data_last$audio_checkin_1year, na.rm = TRUE)/sd)
use_sleep <- broom::tidy(lm(sleep_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sleep_1year, na.rm = TRUE),
         d = mean(data_last$sleep_1year, na.rm = TRUE)/sd)
use_ema <- broom::tidy(lm(daily_survey_4_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$daily_survey_4_1year, na.rm = TRUE),
         d = mean(data_last$daily_survey_4_1year, na.rm = TRUE)/sd)
use_geolocation <- broom::tidy(lm(location_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$location_1year, na.rm = TRUE),
         d = mean(data_last$location_1year, na.rm = TRUE)/sd)
use_logs <- broom::tidy(lm(all_logs_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$all_logs_1year, na.rm = TRUE),
         d = mean(data_last$all_logs_1year, na.rm = TRUE)/sd)
use_text_content <- broom::tidy(lm(sms_content_1year ~ 1, data = data_last)) %>% 
  mutate(sd = sd(data_last$sms_content_1year, na.rm = TRUE),
         d = mean(data_last$sms_content_1year, na.rm = TRUE)/sd)
```



```{r active effort t test}
data_dislike <- data_last %>% 
  # get subject level means for active and passive measures
  group_by(subid) %>% 
  summarise(Active = mean(c(daily_survey_dislike, audio_checkin_dislike), na.rm = TRUE),
            Passive = mean(c(location_dislike, all_logs_dislike, sms_content_dislike), na.rm = TRUE),
            dislike_diff = Passive - Active) 

model_dislike <- lm(dislike_diff ~ 1, data = data_dislike)

data_willingness <- data_last %>% 
  # get subject level means for active and passive measures
  group_by(subid) %>% 
  summarise(Active = mean(c(daily_survey_4_1year, audio_checkin_1year), na.rm = TRUE),
            Passive = mean(c(location_1year, all_logs_1year, sms_content_1year), na.rm = TRUE),
            willingness_diff = Passive - Active) 

model_willingness <- lm(willingness_diff ~ 1, data = data_willingness)
```


### Interference

Figure 2 shows the distribution of participant responses to the self-reported acceptability item about interference. Responses are grouped by personal sensing data stream and the amount of active effort required to collect it. One sample t-tests revealed that each mean interference score (depicted as the solid red line) was significantly more acceptable than 0 (gray dashed line indicating undecided). Table 4 reports the summary statistics for each one sample t-test and pairwise correlations between personal sensing data streams. An ICC (type 3) showed that, on average, interference ratings were moderately consistent across the data streams, ICC = .42, 95% CI = [.31 - .53].

### Dislike.

Figure 3 shows the distribution of participant responses to the self-reported acceptability item about dislike by personal sensing data stream and amount of active effort required to collect it. One sample t-tests revealed that each mean dislike score was significantly more acceptable than 0. Table 5 reports the summary statistics for each one sample t-test and pairwise correlations between personal sensing data streams. An ICC (type 3) showed that, on average, the dislike ratings were moderately consistent across the data streams, ICC = .42, 95% CI = [.35 - .48].

We also assessed the effect of active effort on dislike ratings (Figure 4). We conducted a paired samples t-test to compare the average dislike for active (audio check-in, EMA) vs. passive (geolocation, cellular communication logs, text message content) methods. Participants did not significantly differ in their dislike of active vs. passive methods, $t(153)$ = `r round(broom::tidy(summary(model_dislike))$statistic, 2)`, $p$ = `r round(broom::tidy(summary(model_dislike))$p.value, 2)`, $d$ = `r round((mean(data_dislike$Passive) - mean(data_dislike$Active)) / sd(data_dislike$dislike_diff), 2)`. 

### Willingness to Use for One Year

Figure 5 shows the distribution of participant responses to the self-reported acceptability item about willingness to use for one year for each personal sensing data stream. One sample t-tests revealed that each mean willingness score was significantly more acceptable than 0. Table 6 reports the summary statistics for each one sample t-test and pairwise correlations between personal sensing data streams. An ICC (type 3) showed that, on average, the willingness ratings were moderately consistent across the data streams, ICC = .52, 95% CI = [.46 - .58].

We also assessed the effect of active effort on willingness ratings (Figure 6). We conducted a paired samples t-test of average the average willingness to use for one year for active (audio check-in, EMA) vs. passive (geolocation, cellular communication logs, text message content) signals. Participants reported higher acceptability with respect to willingness for passive data streams ($M$ = `r round(mean(data_willingness$Passive), 1)`, $SD$ = `r round(sd(data_willingness$Passive), 1)`) relative to active data streams ($M$ = `r round(mean(data_willingness$Active), 1)`, $SD$ = `r round(sd(data_willingness$Active), 1)`), $t(153)$ = `r round(broom::tidy(summary(model_willingness))$statistic, 2)`, $p$ = `r round(broom::tidy(summary(model_willingness))$p.value, 2)`, $d$ = `r round((mean(data_willingness$Passive) - mean(data_willingness$Active)) / sd(data_willingness$willingness_diff), 2)`. 


```{r}
footnote_table_4 <- "Initial columns indicate bivariate correlations among data streams. Final columns represent the number of participants (N), mean and standard deviation (M, SD), t-statistic (t) and Cohens d Effect size (d) for the one sample t-tests against 0 (undecided).  Higher values represent higher acceptability.  Active methods are displayed in red and passive methods are displayed in blue."
footnote_table_4a <- "p < .05"
```

```{r table-4}
corrplot_int <- data_last %>% 
  select(`Audio Check-in` = audio_checkin_interfere,
         `EMA` = daily_survey_interfere,
         `Sleep Quality` = sleep_interfere) %>%
  corx(triangle = "lower",
       stars = c(0.05),
       method = "pearson",
       describe = c(`$M$` = mean, `$SD$` = sd))

corrplot_int <- as_tibble(corrplot_int, rownames = " ")
corrplot_int[corrplot_int == " - " ] <- "--"

corrplot_int <- corrplot_int %>% 
  mutate(`$N$` = c(154, 154, 87)) %>% 
  mutate(`$t$` = c(str_c(round(int_audio$statistic, 2), "*"), 
                   str_c(round(int_ema$statistic, 2), "*"), 
                   str_c(round(int_sleep$statistic, 2), "*"))) %>% 
  mutate(`$d$` = (c(round(int_audio$d, 2), round(int_ema$d, 2), round(int_sleep$d, 2)))) %>% 
  select(` `, `1`, `2`, `$N$`, `$M$`, `$SD$`, everything())

corrplot_int %>% 
  kbl(align = c("l", "c", "c", "c", "c", "c", "c", "c"),
      caption = "KENDRA ADD CAPTION otherwise will not appear as table",
      booktabs = TRUE,
      escape = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position")) %>% 
  pack_rows("Active", 1, 3) %>% 
  row_spec(1:3, background = "lightred") %>%
  footnote(general = footnote_table_4, symbol = footnote_table_4a, threeparttable = TRUE, escape = FALSE)
```



```{r}
footnote_table_5 <- "Initial columns indicate bivariate correlations among data streams. Final columns represent the number of participants (N), mean and standard deviation (M, SD), t-statistic (t) and Cohens d Effect size (d) for the one sample t-tests against 0 (undecided).  Higher values represent higher acceptability. Active methods are displayed in red and passive methods are displayed in blue."
footnote_table_5a <- "p < .05"
```

```{r table-5}
corrplot_dis <- data_last %>% 
  select(`Audio Check-in` = audio_checkin_dislike,
         `EMA` = daily_survey_dislike,
         `Sleep Quality` = sleep_dislike,
         `Geolocation` = location_dislike,
         `Cellular Communication Logs` = all_logs_dislike,
         `Text Message Content` = sms_content_dislike) %>% 
  corx(triangle = "lower",
       stars = c(0.05),
       method = "pearson",
       describe = c(`$M$` = mean, `$SD$` = sd))

corrplot_dis <- as_tibble(corrplot_dis, rownames = " ")
corrplot_dis[corrplot_dis== " - " ] <- "--"

corrplot_dis <- corrplot_dis %>% 
  mutate(`$N$` = c(154, 154, 87, 154, 154, 154)) %>% 
  mutate(`$t$` = c(str_c(round(dis_audio$statistic, 2), "*"), 
                   str_c(round(dis_ema$statistic, 2), "*"), 
                   str_c(round(dis_sleep$statistic, 2), "*"),
                   str_c(round(dis_geolocation$statistic, 2), "*"), 
                   str_c(round(dis_logs$statistic, 2), "*"), 
                   str_c(round(dis_text_content$statistic, 2), "*"))) %>% 
  mutate(`$d$` = (c(round(dis_audio$d, 2), round(dis_ema$d, 2), round(dis_sleep$d, 2),
                    round(dis_geolocation$d, 2), round(dis_logs$d, 2), round(dis_text_content$d, 2)))) %>% 
  select(` `, `1`, `2`, `3`, `4`, `5`, `$N$`, `$M$`, `$SD$`, everything())

corrplot_dis %>% 
  kbl(align = c("l", "c", "c", "c", "c", "c", "c", "c"),
      caption = "KENDRA ADD CAPTION otherwise will not appear as table",
      booktabs = TRUE,
      escape = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position", "scale_down")) %>% 
  pack_rows("Active", 1, 3) %>%
  pack_rows("Passive", 4, 6) %>% 
  row_spec(1:3, background = "lightred") %>%
  row_spec(4:6, background = "lightblue") %>%
  footnote(general = footnote_table_5, symbol = footnote_table_5a, threeparttable = TRUE, escape = FALSE)
```

```{r}
footnote_table_6 <- "Initial columns represent bivariate correlations among signals. Final columns represent the number of participants (N), mean acceptability score (M), standard deviation (SD), and t statistic (t) and Cohens d Effect size (d) for a one sample t-test centered around a neutral/ambivalent score of 0 on a 5-point bipolar scale. Higher values represent higher acceptability. Active methods are displayed in red and passive methods are displayed in blue."
footnote_table_6a <- "p < .05"
```  
<!-- Kendra footnote_table_6 is too long. It won't properly wrap the next footnote to a new line if the first footnote is over 4 lines (about 386 characters).  -->


```{r table-6}
corrplot_will <- data_last %>% 
  select(`Audio Check-in` = audio_checkin_1year,
         `EMA` = daily_survey_4_1year,
         `Sleep Quality` = sleep_1year,
         `Geolocation` = location_1year,
         `Cellular Communication Logs` = all_logs_1year,
         `Text Message Content` = sms_content_1year) %>% 
  corx(triangle = "lower",
       stars = c(0.05),
       method = "pearson",
       describe = c(`$M$` = mean, `$SD$` = sd))

corrplot_will <- as_tibble(corrplot_will, rownames = " ")
corrplot_will[corrplot_will== " - " ] <- "--"

corrplot_will <- corrplot_will %>% 
  mutate(`$N$` = c(154, 154, 87, 154, 154, 154)) %>% 
  mutate(`$t$` = c(str_c(round(use_audio$statistic, 2), "*"),
                   str_c(round(use_ema$statistic, 2), "*"), 
                   str_c(round(use_sleep$statistic, 2), "*"),
                   str_c(round(use_geolocation$statistic, 2), "*"), 
                   str_c(round(use_logs$statistic, 2), "*"), 
                   str_c(round(use_text_content$statistic, 2), "*"))) %>% 
  mutate(`$d$` = (c(round(use_audio$d, 2), round(use_ema$d, 2), round(use_sleep$d, 2),
                    round(use_geolocation$d, 2), round(use_logs$d, 2), round(use_text_content$d, 2)))) %>% 
  select(` `, `1`, `2`, `3`, `4`, `5`, `$N$`, `$M$`, `$SD$`, everything())

corrplot_will %>% 
  kbl(align = c("l", "c", "c", "c", "c", "c", "c", "c", "c", "c"),
      caption = "KENDRA ADD CAPTION otherwise will not appear as table",
      booktabs = TRUE,
      escape = FALSE) %>% 
  kable_styling(latex_options = c("HOLD_position", "scale_down")) %>% 
  pack_rows("Active", 1, 3) %>% 
  pack_rows("Passive", 4, 6) %>% 
  row_spec(1:3, background = "lightred") %>%
  row_spec(4:6, background = "lightblue") %>%
  footnote(general = footnote_table_6, symbol = footnote_table_6a, threeparttable = TRUE, escape = FALSE)
```

\newpage


# Discussion


## Behavioral Acceptability

Overall, participants showed behaviors consistent with acceptance of our personal sensing methods. Only one participant did not consent to the study and all enrolled participants agreed to provide us their raw data streams for each of the actively (EMA, audio check-in, sleep) and passively (geolocation, cellular communication logs, text message content) sensed measures.

Participants were willing to opt-in to all passive methods despite the sensitive nature of the raw data streams being collected. Participants concerned about sharing passively sensed data would likely have had these concerns from the beginning (i.e., resulting in not consenting or not opting-in to sensitive personal sensing methods). As a result, any drop-out is likely more indicative of burden from personal sensing methods higher in active effort then from measures high in sensitivity.

Behavioral acceptability was a bit more nuanced for our active methods. Compliance differed based on active effort and type of personal sensing measure. Participants had low overall compliance for the audio check-in. If this type of personal sensing method is desirable, researchers and clinicians may find better compliance with less frequent prompts (i.e., less than daily). Participants did moderately well with overall compliance for the four-time daily EMA (77%). This falls below the 80% recommended compliance rate but is quite typical when working with substance use populations [@jonesComplianceEcologicalMomentary2019; @stoneCapturingMomentarySelfreport2002]. We also note that our study is longer than the average EMA study (spanning three months instead of 2-4 weeks) and this could account for a lower compliance rate. As a result, we can imagine compliance being even lower if participants were asked to provide EMA data for one year. Participants were able to comply with one-time daily EMA at high rates (94%). Thus, fewer prompts may be needed to maintain high compliance rates over longer durations.

## Self-reported Acceptability

With respect to the self-reported measures of acceptability (interference, dislike, willingness to use for one year), all personal sensing methods were rated to be on the acceptable side of neutral. So, it appears participants found these methods to be acceptable (or at least not unacceptable). More importantly, these findings suggest participants are willing to use these methods for up to a year if it is helping their recovery.

We found mild differences in self-reported acceptability for active and passive personal sensing methods. Active and passive methods were viewed equally acceptable in terms of disliking the methods. For willingness to use the methods for one year, we found a significant, but weak, effect. Participants reported they were more willing to use passive methods. This difference is not surprising. Methods requiring high active effort from the participant can be burdensome making it difficult to maintain long-term engagement with the personal sensing platform.

This self-reported difference in willingness to use active and passive methods was further reflected in compliance. Despite participants self-reporting the audio check-in to be acceptable, we found poor daily compliance. Likewise, we found EMA compliance to much higher if the threshold was lowered to a one-time daily method. Additionally, when asked about a one-time daily method, participants self-reported that they would much more likely to use a one-time daily EMA method compared to a four-time EMA method for one year.

Finally, across active and passive measures, we did see some agreement in terms of self-reported acceptability. ICC scores ranging from .42 - .52 suggest some shared variance across measures. However, there was also some individual variation from one measure to the next . <!-- Kendra: Not entirely sure how to frame this-->

## Active Effort and Sensitivity

We labeled our personal sensing methods as active or passive based on the amount of active effort our specific methods required. Small changes in a personal sensing method can shift where it falls on this active-passive continuum. For example, an EMA method will never be passive but by altering the length or prompt frequency it may become more or less active. On the other hand, our measure of sleep was on the low side of the active continuum, but with advances in technology passive methods for sensing sleep duration and quality now exist. Thus, our labels may not necessarily generalize to other methods collecting the same types of raw data streams.

We determined it would not be appropriate to group methods by where they fall on the high-low sensitivity continuum. Sensitivity cannot be defined by the personal sensing method alone. It is highly contextual depending on factors such as characteristics and preferences of the person providing their data, who is receiving the data (e.g., researcher, clinician, health insurance), and what it is being used for [@ackermanPrivacyIssuesHumanComputer2008]. It is also dynamic so that a certain set of circumstances may be acceptable for sharing sensitive information at one point (e.g., when someone is stable in their recovery), but not at a later point (e.g., when someone is in the middle of a series of lapses or a relapse).

In our own sample we saw individual variation in how participants felt about the sensitivity of their data in free-response comments.

<!-- The --- Participant should be single spaced after the quote for all  of these quotes-->

> "I don't mind it at all. It's sort of nice to know I'm being monitored"

> `r tufte::quote_footer("--- Participant 047")`

> "I am concerned with data privacy and felt uncomfortable having my location actively tracked"

> `r tufte::quote_footer("--- Participant 196")`

Individuals may experience privacy differently in their day to day lives. People belonging to historically marginalized groups may find it more difficult to achieve privacy in their daily life [@marwickPrivacyMarginsUnderstanding2018]. Relatedly, our participants are in recovery for AUD. For some, there may be high costs (e.g., legal consequences, or loss of job, relationships, or health insurance) if information related to their recovery (e.g., frequent lapses) gets in the wrong hands. As a result, personal experiences may affect how much one values privacy and as a result whether one would be willing to provide sensitive data streams.

Researchers and clinicians should consider these factors when designing their personal sensing method. For example, in theory the one-time daily audio check-in that could be completed at any point during the day and only had to be 30 seconds in length should require less effort then a four-time daily EMA that was to be completed when prompted. When examining our participants' free-text comments about their experience with the different methods, we found that many said it was difficult to find a private place to record their audio check-in.

> "It takes time out of your day where you have to completely switch locations just so you can do it in private. I don't like that people could hear me and the topic wherever and whenever so I stopped using it"

> `r tufte::quote_footer("--- Participant 018")`

> "Sometimes I wouldn't be able to do it because I was with people too much which was frustrating" [participant 037].

> `r tufte::quote_footer("--- Participant 037")`

This lack of privacy compounded with the nature of their check-in (i.e., their recovery) likely contributed to the poor daily compliance. So, if obtaining a free response commentary (i.e., the content of the audio check-in) each day was crucial to our research goals it might have been in our better interest to provide participants with an alternate response format (e.g., typed text). On the other hand, if it was more important to get data related to vocal inflection, pace, and other information extracted from audio data, a less than daily prompt with reminders might be more effective.

## Theoretical Framework

Whether someone finds a personal sensing method acceptable ultimately comes down costs, benefits, and trust [@pavlouConsumerAcceptanceElectronic2003; @schnallTrustPerceivedRisk2015; @atienzaConsumerAttitudesPerceptions2015]. Our study only looks at acceptability related to cost (time and privacy). However, it is important that we also consider benefits and trust.

### Benefits

```
@bessenyeiComfortabilityPassiveCollection2021 - "Those who received mental health treatment liked the health and the communication features more than those who didn't. They were also more willing to share their data with their doctor than those who didn't receive treatment. Finally, they indicated higher intentions to accept app permissions and to use mobile apps for health, perceived more benefit and control and had more privacy related experience than the non-treated group."
```

Participants in our study received minimum benefits. Benefits included financial compensation for their time and possible gratification for their contribution to a risk prediction algorithm with the aim to help other people in recovery for AUD and from the self-reflection required for some of the daily tasks (audio check-in, EMA).

Research suggests that privacy concerns about personal sensing methods may be mitigated by the perceived value of the sensing platform [@atienzaConsumerAttitudesPerceptions2015; @klasnjaExploringPrivacyConcerns2009]. Free response comments from our participants support this.

> "As long as my personal information was secure and only shared by study staff, and its collection could help in my recovery, I'm comfortable with it"

> `r tufte::quote_footer("--- Participant 189")`

> "I felt all right having my location tracked. If it were used in a way to keep me from relapsing my feeling about it would be even more positive"

> `r tufte::quote_footer("--- Participant 016")`

As a result, individuals may find the personal sensing methods more acceptable if they were also receiving treatment through the application or receiving other benefits from using the application (e.g., resources, personalized notifications).

### Trust

In our study, trust levels were high. We had several protections in place to keep participants' data confidential and we thoroughly explained them during the consent process. Additionally, participants came into our lab for their monthly visit where they often built report with a limited number of study staff. Participant comments suggest this trust influenced their perceived acceptability of our sensing methods.

```
Situation matters - Ben-Zeev et al., 2016 found that 1/3rd of inpatient participants were concerned about privacy but no outpatients were.; additionally all 9 outpatients approached consented to be in study but 9 out of 20 approached inpatients declined due to concerns about tracking technology.
```


> "Invasive in theory, no problems actually because of confidentiality"

> `r tufte::quote_footer("--- Participant 193")`

> "I trusted the study group to not use my personal information for any other use"

> `r tufte::quote_footer("--- Participant 166")`

It is not surprising that trust plays an integral role in a person's acceptance of these methods. We know form the literature who people are sharing their personal sensing data with matters [@atienzaConsumerAttitudesPerceptions2015; @riegerPsychiatryOutpatientsWillingness2019; @rendinaPrivacyTrustData2018; @nicholasRoleDataType2019; @prasadExposingPrivacyConcerns2011]. Focus group studies suggest that people are most comfortable sharing data anonymously with researchers and with their doctor, and least comfortable sharing information with family members, electronic health record databases, and third party apps and websites [@rendinaPrivacyTrustData2018; @nicholasRoleDataType2019; @prasadExposingPrivacyConcerns2011]. This suggests trust is most closely related to trust that the information will be kept confidential.

## Ethics

Acceptability of personal sensing methods should not be taken as minimizing the sensitive nature of these data. In fact, we use the term personal sensing to make explicit how personal and sensitive these data are [@mohrPersonalSensingUnderstanding2017]. Personal sensing has the potential to seep into almost every aspect of an individual's world. Research and applications using personal sensing methods must only be done with appropriate protections in place.

Our participants generally felt comfortable providing these data to us. We had the strictest possible measures in place to keep their data safe and as a result they trusted us. One of these safeguards, a certificate of confidentiality, protects research participants by preventing forced disclosure of any identifying information. However, this is unique to the research context. Implementing personal sensing platforms in applied settings could potentially put the end user at risk [@oneilWeaponsMathDestruction2016]. Companies and third-party applications could be subpoenaed and required to disclose sensitive information to be used as ammunition in a custody case or as evidence in a criminal trial. Likewise, a company might be providing a patient's healthcare data to both their provider and their health insurance company resulting in higher insurance premiums or dropped coverage. This risk is not lost on the people being asked to provide their data and control over who can access their data and unauthorized secondary use of that information are often cited concerns [@ackermanPrivacyIssuesHumanComputer2008; @atienzaConsumerAttitudesPerceptions2015].

The current study assesses the acceptability of personal sensing methods that start with the collection of raw data streams and end with indicators for constructs. The likely next step is to use these indicators as inputs into a statistical algorithm to create a predictive machine learning model. Predictive models built on personal sensing data can be valuable in clinical areas such as risk prediction, mental health screening and diagnosis, and treatment matching. However, to benefit the patient it is critical that these models are trained on large and diverse samples of participants. <!-- Kendra: Elaborate why this is important and harmful consequences if not done-->

Controlling who gets access to personal sensing data, preventing conflicts of interest, and creating equitable and fair machine learning models are only some of the ethical concerns that must be addressed before personal sensing methods can be implemented.

## Feasibility

Acceptability of personal sensing methods is only the first step in using such methods in applied settings. It must also be feasible to implement a personal sensing platform or series of personal sensing methods. In other words, it must be possible to do in a convenient, realistic, and sustainable way. Feasibility of applying personal sensing methods can be broken down into feasibility for the participant, client, or patient and feasibility for the researcher, provider, or clinician.

### Participant, client, or patient

The primary feasibility concern on the user side of personal sensing is the ability to access the personal sensing platform and implement it into their daily routine.

The minimum technology needed for access to a personal sensing platform (e.g., an app) is a smartphone. While this may have been a large financial burden in the past, today there are more affordable options. Smartphone ownership is estimated to be at about 81% of the population and is not much lower in substance use populations and low socio-economic groups [@massonHealthrelatedInternetUse2019; @pewresearchcenterMobileFactSheet2021].

Not only do people generally have the requisite technology needed to access personal sensing platforms, but they are also willing to sustain interaction with personal sensing methods for long durations of time. We found participants accepted the personal sensing methods and were able to incorporate them into their daily lives with little interference. Additionally, most participants were willing to use these methods for one year to help their recovery. This is valuable in that they are reporting this after already having used these methods daily for about three months. So, they are using these methods and willing to continue using them for longer periods of time. This potential for sustained engagement with a personal sensing platform is promising if we are to ultimately use such methods in the real world.

### Researcher, provider, or clinician

```
We might also want to mention difficulty getting access to data (restricted to Androids - Bai et al., 2021; Lind et al., 2018; app not running in background 24x7 -  Bai et al., 2021; GPS tracking not always working accurately and consistently - Palmius et al., 2016) 

 Mendes et al., 2021 review - "A related issue is the predominance of solutions developed for Android OS, for which all apps have a version. This is expected as Android provides an open development platform, different from iOS, with significantly more flexibility to gather the data of interest. The divergent approaches to sensing on iOS and Android yield further issues in terms of standardization and the collection of comparable results across large cohorts, invariably with both Android and iOS users." 

Trifan et al., 2019 review - "In the spectrum of smartphone technologies, one of the main challenges that can affect the health-related collection of data when developing monitoring systems is the choice of the operating system. In fact, there are some differences and difficulties in development for Android or IOS systems, the 2 most used phone operating systems worldwide. Android is currently the most popular system and has the advantage of being convenient from the programming point of view. Scanning rates of sensors are found to be superior with this operating system. Furthermore, IOS hampers third-party apps to run endlessly in background, which may make the data collection difficult. Of the selected papers, 56.7% (67/118) developed their system only for Android smartphones, 6 developed for both Android and IOS, and 45 did not provide any information about the chosen operating system."
```


Feasibility concerns on the data collection end revolve mostly around the technology. These include technical difficulties, expense, and churn with companies.

We started out with three platforms (physiology wristband, sleep monitor, and study iPhone) and experienced varying levels of technological difficulties with each. Most notably we had to stop providing the Empatica E4 physiology wristband to participants almost immediately because it was too difficult to retrieve the data. The physiology wristband did not support Bluetooth streaming of data to the cloud and participants were required to manually upload the data each day on a study tablet. This process soon proved to be too burdensome, and it was too difficult to trouble shoot software problems that came up during the data upload process.

Expense must also be considered when evaluating feasibility of personal sensing methods. Had we continued using the physiology wristband we would have been providing participants with two pieces of hardware (wristband and tablet) to collect a single data stream. Early participants were also offered a study iPhone if they did not have an eligible smartphone (Android or IOS). This can be expensive and discouraging. However, as technology improves, we see how more can be done with a smartphone as a single all-encompassing personal sensing platform. Additionally, many people now own a smartphone. We stopped offering study smartphones because so many people already had one. Only six participants were given a study iPhone and five participants were screened out for not having an eligible phone. It then seems that personal sensing technology is becoming more accessible and affordable as time goes on.

One unexpected issue we faced was the high churn rate of companies. In just our two-year data collection period, we had to discontinue use of sleep monitor platform and our geolocation tracking app. The company that ran the sleep monitor platform, Beddit Oy, was bought out by Apple about halfway through the study. Apple discontinued cloud support and we were unable to continue accessing sleep monitor data. Additionally, the app we were using for geolocation tracking (Moves) was acquired and shut down by Facebook. As a result, we had to switch to a completely different app (FollowMee) mid-study.

While this churn is difficult in the middle of data collection, it does signal the fast rate of technology development and improvement. Some of our active measures, like sleep and physiology can now be done much more passively. Free response comments from participants about using the sleep monitor illustrate how a seemingly passive raw data stream (sleep) can still be active.

> "Would be better if it would just connect automatically"

> `r tufte::quote_footer("--- Participant 021")`

> "There were problems with the sleep monitor in that I was not able to activate the monitor when I was going to sleep"

> `r tufte::quote_footer("--- Participant 001")`

> "Sometimes it recorded all night but when I pressed I'm up there was no data recorded" [participant 053].

> `r tufte::quote_footer("--- Participant 053")`

> "Would have to wake up to use"

> `r tufte::quote_footer("--- Participant 056")`

## Limitations and Future Directions

There are many underlying factors that influence people's view of personal sensing measures. Highly active measures may only be tolerable within short time frames. Personal data may differ in their perceived sensitivity from individual to individual and group to group. As such our study offers an initial insight into the acceptability of personal sensing methods. Future studies should use longer study durations (i.e., one year) and recruit nationally representative samples.

Our study also only generalizes to our specific personal sensing methods. Although we do not expect general self-reported acceptability about the types of raw data streams to change, different methods may increase or reduce user burden resulting in shifts of behavioral acceptability (i.e., compliance and retention). For example, participants were more willing to use passive personal sensing measures for one year compared to active measures. In clinical applications we may expect individuals to be engaging with these methods for years. Thus, it is important to test newer and more passive personal sensing methods as they become available.

Finally, acceptability is not limited to perceived costs but is an exercise of weighing costs, benefits, and trust. Future research should directly measure benefits (e.g., with self-report) and manipulate how the data is benefiting the participant (e.g., incorporating it into treatment). Trust should also be examined by applying this research to other contexts (e.g., who is receiving the data, what level of relationship exists between participant and data collector). It is critical we understand the entire framework of personal sensing acceptability if we are to implement these methods in real treatment settings.

## Conclusion


```
Cut from intro. The ability to predict the presence or absence of these constructs prospectively can also open the door for more efficient monitoring. Current treatment approaches lack the necessary resources required for long-term monitoring of symptom onset and overall mental health stability. Yet detecting a symptom lapse early on could have benefits for both an patient's prognosis [@mcglashanEarlyDetectionIntervention1996; @scottCanWePredict1992] and overall cost [@kesslerGlobalBurdenMental2009; @wangTelephoneScreeningOutreach2007]. Additionally, mental health disorders are not static states [@nelsonMovingStaticDynamic2017]. Someone might be stable for months or years before re-experiencing symptoms [@hayesChangeNotAlways2007; @witkiewitzModelingComplexityPosttreatment2007]. Integrating personal sensing methods into clinical settings may be one way to improve the reach of treatment resources and may allow for intervention prior to a full symptom relapse.
```


## Cuts from intro that might get included

```
The ability to predict the presence or absence of these constructs prospectively can also open the door for more efficient monitoring. Current treatment approaches lack the necessary resources required for long-term monitoring of symptom onset and overall mental health stability. Yet detecting a symptom lapse early on could have benefits for both an patient's prognosis [@mcglashanEarlyDetectionIntervention1996; @scottCanWePredict1992] and overall cost [@kesslerGlobalBurdenMental2009; @wangTelephoneScreeningOutreach2007]. Additionally, mental health disorders are not static states [@nelsonMovingStaticDynamic2017]. Someone might be stable for months or years before re-experiencing symptoms [@hayesChangeNotAlways2007; @witkiewitzModelingComplexityPosttreatment2007]. Integrating personal sensing methods into clinical settings may be one way to improve the reach of treatment resources and may allow for intervention prior to a full symptom relapse.
```


``` 
Two key aspects of personal sensing methods that may affect their acceptability are the sensitivity of the data being collected [@pasipanodyaPerceivedRisksAmelioration2020; @rendinaPrivacyTrustData2018], and the active effort required by the individual [@atienzaConsumerAttitudesPerceptions2015; @eiseleEffectsSamplingFrequency2020]. Importantly, these two dimensions explicitly differ from one personal sensing measure to the next. For example, text message content monitoring is completely passive, requiring no active effort from the participant, but the data are highly sensitive. On the other hand, an EMA method may ask generic questions that are not perceived to be sensitive, but the high level of effort required (e.g., lengthy questionnaire, high prompt frequency) may be too burdensome. As a result, acceptability of personal sensing methods may be best assessed at the individual level of the measure being sensed.

Sensitivity of information may be an especially salient concern when working with clinical populations. Public opinion often holds a stigmatizing view of mental health disorders [@schomerusStigmaAlcoholDependence2011; @barryStigmaDiscriminationTreatment2014; @overtonStigmaMentalIllness2008; @parcesepePublicStigmaMental2013]. As a result, people may fear that their employer or peers will become aware of their diagnosis. These fears can become further compounded when illicit factors like drug use are involved. A negative opinion by neighbors or consequences at work are often cited reasons for not seeking treatment for substance use [@centerforbehavioralhealthstatisticsandquality2019NationalSurvey; @stringerStigmaBarrierSubstance2018]. Thus, the type and sensitivity of data that a personal sensing method collects may be an important contributor to the acceptability of that measure.

This component of sensitivity, however, can be difficult to quantify. It is possible to vary within group and within measure. For example, it is expected that some people will be more private than others. Some people may not feel comfortable sharing any information or they may only feel comfortable sharing information under certain contexts. Additionally, a single personal sensing measure could be viewed as having different degrees of sensitivity depending on the specific information it is collecting. An EMA that asks participants about their mood compared to drug-related behaviors is likely going to be seen as less sensitive. A study using focus-groups to assess reactions to a hypothetical personal-sensing smartphone application in a sample of men who use methamphetamine and are living with HIV found that participants expressed privacy concerns with both location and self-reported drug use information citing fears of legal consequences and unintentional disclosure of their drug use or HIV status to family and friends. [@pasipanodyaPerceivedRisksAmelioration2020]. Thus, the threshold of what level of sensitivity is acceptable is not easy to operationalize. Instead, these individual and group differences may be reflected as a strong dislike for a particular measure or perhaps a willingness to use the measure for several weeks as a paid research participant but an unwillingness to continue using the measure in other settings (e.g., as a recovery plan with a clinician) or for longer durations (e.g., one year).

As mentioned, personal sensing methods also differ in the level of active effort needed from the participant for data collection. Daily or more frequent self-report surveys (e.g., EMA) delivered to the participant's smartphone are active in nature, requiring substantial involvement. In contrast, background monitoring of cellular communications, location, or other sensor or log data are passive signals that are collected automatically with minimal to no engagement needed from the individual.

```



``` 
However, missing data from non-compliance is not the only way bias can be introduced into the data. Data provided irrespective of burden could result in hurried or careless response patterns likely due to study fatigue [@eiseleEffectsSamplingFrequency2020; @meadeIdentifyingCarelessResponses2012]. For example, when reimbursement is structured around compliance rates, participants may answer questions from memory or strategically answer items to avoid follow-up questions and shorten the length of the survey [@freedmanCellPhonesEcological2006].

```
\newpage

# References
<div id="refs"></div>

\newpage
<!--We need to generate figure 1 by code. Also, I can't get text references to work. This hacky solution works as long as we don't need special formating in the figure caption. See: http://frederikaust.com/papaja_man/tips-and-tricks.html#text-references-->

```{r}
fig_caption_1 <- "Flowchart of participant retention over the course of the three-month study. This figure displays retention and attrition of all eligible participants at various stages from consent though study completion.  It also display the reasons for attrition categorized as due to acceptability, other reasons, or unknown.  We present additional detail on reasons for attrition in Table 3."
```

```{r figure-1, fig.cap = fig_caption_1, out.extra = "", fig.pos="h"}
include_graphics(here("burden/manuscript/figs/fig_disposition.jpg"), dpi = 150)
```

\newpage


```{r}
fig_2_caption <- "Interference Ratings by Personal Sensing Data Stream.\\linebreak Notes:  X-axes are ordered to display higher acceptability on the right side.  N = 154 for all data streams except sleep  quality (N = 87). Solid red line represents the mean and dashed black line represents the neutral midpoint (undecided). All raw data streams had a mean significantly higher than the neutral midpoint.  Interference ratings were only collected for active methods."

```

<!-- Caption figure # is not parsing right -->

```{r figure-2, fig.height = 3.5, fig.cap = fig_2_caption, out.extra = "", fig.pos="h"}
interference_plot_data <- data_last %>% 
  select(contains("interfere")) %>%   
  pivot_longer(everything(), "measure", values_drop_na = TRUE) %>% 
  mutate(measure = factor(measure, 
                          levels = c("audio_checkin_interfere", "daily_survey_interfere", 
                                     "sleep_interfere"),
                          labels = c("Audio Check-in", "EMA", "Sleep Quality"))) %>% 
  mutate(value = factor(value, levels = c(-2:2), labels = c("Strongly agree", "Agree", "Undecided", "Disagree", "Strongly disagree"))) %>% 
  mutate(active = case_when(measure == "Audio Check-in" ~ "Active",
                            measure == "EMA" ~ "Active",
                            measure == "Sleep Quality" ~ "Active")) 

interference_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#FFDEDE") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = "Proportion",
       x = NULL) +
  theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575") +
  geom_vline(aes(xintercept = means), interference_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#b44343") 
```


\newpage


```{r}
fig_3_caption <- "Dislike Ratings by Personal Sensing Data Stream.\\linebreak  Notes:  X-axes are ordered to display higher acceptability on the right side.  N = 154 for all data streams except sleep quality (N = 87). Solid red or blue line represents the mean and dashed line represents the neutral midpoint (undecided). All raw data streams had a mean significantly higher than the neutral midpoint.  Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-3, fig.height = 6, fig.cap = fig_3_caption, out.extra = "", fig.pos="h"}
dislike_plot_data <- data_last %>% 
  select(contains("dislike")) %>%  
  pivot_longer(everything(), "measure", values_drop_na = TRUE) %>% 
   mutate(measure = factor(measure, 
                          levels = c("audio_checkin_dislike", "daily_survey_dislike", "sleep_dislike",
                                     "location_dislike", "all_logs_dislike", "sms_content_dislike"),
                          labels = c("Audio Check-in", "EMA", "Sleep Quality",
                                     "Geolocation", "Cellular Communication Logs", "Text Message Content"))) %>% 
  mutate(value = factor(value, levels = c(-2:2), labels = c("Strongly agree", "Agree", "Undecided", "Disagree", "Strongly disagree"))) %>% 
  mutate(active = case_when(measure == "Audio Check-in" ~ "Active",
                            measure == "EMA" ~ "Active",
                            measure == "Sleep Quality" ~ "Active",
                            measure == "Geolocation" ~ "Passive",
                            measure == "Cellular Communication Logs" ~ "Passive",
                            measure == "Text Message Content" ~ "Passive")) 

active_dis <- dislike_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#FFDEDE") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
  theme(legend.position = "none",
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        text = element_text(size = 12)) +
  ylim(0, .65) +
  geom_vline(aes(xintercept = means), dislike_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#b44343") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575") +
  theme(text = element_text(size = 12))


passive_dis <- dislike_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#DBF8FF") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = means), dislike_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>%  
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#05667b") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575") 

dislike_plot <- wrap_plots(active_dis, passive_dis, ncol = 1)

dislike_plot  %>% 
  add_global_label(Ylab = "                   Proportion",
                   Ygap = .02
)
```


\newpage
```{r}
fig_4_caption <- "Average Dislike by Active vs. Passive Methods.\\linebreak  Notes:  X-axes are ordered to display higher acceptability on the right side.  Active methods (displayed in red) represent an average of audio check-in and EMA.  Passive methods (displayed in blue) represent an average of geolocation, cellular communication logs and text message content.  Solid red or blue line represents the mean and dashed line represents the neutral midpoint (undecided).  Participants did not different significantly in their dislike of active vs. passive methods. N= 154. Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-4, fig.width = 5, fig.cap = fig_4_caption, out.extra = "", fig.pos="h"}
# transform dataframe of active/passive means to long
data_dislike_long <- data_dislike %>% 
  pivot_longer(cols = c(Active, Passive), names_to = "effort", values_to = "dislike") %>% 
  select(-dislike_diff)

data_dislike_long %>% 
  # bin continuous means, keep numeric
  mutate(dislike_binned = as.numeric(cut(dislike, breaks = c(-2.5, -1.5, -0.5, 0.5, 1.5, 2.5)))) %>% 
  # change 1 to 5 scale to -2 to 2
  mutate(dislike_binned = dislike_binned - 3) %>% 
  ggplot(aes(x = dislike_binned, y = ..prop.., group = effort, fill = effort)) +
  geom_bar(color = "black") +
  facet_wrap(~ effort, ncol = 1) +
  theme_classic() +
  labs(y = "Proportion",
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2),
                   labels = c("Strongly agree", "Agree", "Undecided", "Disagree", "Strongly disagree")) +
  scale_fill_manual(values = c("#FFDEDE", "#DBF8FF")) +
  geom_vline(aes(xintercept = m), data_dislike_long %>% 
               group_by(effort) %>% 
               summarise(m = mean(dislike)), size = .705, color = c("#b44343", "#05667b")) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", size = .4, color = "#787575")
```


\newpage

```{r}
fig_5_caption <- "Willingness to Use for One Year Ratings by Personal Sensing Data Stream.\\linebreak  Notes:  X-axes are ordered to display higher acceptability on the right side.  N = 154 for all data streams except sleep monitoring (N = 87). Solid blue or red line represents the mean and dashed line represents the neutral midpoint (undecided). All raw data streams had a mean significantly higher than the neutral midpoint.  Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-5, fig.height = 6, fig.cap = fig_5_caption, out.extra = "", fig.pos="h"}
willingness_plot_data <- data_last %>% 
  select(contains("1year")) %>%  
  pivot_longer(everything(), "measure", values_drop_na = TRUE) %>% 
  mutate(measure = factor(measure, 
                          levels = c("audio_checkin_1year", "daily_survey_4_1year", 
                                     "daily_survey_1_1year", "sleep_1year", "location_1year", 
                                     "all_logs_1year", "sms_content_1year"),
                          labels = c("Audio Check-in", "EMA", "Daily Survey (x1)<i><sup>a</sup></i>",
                                     "Sleep Quality", "Geolocation", "Cellular Communication Logs", 
                                     "Text Message Content"))) %>% 
  mutate(value = factor(value, levels = c(-2:2), labels = c("Strongly disagree", "Disagree", "Undecided", "Agree", "Strongly agree"))) %>% 
  mutate(active = case_when(measure == "Audio Check-in" ~ "Active",
                            measure == "EMA" ~ "Active",
                            measure == "Sleep Quality" ~ "Active",
                            measure == "Geolocation" ~ "Passive",
                            measure == "Cellular Communication Logs" ~ "Passive",
                            measure == "Text Message Content" ~ "Passive")) 

willingness_active <- willingness_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#FFDEDE") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
  theme(legend.position = "none",
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        text = element_text(size = 12),
        strip.text = element_markdown()) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = means), willingness_plot_data %>% 
  filter(measure == "Audio Check-in" | measure == "EMA" | measure == "Sleep Quality") %>% 
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#b44343") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575")


willingness_passive <- willingness_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>% 
  ggplot(aes(x = value, y = ..prop.., group = measure)) +
  geom_bar(color = "black", fill = "#DBF8FF") +
  facet_grid(active ~ measure) +
  theme_classic() +
  labs(y = NULL,
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  geom_vline(aes(xintercept = means), willingness_plot_data %>% 
  filter(measure == "Geolocation" | measure == "Cellular Communication Logs" | measure == "Text Message Content") %>%  
  group_by(measure) %>% 
  summarise(means = mean(as.numeric(value), na.rm = TRUE)), size = .705, color = "#05667b") +
  geom_vline(aes(xintercept = "Undecided"), linetype = "dashed", size = .4, color = "#787575")

willingness_final_plot <- wrap_plots(willingness_active, willingness_passive, ncol = 1)

willingness_final_plot  %>% 
  add_global_label(Ylab = "                   Proportion",
                   Ygap = .02
)
```

\newpage

```{r}
fig_6_caption <- "Average Willingness to Continue for One Year by Active vs. Passive Methods.\\linebreak  Notes:  X-axes are ordered to display higher acceptability on the right side.  Active methods (displayed in red) represent an average of audio check-in and EMA.  Passive methods (displayed in blue) represent an average of geolocation, cellular communication logs and text message content.  Solid red or blue line represents the mean and dashed line represents the neutral midpoint (undecided). Participants reported on average significantly higher acceptability with respect to willingness to continue using for one year for passive compared to active methods. N= 154. Active methods are displayed in red and passive methods are displayed in blue."
```

```{r figure-6, fig.width = 5, fig.cap = fig_6_caption, out.extra = "", fig.pos="h"}
# transform dataframe of active/passive means to long
data_willingness_long <- data_willingness %>% 
  pivot_longer(cols = c(Active, Passive), names_to = "effort", values_to = "willingness") %>% 
  select(-willingness_diff)

data_willingness_long %>% 
  # bin continuous means, keep numeric
  mutate(willingness_binned = as.numeric(cut(willingness, breaks = c(-2.5, -1.5, -0.5, 0.5, 1.5, 2.5)))) %>% 
  # change 1 to 5 scale to -2 to 2
  mutate(willingness_binned = willingness_binned - 3) %>% 
  ggplot(aes(x = willingness_binned, y = ..prop.., group = effort, fill = effort)) +
  geom_bar(color = "black") +
  facet_wrap(~ effort, ncol = 1) +
  theme_classic() +
  labs(y = "Proportion",
       x = NULL) +
    theme(legend.position = "none",
        text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  ylim(0, .6) +
  scale_x_continuous(breaks = c(-2, -1, 0, 1, 2),
                   labels = c("Strongly disagree", "Disgree", "Undecided", "Agree", "Strongly agree")) +
  scale_fill_manual(values = c("#FFDEDE", "#DBF8FF")) +
  geom_vline(aes(xintercept = m), data_willingness_long %>% 
               group_by(effort) %>% 
               summarise(m = mean(willingness)), size = .705, color = c("#b44343", "#05667b")) +
  geom_vline(aes(xintercept = 0), linetype = "dashed", size = .4, color = "#787575")
```
