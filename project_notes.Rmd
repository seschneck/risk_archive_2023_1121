---
title: "Project and Study notes for RISK"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk", 
      "/Volumes/private/studydata/risk")
    )
  })
---

# Project Level Notes

## Why Lapse?

There are a number of clear reasons to build models to predict lapse rather than 
relapse:

* Lapses are clearly defined and have a precise onset.  Relapse does not.

* Lapses are necessary for relapse.  You cannot lapse without a relapse.  

* Lapses will *often* precede relapse.  You may have a string of small lapses before
full relapse. This may give some cushion to intervene before relapse

* Lapses can produce a abstinence violation effect (AVE).  AVEs can lead to relapse.
Digital therapeutics could intervene to minimize the impact of any AVE.

* Lapse prediction allows for harm reduction.  If a lapse is predicted, digital therapeutics
may support more healthy drinking that is less likely to result in a relapse or other
harm.

## Lapse Labels

We use lapses reported by EMA and directly to study staff during visits.  

* These lapses have a start and end time recorded with one hour precision.  
* We want cases with accurate labels (`lapse` and `no_lapse`).  
* For *lapse* cases:
  * We filter out lapse reports that are unusable (no specific start time).  
  * We also filter lapses that are suspect because they have negative duration 
  or very long duration (currently > 24 hours).  
  * This results in a list of valid lapses with onsets reported by date and hour.

* For *no_lapse* cases:
  * We don't need all hours that are not lapse onsets b/c that would yield VERY 
  unbalanced labels and there is diminishing benefit from additional negative cases.  
  * We can instead sample from the hours where participant was not drinking
  * Currently thinking 9:1 for no_lapse:lapse (10% minority class) or 14:1 (6.67%)
  or 19:1 (5%).  I lean toward 19:1.  This will get us a LOT of negative cases.  
  We can the consider   models trained with all cases or resampling classes using 
  upsample, downsample, and SMOTE.
  * We want accurate negative cases.   To do this we filter out: 
    * All hours within a valid lapse period
    * 3 hours (current value; open for discussion) before or after a valid lapse period
    * Full day (and 3 hours before/after) for excluded lapse reports that listed 
    only the date (mostly from staff reports)
    * 24 + 3 hours after lapse onsets when no lapse end was reported.


## Variable Name Conventions

We will call the timestamp associated with our raw predictor data `dttm_obs` for 
all our raw data streams.  We can expect a column with this name in all raw predictor
data.

## Feature Engineering

Write individual feature functions.  These functions map over labels
They  take subid and `label_hour` (to map over) and `lead_time` and `period_duration` and data (full raw data with all subjects and all times but selected down to just relevant columns).   These functions will also have a parameter called `fun_list` that is a list of the specific univariate feature function that will be applied across the columns of data.  They will also need a parameter called `raw_type`

These functions call Kendra filter by lead_time, subid, and period duration.   
These functions apply the same feature function to all columns in data.  These functions name the resulting feature columns based on raw data, `lead_time`, `period_duration` and feature function.   These functions can be used in a pipe inside of `bind_col()`  different `data` can be passed into different maps if needed (to handle different raw data or different columns).  

Can have generic function where the feature function or functions are passed in as a list

https://dplyr.tidyverse.org/reference/across.html


You will want columns named .names = "{raw_type}_{.col}_{.fn}")
```

iris %>%
  group_by(Species) %>%
  summarise(across(starts_with("Sepal"), list(mean = mean, sd = sd), .names = "{.col}.{.fn}"))
#> # A tibble: 3 x 5
#>   Species    Sepal.Length.mean Sepal.Length.sd Sepal.Width.mean Sepal.Width.sd
#>   <fct>                  <dbl>           <dbl>            <dbl>          <dbl>
#> 1 setosa                  5.01           0.352             3.43          0.379
#> 2 versicolor              5.94           0.516             2.77          0.314
#> 3 virginica               6.59           0.636             2.97          0.322
```

```
iris %>%
#'   group_by(Species) %>%
#'   summarise(across(starts_with("Sepal"), mean, .names = "mean_{.col}"))
```


```
median_and_max <- list(
  med = ~median(.x, na.rm = TRUE),
  max = ~max(.x, na.rm = TRUE)
)

april_median_and_max <- ny %>%
  summarize(
    across(starts_with("4"), median_and_max)
  )
```


See [glue](https://glue.tidyverse.org/) for naming features

## Prediction Models

### Prediction Lead Time

We will predict lapse onset using 1 hour precision.  We will have *lapse* (minority class)
and *no_lapse* (majority class) cases.  

We could consider *drinking* cases as a third category of outcome but currently 
the benefits don't seem to outweigh the added cost of complexity.  

We will build true prediction models that use features derived from raw data 
available BEFORE the outcome observation time.  

We will consider models that vary in how far ahead they predict

* 1 hour
* 6 hours
* 1 day
* 3 days
cases

The first two prediction lead times serve primarily harm reduction or interventions
that might be effective immediately (distraction, urge surfing, etc)

The latter lead times provide a longer intervention window.  With a longer window, 
there is more time and a greater diversity of interventions that can be implemented

We could consider a 7 day lead time for prediction but this will likely result in too 
much data loss.   To build a model with a specific lead time, we need will need to drop 
cases that occur at the start of the study within the lead time AND the time window for
feature engineering (e.g., if we calculate features over the last week and are predicting
a week into the future, we may need to drop lapses in the first two weeks.)

John is leaning toward dropping the first week of data from the study to allow for features
based on up to a week of raw data preceding the observation of the outcome

### Algorithm Considerations


### Resampling Choices

## Dates and Time

We save all times in the UTC timezone.  This timezone conversion happens automatically 
when using `write_csv()` or `vroom_write()` to save a csv file.  These functions timestamp
the time column values with a "Z" to indicate the UTC timezone.  These same functions
read these time variables properly and assign the UTC timezone.

We are converting times to "America/Chicago" timezone when doing EDA or otherwise 
working with time variables.  This makes it easier to detect unusual values

When working with date variables, we convert them to dttm time variables and set
the time to midnight (the first moment in time of the day).  This allows for easier
comparisons between date variables and time variables.  We can add days(1) to check 
for times that occur after any date column.

## Data Issues that cannot be cleaned
**3**- Data: **GPS*. Participant had a major accident while in the study and was primarily at home recovering prior to follow-up #1 through follow-up #3.

**047**- Data: **EMA**. Participant was withdrawn by study team due to EMA issues that could not be resolved.; Participant was not able to receive messages from survey signal; Participant was withdrawn at his follow-up #1 visit; Completed through follow-up #1; Had a tech troubleshooting appointment on 04/11/2018 RAs were not successful in assessing why the participant was not receiving messages

**055, 061**- Data:  **All data**.  2/3/21 KW: Subject 55 was continued as subject 61 after a brief discontinuation. We are using their previous screen completed as subject 55, but a new intake completed as subject 61. All other files have been merged. See addition notes in `raw_notes.csv`

**066**- Data: **People, Places**.  RA does not trust the contacts data given at FU #2 and FU#3. The participant seemed to be in a rush and marked several contacts irrelevant/spam and neglected to answer the contact follow-up questions when asked directly by RA; from session form - RA does not believe the participant gave quality data. She marked irrelevant spam for all of the unreported contacts she was asked about. Additionally, RA only had the opportunity to review one unreported contact because the participant had to leave to go to work. She arrived 20 minutes late for her visit and had to leave 40 minutes later, so we did not have time to complete the appointment. The participant did not tell RA ahead of time that she only had a short time to meet.

**080**- Data: **GPS, EMA, Surveys**. From session form fu3 - Participant did not have any GPS data due to not putting FollowMee app on their phone. The participant also changed their number before their final visit and did not notify the lab, so they did not complete any surveys since early September. The new number has been saved in the enrollment database.

**082**- Data: **All data**. Participant was discontinued because cell service was shut off; Participant reported that their phone was stolen when they arrived at their Follow up 2 - they had not been giving us data since 7/31/18; 7/20/2018

**084**- Data: **GPS, All data**. Issues with phone throughout study; At FU1 participant said they had been out of the country and had issues with their technology

## Dates and Times

All times are saved in UTC.   However, when we work with times, we mutate to America/Chicago.
Dates are mutated to dttm and then set to midnight on that date.



# Burden Study Notes

This study focuses on the acceptability of various personal sensing methods 
to research participants with alcohol use disorder.

## Paper info

**Authors**: Wyant, Moshontz, Curtin

**Target journal**: *The Journal of Medical Internet Research*

# Meta Study Notes

This study focuses on the use of cellular communications meta data (logs) to 
predict lapses.

## Paper info

**Authors**: TBD.   

Tentative is Wyant, Sant'ana, Curtin



# Geolocation Study Notes

This study will focus on the use of geolocation data to predict lapses.  It will
likely also include the use of stable individual differences as these may prove 
to be important moderators of the location - lapse relationships


## Paper Info

**Authors**: TBD.   

Srijan Kumar (srijan@gatech.edu) and Gaurav Verma (gverma9@gatech.edu) are collaborators
from Georgia Tech who are taking the lead at this point in modeling.

Hannah will be a co-author given her contributions to processing raw geolocation 
data.  Her specific role is TBD based on future discussions and level of 
involvement with Georgia Tech in analysis, writing, etc.  


## Data Issues

**3**- Data: **GPS*. Participant had a major accident while in the study and was primarily at home recovering prior to follow-up #1 through follow-up #3.


# Full Cellular Communications Study Notes

This will be a follow-up to Kendra's FYP that also uses NLP on text messages.
Currently, this is planned as a second study (independent of Kendra's FYP) unless
the meta data prove insufficient by themselves for reasonable prediction.

## Paper Info

**Authors** TBD

Kendra will be a co-author given her role in processing the communications data.
Currently, John has reserved the option for her to lead study (first author) 
assuming timely progress, etc.

# Audio Check-ins Study

A good, focused NLP project.   Also interesting because of past research on 
journaling in addiction

## Paper Info

Yibing Sun (ysun326@wisc.edu), a student of Dhavan's, will lead this study.  John has also discussed a role for Susan.  


# Stressor-use Replication Study
 
May focus on conceptual replication of either the stressor-use or the 
pleasant/unpleasant stressor studies from NRT

## Paper Info

Sarah has expressed interest in this study.  Gaylen should be included given her 
role in stressor-use.  Ali may be involved depending on her involvement in 
pleasant stressors study.

