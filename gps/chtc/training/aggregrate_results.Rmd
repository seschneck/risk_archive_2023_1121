---
title: "Aggregate features"
author: "John Curtin"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk/knits/gps", 
      "/Volumes/private/studydata/risk/knits/gps")
    )
  })
---

### Code Status

in development

### Notes
This script aggregates all CHTC results for the job `r name_job`. It runs checks for 
missing jobs, plots hyperparameters, and summarizes model performance across all folds.    
  

Inputs:  

Returned CHTC files   

- output.zip  
- results.zip    
- output.zip   

Jobs input file   

- jobs.csv   

Output:   

- results_aggregate.csv   


### Set Up Environment

Absolute paths
```{r, absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_shared <- "P:/studydata/risk/data_processed/shared"
          path_gps <- "P:/studydata/risk/data_processed/gps"},

        # IOS paths
        Darwin = {
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"
          path_gps <- "/Volumes/private/studydata/risk/data_processed/gps"}
        )
```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) # detect and warn about function conflicts
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("select", "dplyr")

library(here)  # establish project directory consistently as working directory
```


Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
# for data wrangling
library(tidyverse)
library(lubridate)
library(vroom)
library(purrr)
library(furrr)
library(janitor)
library(ggplot2)
library(kableExtra)
library(vip)

theme_set(theme_classic()) 
```


Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```


Source training controls 
```{r}
source("gps/chtc/training/training_controls_1day.R")

source(here("../lab_support/chtc/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))
```

Set additional paths
```{r}
path_input <- here(path_jobs, name_job, "input")
path_output <- here(path_jobs, name_job, "output/output")
path_results <-  here(path_jobs, name_job, "output/results")
path_error <-  here(path_jobs, name_job, "output/error")
```





### Read and check raw data

```{r}
(n_core <- parallel::detectCores(logical = FALSE))
plan(multisession, workers = n_core)
```


Read in jobs
```{r}
jobs <- vroom(here(path_input, "jobs.csv"), show_col_types = FALSE) %>% 
  glimpse
```



check all error files are blank (0 kb)    
Note: error files also contain warnings
```{r}
# err_files <- map_df(list.files(path_error, full.names = TRUE), file.info)
# tabyl(err_files$size)
```

Pull error messages and jobs if error files are not all blank
```{r}
# if (nrow(subset(err_files, size > 0)) > 0) {
#   err_paths <- err_files %>% 
#     filter(size > 0) %>% 
#     rownames_to_column("path") %>% 
#     pull(path) 
#   for (i in err_paths) {
#     err_i <- read_file(i) %>% 
#     enframe(value = "message", name = NULL) %>% 
#     mutate(job_num = as.numeric(str_remove(str_split(str_split(i, "/")[[1]][11], "_")[[1]][2], ".err")),
#     message = str_remove_all(message, "\\n")) %>% 
#     relocate(job_num)
#     errs <- if (i == err_paths[1]) {
#       err_i
#     } else rbind(errs, err_i)
#   }
#   
#   # print error messages
#   print_kbl(errs, align = "l")
# }
```


Job rows with error messages
```{r}
# if (nrow(subset(err_files, size > 0)) > 0) {
#   # print jobs
#   jobs %>% 
#     inner_join(errs, by = "job_num") %>% 
#     print_kbl(align = "l")
# } else print("no errors or warnings")
```


check all output files are blank (0 kb)
```{r}
# out_files <- map_df(list.files(path_output, full.names = TRUE), file.info)
# tabyl(out_files$size)
```



read in all result CSVs
```{r}

metrics_raw <- list.files(path_results, full.names = TRUE) %>% 
  future_map_dfr(read_csv, col_types = readr::cols()) %>% 
  glimpse()
```


TEMPORARY DURIG DEVELOPMENT
```{r}
metrics_raw <- metrics_raw %>% 
  filter(algorithm == "glmnet") %>%
  select(-hp3, -n_fold, -n_repeat, -algorithm) %>% 
  glimpse()
```

Basic counts on raw file

```{r}
job_nums <- as.numeric(unique(metrics_raw_glm$job_num))
length(job_nums)  # should be 132!
# metrics_raw %>% tabyl(algorithm)
metrics_raw %>% tabyl(feature_set)  # missing some all feature set jobs
metrics_raw %>% tabyl(resample)  # missing some smote_1 feature sets
```


Save raw file.  
```{r}
metrics_raw %>% 
  vroom_write(here(path_gps, str_c("metrics_raw_", name_job, ".csv")))
```


check for missing jobs
```{r}
# missing_job_nums <- enframe(seq(1:nrow(jobs)), name = NULL, value = "job_num") %>% 
#   filter(!job_num %in% results$job_num)
```

`r nrow(results)` results from `r nrow(jobs)` jobs.      

```{r}
# if (nrow(missing_job_nums) > 0) {
#   print(str_c("missing ", nrow(missing_job_nums), " of ", nrow(jobs),
#               " jobs: ", str_c(as.character(missing_job_nums$job_num), collapse=", ")))
#   
#   # look at missing jobs closer
#   
#   jobs %>% 
#     filter(job_num %in% missing_job_nums$job_num)  %>% 
#     tabyl(algorithm)
# 
#   jobs %>% 
#     filter(job_num %in% missing_job_nums$job_num)  %>% 
#     tabyl(feature_set)
# 
#   jobs %>% 
#     filter(job_num %in% missing_job_nums$job_num) %>% 
#     print_kbl()
# 
#   } else print("No missing jobs")
```

### TEMP CODE FOR DEVELOPMENT SCENARIO



back to sequential
```{r}
plan(sequential)
```


### Average metrics across folds 

NOTE: glmnet algorithms will have 1 returned model for each penalty/lambda combination. 
This has already been averaged across folds on the whole dataset in `tune_model()`.   

Other models (Knn and random forest) may have 10 - 100 models per unique configuration.   

```{r}
metrics_agg <- metrics_raw  #TEMP FOR WHILE ONLY USING GLMNET
# results_aggregate <- results %>% 
#   group_by(algorithm, feature_set, hp1, hp2, hp3, resample, n_feats) %>% 
#   summarize(across(c(accuracy, bal_accuracy, sens, spec, roc_auc),
#                    mean),
#             n_jobs = n(), .groups = "drop") 
```


`r nrow(results_aggregate)` unique model configurations.    

Check all models have jobs that a 1 or in mulitiples of 10
```{r}
# results_aggregate %>% 
#   filter(!n_jobs %in% c(1, seq(10, 1000, by = 10)))
```
   

```{r}
# optional print kbl  commented out since sometimes causes error with knitting if too large
# results_aggregate %>%
#   print_kbl(digits = 4)
```

Save configuration summaries
```{r}
# write_csv(results_aggregate, here(path_job, "output/results_aggregate.csv")) 
```

### View best performing models   


Summary of feature sets
```{r}
# results_aggregate %>% 
#   count(feature_set, feature_fun_type, n_feats) %>% 
#   select(-n)
```

Balanced Accuracy
```{r}
metrics_agg %>% 
  arrange(desc(bal_accuracy)) %>% 
  slice(1:5) %>% 
  View
```

F1 - DONT TRUST
```{r}
metrics_agg %>% 
  arrange(desc(f_meas)) %>% 
  slice(1:5) %>% 
  View
```

ROC
```{r}
metrics_agg %>% 
  arrange(desc(roc_auc)) %>% 
  slice(1:5) %>% 
  View
```
Overall best model performance     

Highest balanced accuracy is `r round(max(results_aggregate$bal_accuracy), 2)`
```{r}
if (slice_max(results_aggregate, bal_accuracy)$algorithm == "glmnet") {
  results_aggregate %>% 
    slice_max(bal_accuracy) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_aggregate %>% 
    slice_max(bal_accuracy) %>% 
    glimpse()
}
```

highest ROC AUC is `r round(max(results_aggregate$roc_auc), 2)`
```{r}
# pull best AUC model if different than best balanced accuracy model
if (slice_max(results_aggregate, bal_accuracy)$algorithm == "glmnet" & slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results_aggregate %>% 
    slice_max(roc_auc) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else if (slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results_aggregate %>% 
    slice_max(roc_auc) %>% 
    glimpse()
  }
```

Save best model according to performance metric specified in training controls
```{r}
results_aggregate %>% 
  slice_max(get(perf_metric)) %>% 
  write_csv(., here(path_project, "best_model.csv"))

# glimpse best model if using different perf_metric
if (perf_metric != "roc_auc" & perf_metric != "bal_accuracy") {
  results_aggregate %>% 
    slice_max(get(perf_metric)) %>% 
    glimpse()
}
```


Get metrics for top models by feature set
```{r}
feature_sets <- unique(results_aggregate$feature_set) 

for (i in feature_sets) {
  
  results_i <- results_aggregate %>% 
    filter(feature_set == i)
  
  print(str_c("Best performing model for ", i, " feature set"))
  
  if (slice_max(results_i, bal_accuracy)$algorithm == "glmnet") {
  results_i_best <- results_i %>% 
    slice_max(get(perf_metric)) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_i_best <- results_i %>% 
    slice_max(get(perf_metric)) %>% 
    glimpse()
}
  
  # save out best active/passive model
  file_name <- str_c("best_", i, ".csv")
  write_csv(results_i_best, here(path_project, file_name))
  
}
```

Get metrics for top models by algorithm
```{r}
algorithms <- unique(results_aggregate$algorithm) 

for (k in algorithms) {
  
  results_k <- results_aggregate %>% 
    filter(algorithm == k)
  
  print(str_c("Best performing model for ", k, " algorithm"))
  
  if (slice_max(results_k, bal_accuracy)$algorithm == "glmnet") {
  results_k_best <- results_k %>% 
    slice_max(get(perf_metric)) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_k_best <- results_k %>% 
    slice_max(get(perf_metric)) %>% 
    glimpse()
}
  
  # save out best model for each algorithm
  file_name <- str_c("best_", k, ".csv")
  write_csv(results_k_best, here(path_project, file_name))
}
```


### Plot hyperparameters

```{r}
algorithms <- unique(results_aggregate$algorithm) 
for (k in algorithms) {
  
  results_k <- results_aggregate %>% 
      filter(algorithm == k)
  
  for (i in feature_sets) {
  
    results_i <- results_k %>% 
      filter(feature_set == i)
    
    
     # glmnet
    if (nrow(subset(results_i, algorithm == "glmnet")) != 0){
  
      plot_title <- str_c("Plotting glmnet hyperparameters for ", i, " feature set")
  
  
      plot_i <- results_i %>%
        filter(algorithm == "glmnet") %>% 
        mutate(hp1 = factor(hp1, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = log(hp2), 
                         y = bal_accuracy, 
                         group = hp1, 
                         color = hp1)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "mixture (alpha)") +
          labs(title = plot_title, x = "penalty (lambda)", y = "balanced accuracy")
  
      print(plot_i)
    }


    # random forest
    if (nrow(subset(results_i, algorithm == "random_forest")) != 0) {
      
      plot_title <- str_c("Plotting RF hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
        filter(algorithm == "random_forest") %>% 
        mutate(hp2 = factor(hp2, ordered = TRUE),
              resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = bal_accuracy, 
                         group = hp2, 
                         color = hp2)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "min n") +
          labs(title = plot_title, x = "mtry", y = "balanced accuracy")
      
       print(plot_i)
    }  
  
    # knn
    if (nrow(subset(results_i, algorithm == "knn")) != 0) {
      
      plot_title <- str_c("Plotting knn hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
        filter(algorithm == "knn") %>%
        mutate(resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = bal_accuracy)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          labs(title = plot_title, x = "neighbors", y = "balanced accuracy")
      
        print(plot_i)
    }
  }
}
```



### Refit best model
Read in data and create splits
```{r}
# read in data
d <- read_rds(here(path_job, "input/data_trn.rds")) %>% 
  rename(y = {{y_col_name}})

# create splits object ---------------
set.seed(102030)
splits <- if (str_split(str_remove(cv_type, "_x"), "_")[[1]][1] == "group") {
  make_splits(d = d, cv_type = cv_type, group = group)
} else { 
  make_splits(d = d, cv_type = cv_type)
}
```

```{r}
best_model <- slice_max(results_aggregate, get(perf_metric)) %>% 
      glimpse()
```

Replicates best CHTC models using decision metric specified in study's training controls  
```{r}
# build recipe
rec <- build_recipe(d = d, job = best_model)
    
# fit model and get results
results_best <- tune_best_model(best_model = best_model, rec = rec, folds = splits, cv_type = cv_type)
    
# pull out model results
best_model_results <- results_best[[1]] 
    
# compare results to CHTC model
best_model_results %>% 
      select(-.config) %>% 
      glimpse()
    
# pull out model predictions (optional)
# predictions <- results_best[[2]]

# add subids by row number in original data set 
# predictions <- predictions %>% 
#   left_join(d %>% 
#             rowid_to_column() %>% 
#             select(rowid, subid, dttm_label), by = c(".row" = "rowid"))
    
# pull out model fits (optional)   
# Unnest .extracts column to see feature estimates
# best_model_fits <- results_best[[3]]
    
# save out model to path_project (optional)
# fits_path <- here(path_project, str_c("best_", k, "_", "model_fits.rds"))
# write_rds(best_model_fits, fits_path, compress = "xz")
    
# save out predictions (optional)
# preds_path <- here(path_project, str_c("best_", k, "_", "model_preds.rds"))
# write_rds(predictions, preds_path, compress = "xz") 
    
# save out results (optional)
# res_path <- here(path_project, str_c("best_", k, "_", "model_results.csv"))
# write_csv(best_model_results, res_path)
```








