---
title: "Aggregate features"
author: "John Curtin"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk/knits/gps", 
      "/Volumes/private/studydata/risk/knits/gps")
    )
  })
---

### Code Status

in development

### Notes
This script aggregates all results/metrics for a batch or batches of jobs
that train all model configurations for a specific outcome/label window.  Set label here

```{r}
window <- "1day"
```

  



### Set Up Environment

Absolute paths
```{r, absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          #path_shared <- "P:/studydata/risk/data_processed/shared"
          path_gps_input <- "P:/studydata/risk/chtc/gps"
          path_gps_processed <- "P:/studydata/risk/data_processed/gps"},

        # IOS paths
        Darwin = {
          #path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"
          path_gps_input <- "/Volumes/private/studydata/risk/chtc/gps"
          path_gps_processed <- "/Volumes/private/studydata/risk/data_processed/gps"}
        )
```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) # detect and warn about function conflicts
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")

library(here)  # establish project directory consistently as working directory
```


Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
# for data wrangling
library(tidyverse)
library(lubridate)
library(vroom)
library(purrr)
library(furrr)
library(janitor)
library(ggplot2)
library(kableExtra)
library(vip)

theme_set(theme_classic()) 
```


Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```


Source training controls 
```{r}
# source("gps/chtc/training/training_controls_1day.R")
# 
# source(here("../lab_support/chtc/fun_chtc.R"))
# source(here("../lab_support/print_kbl.R"))
```

Set additional paths
```{r}
# path_input <- here(path_jobs, name_job, "input")
# path_output <- here(path_jobs, name_job, "output/output")
# path_results <-  here(path_jobs, name_job, "output/results")
# path_error <-  here(path_jobs, name_job, "output/error")
```





### Check for proper results, error, and output files

Will do this part in parallel as needed
```{r}
(n_core <- parallel::detectCores(logical = FALSE))
plan(multisession, workers = n_core)
```

#### Batch 1

```{r}
batch_name <- "train_1day_glmnet"
```

Read in jobs file
```{r}
jobs <- vroom(here(path_gps_input, batch_name, "input", "jobs.csv"), show_col_types = FALSE) %>% 
  glimpse

(n_jobs <- nrow(jobs))
```


```{r}
err_files <- map_dfr(list.files(here(path_gps_input, batch_name, "output", "error"), 
                               full.names = TRUE), 
                    file.info)
n_err_files <- nrow(err_files)
# out_files <- map_dfr(list.files(here(path_gps_input, batch_name, "output", "out"), 
#                                full.names = TRUE), 
#                     file.info)
# n_out_files <- nrow(out_files)

results_files <- map_dfr(list.files(here(path_gps_input, batch_name, "output", "results"), 
                               full.names = TRUE), 
                    file.info)
n_results_files <- nrow(results_files)
```

Check counts of error, out, and results
```{r}
if (!(n_jobs == n_err_files)) {
  stop(n_jobs, " jobs != ", n_err_files, " error files!") 
} else {
  message(n_err_files, " error files detected.  Correct!")
}

# if (!(n_jobs == n_out_files)) {
#   stop(n_jobs, " jobs != ", n_out_files, " out files!")
# } else {
#   message(n_out_files, " out files detected.  Correct!")
# }

if (!(n_jobs == n_results_files)) {
  stop(n_jobs, " jobs != ", n_feature_files, " feature files!")
} else {
  message(n_feature_files, " feature files detected.  Correct!")
}
```

Display path/filename of non-zero error files
```{r}
err_files %>%
    filter(size > 0) %>%
    rownames_to_column("path") %>%
    pull(path)
```

Display path/filename of non-zero out files
```{r}
# out_files %>%
#     filter(size > 0) %>%
#     rownames_to_column("path") %>%
#     pull(path)
```


read in all result CSVs
```{r}
metrics_raw_batch <- list.files(here(path_gps_input, batch_name, "output", "results"), full.names = TRUE) %>% 
  future_map_dfr(read_csv, col_types = readr::cols()) %>% 
  glimpse()
```


Basic counts

```{r}
# job_nums <- as.numeric(unique(metrics_batch$job_num))
# length(job_nums)  # should be 132!
# # metrics_raw %>% tabyl(algorithm)
# metrics_batch %>% tabyl(feature_set)  # missing some all feature set jobs
# metrics_batch %>% tabyl(resample)  # missing some smote_1 feature sets
```


metrics_raw_all <- metrics_raw_batch

#### Batch 2











### Wrap up processing of raw metrics


back to sequential
```{r}
plan(sequential)
```


SAVE RAW METRICS FILE

### Average metrics across folds 

NOTE: glmnet algorithms will have 1 returned model for each penalty/lambda combination. 
This has already been averaged across folds on the whole dataset in `tune_model()`.   

Other models (Knn and random forest) may have 10 - 100 models per unique configuration.   

```{r}
metrics_agg <- metrics_raw  #TEMP FOR WHILE ONLY USING GLMNET
# results_aggregate <- results %>% 
#   group_by(algorithm, feature_set, hp1, hp2, hp3, resample, n_feats) %>% 
#   summarize(across(c(accuracy, bal_accuracy, sens, spec, roc_auc),
#                    mean),
#             n_jobs = n(), .groups = "drop") 
```


`r nrow(results_aggregate)` unique model configurations.    

Check all models have jobs that a 1 or in mulitiples of 10
```{r}
# results_aggregate %>% 
#   filter(!n_jobs %in% c(1, seq(10, 1000, by = 10)))
```
   

```{r}
# optional print kbl  commented out since sometimes causes error with knitting if too large
# results_aggregate %>%
#   print_kbl(digits = 4)
```

Save configuration summaries
```{r}
# write_csv(results_aggregate, here(path_job, "output/results_aggregate.csv")) 
```

### View best performing models   


Summary of feature sets
```{r}
# results_aggregate %>% 
#   count(feature_set, feature_fun_type, n_feats) %>% 
#   select(-n)
```

Balanced Accuracy
```{r}
metrics_agg %>% 
  arrange(desc(bal_accuracy)) %>% 
  slice(1:5) %>% 
  View
```

F1 - DONT TRUST
```{r}
metrics_agg %>% 
  arrange(desc(f_meas)) %>% 
  slice(1:5) %>% 
  View
```

ROC
```{r}
metrics_agg %>% 
  arrange(desc(roc_auc)) %>% 
  slice(1:5) %>% 
  View
```
Overall best model performance     

Highest balanced accuracy is `r round(max(results_aggregate$bal_accuracy), 2)`
```{r}
if (slice_max(results_aggregate, bal_accuracy)$algorithm == "glmnet") {
  results_aggregate %>% 
    slice_max(bal_accuracy) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_aggregate %>% 
    slice_max(bal_accuracy) %>% 
    glimpse()
}
```

highest ROC AUC is `r round(max(results_aggregate$roc_auc), 2)`
```{r}
# pull best AUC model if different than best balanced accuracy model
if (slice_max(results_aggregate, bal_accuracy)$algorithm == "glmnet" & slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results_aggregate %>% 
    slice_max(roc_auc) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else if (slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results_aggregate %>% 
    slice_max(roc_auc) %>% 
    glimpse()
  }
```

Save best model according to performance metric specified in training controls
```{r}
results_aggregate %>% 
  slice_max(get(perf_metric)) %>% 
  write_csv(., here(path_project, "best_model.csv"))

# glimpse best model if using different perf_metric
if (perf_metric != "roc_auc" & perf_metric != "bal_accuracy") {
  results_aggregate %>% 
    slice_max(get(perf_metric)) %>% 
    glimpse()
}
```


Get metrics for top models by feature set
```{r}
feature_sets <- unique(results_aggregate$feature_set) 

for (i in feature_sets) {
  
  results_i <- results_aggregate %>% 
    filter(feature_set == i)
  
  print(str_c("Best performing model for ", i, " feature set"))
  
  if (slice_max(results_i, bal_accuracy)$algorithm == "glmnet") {
  results_i_best <- results_i %>% 
    slice_max(get(perf_metric)) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_i_best <- results_i %>% 
    slice_max(get(perf_metric)) %>% 
    glimpse()
}
  
  # save out best active/passive model
  file_name <- str_c("best_", i, ".csv")
  write_csv(results_i_best, here(path_project, file_name))
  
}
```

Get metrics for top models by algorithm
```{r}
algorithms <- unique(results_aggregate$algorithm) 

for (k in algorithms) {
  
  results_k <- results_aggregate %>% 
    filter(algorithm == k)
  
  print(str_c("Best performing model for ", k, " algorithm"))
  
  if (slice_max(results_k, bal_accuracy)$algorithm == "glmnet") {
  results_k_best <- results_k %>% 
    slice_max(get(perf_metric)) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_k_best <- results_k %>% 
    slice_max(get(perf_metric)) %>% 
    glimpse()
}
  
  # save out best model for each algorithm
  file_name <- str_c("best_", k, ".csv")
  write_csv(results_k_best, here(path_project, file_name))
}
```


### Plot hyperparameters

```{r}
algorithms <- unique(results_aggregate$algorithm) 
for (k in algorithms) {
  
  results_k <- results_aggregate %>% 
      filter(algorithm == k)
  
  for (i in feature_sets) {
  
    results_i <- results_k %>% 
      filter(feature_set == i)
    
    
     # glmnet
    if (nrow(subset(results_i, algorithm == "glmnet")) != 0){
  
      plot_title <- str_c("Plotting glmnet hyperparameters for ", i, " feature set")
  
  
      plot_i <- results_i %>%
        filter(algorithm == "glmnet") %>% 
        mutate(hp1 = factor(hp1, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = log(hp2), 
                         y = bal_accuracy, 
                         group = hp1, 
                         color = hp1)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "mixture (alpha)") +
          labs(title = plot_title, x = "penalty (lambda)", y = "balanced accuracy")
  
      print(plot_i)
    }


    # random forest
    if (nrow(subset(results_i, algorithm == "random_forest")) != 0) {
      
      plot_title <- str_c("Plotting RF hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
        filter(algorithm == "random_forest") %>% 
        mutate(hp2 = factor(hp2, ordered = TRUE),
              resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = bal_accuracy, 
                         group = hp2, 
                         color = hp2)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "min n") +
          labs(title = plot_title, x = "mtry", y = "balanced accuracy")
      
       print(plot_i)
    }  
  
    # knn
    if (nrow(subset(results_i, algorithm == "knn")) != 0) {
      
      plot_title <- str_c("Plotting knn hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
        filter(algorithm == "knn") %>%
        mutate(resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = bal_accuracy)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          labs(title = plot_title, x = "neighbors", y = "balanced accuracy")
      
        print(plot_i)
    }
  }
}
```



### Refit best model
Read in data and create splits
```{r}
# read in data
d <- read_rds(here(path_job, "input/data_trn.rds")) %>% 
  rename(y = {{y_col_name}})

# create splits object ---------------
set.seed(102030)
splits <- if (str_split(str_remove(cv_type, "_x"), "_")[[1]][1] == "group") {
  make_splits(d = d, cv_type = cv_type, group = group)
} else { 
  make_splits(d = d, cv_type = cv_type)
}
```

```{r}
best_model <- slice_max(results_aggregate, get(perf_metric)) %>% 
      glimpse()
```

Replicates best CHTC models using decision metric specified in study's training controls  
```{r}
# build recipe
rec <- build_recipe(d = d, job = best_model)
    
# fit model and get results
results_best <- tune_best_model(best_model = best_model, rec = rec, folds = splits, cv_type = cv_type)
    
# pull out model results
best_model_results <- results_best[[1]] 
    
# compare results to CHTC model
best_model_results %>% 
      select(-.config) %>% 
      glimpse()
    
# pull out model predictions (optional)
# predictions <- results_best[[2]]

# add subids by row number in original data set 
# predictions <- predictions %>% 
#   left_join(d %>% 
#             rowid_to_column() %>% 
#             select(rowid, subid, dttm_label), by = c(".row" = "rowid"))
    
# pull out model fits (optional)   
# Unnest .extracts column to see feature estimates
# best_model_fits <- results_best[[3]]
    
# save out model to path_project (optional)
# fits_path <- here(path_project, str_c("best_", k, "_", "model_fits.rds"))
# write_rds(best_model_fits, fits_path, compress = "xz")
    
# save out predictions (optional)
# preds_path <- here(path_project, str_c("best_", k, "_", "model_preds.rds"))
# write_rds(predictions, preds_path, compress = "xz") 
    
# save out results (optional)
# res_path <- here(path_project, str_c("best_", k, "_", "model_results.csv"))
# write_csv(best_model_results, res_path)
```








