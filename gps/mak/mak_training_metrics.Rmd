---
title: "Aggregate raw training metrics"
author: "John Curtin"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk/knits/gps", 
      "/Volumes/private/studydata/risk/knits/gps")
    )
  })
---

### Code Status

in development

### Notes
This script aggregates all results/metrics for a batch or batches of jobs
that train all model configurations for a specific outcome/label window.  Set label here

```{r}
window <- "1day"
```


### Set Up Environment

Absolute paths
```{r, absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          #path_shared <- "P:/studydata/risk/data_processed/shared"
          path_gps_input <- "P:/studydata/risk/chtc/gps"
          path_gps_processed <- "P:/studydata/risk/data_processed/gps"},

        # IOS paths
        Darwin = {
          #path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"
          path_gps_input <- "/Volumes/private/studydata/risk/chtc/gps"
          path_gps_processed <- "/Volumes/private/studydata/risk/data_processed/gps"}
        )
```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) # detect and warn about function conflicts
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("spec", "yardstick")
conflict_prefer("col_factor", "vroom")

library(here)  # establish project directory consistently as working directory
```


Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
# for data wrangling
library(tidyverse)
library(lubridate)
library(vroom)
library(purrr)
library(furrr)
library(janitor)
library(ggplot2)
library(kableExtra)
library(vip)

theme_set(theme_classic()) 
```


Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```


Source training controls 
```{r}
# source("gps/chtc/training/training_controls_1day.R")
# 
source(here("../lab_support/chtc/static_files/input/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))

```

Set additional paths
```{r}
# path_input <- here(path_jobs, name_job, "input")
# path_output <- here(path_jobs, name_job, "output/output")
# path_results <-  here(path_jobs, name_job, "output/results")
# path_error <-  here(path_jobs, name_job, "output/error")
```





### Check for proper results, error, and output files

Will do this part in parallel as needed
```{r}
(n_core <- parallel::detectCores(logical = FALSE))
plan(multisession, workers = n_core)
```

#### Batch 1: GLMNET

```{r}
batch_name <- "train_1day_glmnet"
```

Read in jobs file
```{r}
jobs <- vroom(here(path_gps_input, batch_name, "input", "jobs.csv"), show_col_types = FALSE) %>% 
  glimpse

(n_jobs <- nrow(jobs))
```


```{r}
err_files <- map_dfr(list.files(here(path_gps_input, batch_name, "output", "error"), 
                               full.names = TRUE), 
                    file.info)
n_err_files <- nrow(err_files)

results_files <- map_dfr(list.files(here(path_gps_input, batch_name, "output", "results"), 
                               full.names = TRUE), 
                    file.info)
n_results_files <- nrow(results_files)
```

Check counts of error, out, and results
```{r}
if (!(n_jobs == n_err_files)) {
  stop(n_jobs, " jobs != ", n_err_files, " error files!") 
} else {
  message(n_err_files, " error files detected.  Correct!")
}

if (!(n_jobs == n_results_files)) {
  stop(n_jobs, " jobs != ", n_results_files, " feature files!")
} else {
  message(n_results_files, " feature files detected.  Correct!")
}
```

Display path/filename of non-zero error files
```{r}
err_files %>%
    filter(size > 0) %>%
    rownames_to_column("path") %>%
    pull(path)
```

read in all result CSVs
```{r}
metrics_raw_batch <- list.files(here(path_gps_input, batch_name, "output", "results"), full.names = TRUE) %>% 
  future_map_dfr(read_csv, col_types = readr::cols()) %>% 
  glimpse()
```


Basic counts and checks

```{r}
job_nums <- as.numeric(unique(metrics_raw_batch$job_num))
if (!(n_jobs == length(job_nums))) {
  stop(n_jobs, " jobs != ", length(job_nums), "  job_nums in metrics!") 
} else {
  message(length(job_nums), " unique job numbs detected.  Correct!")
}

metrics_raw_batch %>% tabyl(job_num)
metrics_raw_batch %>% tabyl(n_repeat)
metrics_raw_batch %>% tabyl(n_fold)
metrics_raw_batch %>% tabyl(algorithm)
metrics_raw_batch %>% tabyl(feature_set)
metrics_raw_batch %>% tabyl(hp1)
metrics_raw_batch %>% tabyl(hp2)
metrics_raw_batch %>% tabyl(resample)
metrics_raw_batch %>% tabyl(n_feats)
```

Add batch 1 to all metrics
```{r}
metrics_raw_all <- metrics_raw_batch
```



#### Batch 2











### Wrap up processing of raw metrics


back to sequential
```{r}
plan(sequential)
```


Save raw metrics file
```{r}
metrics_raw_all %>% 
  glimpse %>% 
  vroom_write(here(path_gps_processed, str_c("metrics_raw_train_", window, ".csv")), delim = ",")
```

### Average metrics across folds for model configurations



```{r}
metrics_avg <- metrics_raw_all %>% 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample) %>% 
   summarize(across(c(accuracy, bal_accuracy, roc_auc, sens, spec, ppv, npv),
                    mean),
             n_jobs = n(), .groups = "drop") %>% 
  relocate(n_jobs)
```

Brief check of job counts. 
Glmnet should have 1 job.  Other algorithms should have folds X repeats jobs
```{r}
metrics_avg %>% 
  group_by(algorithm) %>% 
  tabyl(n_jobs)
```

Save average metrics
```{r}
metrics_avg %>% 
  vroom_write(here(path_gps_processed, str_c("metrics_avg_train_", window, ".csv")), delim = ",")
```


### View best performing models   

Best Balanced Accuracy for algorithms x feature sets
```{r}
metrics_avg %>% 
  group_by(algorithm, feature_set, resample) %>% 
  arrange(desc(bal_accuracy)) %>% 
  slice(1) %>% 
  ungroup %>% 
  arrange(desc(bal_accuracy)) %>% 
  print_kbl()
```

Best AUC for algorithms x feature sets
```{r}
metrics_avg %>% 
  group_by(algorithm, feature_set, resample) %>% 
  arrange(desc(roc_auc)) %>% 
  slice(1) %>% 
  ungroup %>% 
  arrange(desc(roc_auc)) %>% 
  print_kbl()
```


### Plot hyperparameters

```{r}
algorithms <- unique(metrics_avg$algorithm) 
feature_sets <- unique(metrics_avg$feature_set) 

for (k in algorithms) {
  
  results_k <- metrics_avg %>% 
      filter(algorithm == k)
  
  for (i in feature_sets) {
  
    results_i <- results_k %>% 
      filter(feature_set == i)
    
    
    # glmnet
    if (k == "glmnet") {
  
      plot_title <- str_c("Plotting glmnet hyperparameters for ", i, " feature set")
  
  
      plot_i <- results_i %>%
#        filter(algorithm == "glmnet") %>% 
        mutate(hp1 = factor(hp1, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = log(hp2), 
                         y = bal_accuracy, 
                         group = hp1, 
                         color = hp1)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "mixture (alpha)") +
          labs(title = plot_title, x = "penalty (lambda)", y = "balanced accuracy")
  
      print(plot_i)
    }


    # random forest
    if (k == "random_forest") {
      
      plot_title <- str_c("Plotting RF hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
    #    filter(algorithm == "random_forest") %>% 
        mutate(hp2 = factor(hp2, ordered = TRUE),
              resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = bal_accuracy, 
                         group = hp2, 
                         color = hp2)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "min n") +
          labs(title = plot_title, x = "mtry", y = "balanced accuracy")
      
       print(plot_i)
    }  
  
    # # knn
    # if (k == "knn") {
    #   
    #   plot_title <- str_c("Plotting knn hyperparameters for ", i, " feature set")
    #   
    #   plot_i <- results_i %>%
    #    # filter(algorithm == "knn") %>%
    #     mutate(resample = case_when(resample == "none" ~ "none_19",
    #                                 TRUE ~ resample)) %>% 
    #     separate(resample, c("resample", "under_ratio"), "_") %>% 
    #     mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
    #     ggplot(mapping = aes(x = hp1, 
    #                      y = bal_accuracy)) +
    #       geom_line() +
    #       facet_grid(under_ratio ~ resample) +
    #       labs(title = plot_title, x = "neighbors", y = "balanced accuracy")
    #   
    #     print(plot_i)
    # }
  }
}
```



### Refit best model

TEMP.  HARD CODE FOR NOW
Best model is glmnet (CURRENTLY)
```{r}
path_best <- "P:/studydata/risk/chtc/gps/train_1day_glmnet/input"   #UPDATE/EDIT
source(here(path_best, "training_controls.R"))
```




Read in data and create splits
```{r}
# read in data
d <- vroom(here(path_best, "data_trn.csv.xz")) %>% 
  rename(y = {{y_col_name}})

# create splits object ---------------
set.seed(102030)
splits <- if (str_split(str_remove(cv_type, "_x"), "_")[[1]][1] == "group") {
  make_splits(d = d, cv_type = cv_type, group = group)
} else { 
  make_splits(d = d, cv_type = cv_type)
}
```

```{r}
best_model <- metrics_avg %>% 
  arrange(desc(roc_auc)) %>% 
  slice(1) %>% 
  glimpse()
```

Replicates best CHTC models using decision metric specified in study's training controls  
```{r}
# build recipe
rec <- build_recipe(d = d, job = best_model)
    
# fit model and get results
fit_best <- tune_best_model(best_model = best_model, rec = rec, folds = splits, cv_type = cv_type)

fits <- fit_best[[3]]
preds <- fit_best[[2]]
```


```{r}
fits %>% collect_metrics()
```



```{r}
cm <- tibble(truth = preds$y, estimate = preds$.pred_class) %>% 
  conf_mat(truth, estimate)

cm %>% autoplot()

cm %>% summary(event_level = "second")
```



```{r}
roc_plot <- 
  tibble(truth = preds$y,
         prob = preds$.pred_yes) %>% 
  roc_curve(prob, truth = truth, event_level = "second")


tibble(truth = preds$y,
       prob = preds$.pred_yes) %>% 
  roc_auc(prob, truth = truth, event_level = "second")

roc_plot %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  labs(x = "1 - Specificity (FPR)",
       y = "Sensitivity (TPR)")
```



