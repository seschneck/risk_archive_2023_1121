---
title: "Model Comparisons"
author: "Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
    code_folding: show
editor_options: 
  chunk_output_type: console
---

### Setup

Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

Source training controls 
```{r}
source('./meta/chtc/training/training_controls_model_selection.R')
```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) 
 conflict_prefer("filter", "dplyr")
 conflict_prefer("select", "dplyr")
 conflict_prefer("spec", "yardstick")

library(here)  
```

Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
library(tidyverse)  
library(janitor) 
library(lubridate)
library(ggplot2)
library(kableExtra)
library(vip)
library(tictoc)

theme_set(theme_classic()) 
```

Set additional paths
```{r}
path_models <- "./meta/ana_scripts/model_output"
path_data <- "P:/studydata/risk/data_processed/meta/features"
```

Source function scripts from lab support
```{r  source_script, message=FALSE, warning=FALSE}
source(here("../lab_support/chtc/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))
```

### Notes
This script does model comparisons on the best models for the meta project.

### Read in results

```{r}
results_all <- read_csv(here(path_models, "results_aggregate.csv"), col_types = readr::cols()) %>% 
  glimpse()
```


### Best model compared to NULL

Best model
```{r}
best_model <- results_all %>% 
  slice_max(bal_accuracy) %>% 
  glimpse()
```

Read in model
```{r}
best_model_fits <- read_rds(here(path_models, str_c("best_", best_model$algorithm, "_model_fits.rds")))
```

Permutation test with 1 x 10 cv? 10 x 10 cv?
```{r warning = FALSE}
# read in data
data_all <- read_csv(here(path_data, "features_all_aggregate.csv"), col_types = readr::cols()) %>% 
  rename(y = {{y_col_name}}) %>% 
  glimpse()
data_perm <- data_all # don't overwrite original data set

# build recipe
rec <- build_recipe(d = data_all, job = best_model)

# permute outcome in full dataset
n_all <- nrow(data_perm)
data_perm$y <- data_perm$y[sample(n_all)]
  
# create splits on permuted data
set.seed(102030)
splits_perm <- make_splits(d = data_perm, cv_type = cv_type, group = group)
  
# fit permuted model
fits_perm <- if (best_model$algorithm == "glmnet") {
  models <- logistic_reg(penalty = best_model$hp2,
                         mixture = best_model$hp1) %>%
      set_engine("glmnet") %>%
      set_mode("classification") %>%
      tune_grid(preprocessor = rec,
                resamples = splits_perm,
                metrics = metric_set(bal_accuracy))
  
} else if (best_model$algorithm == "random_forest") {
  models <- rand_forest(mtry = best_model$hp1,
                         min_n = best_model$hp2,
                         trees = best_model$hp3) %>%
      set_engine("ranger",
                 importance = "impurity",
                 respect.unordered.factors = "order",
                 oob.error = FALSE,
                 seed = 102030) %>%
      set_mode("classification") %>%
      tune_grid(preprocessor = rec,
                resamples = splits_perm,
                metrics = metric_set(bal_accuracy))
  
} else if (best_model$algorithm == "knn") {
   models <- nearest_neighbor(neighbors = best_model$hp1) %>% 
      set_engine("kknn") %>% 
      set_mode("classification") %>% 
      tune_grid(preprocessor = rec,
                resamples = splits_perm,
                metrics = metric_set(bal_accuracy))
  
}
  
  
# get results
results_perm <- collect_metrics(models) %>%
      # summarise across repeats
      group_by(.metric, .estimator, .config) %>% 
      summarise(mean = mean(mean), .groups = "drop") %>% 
      pivot_wider(., names_from = ".metric",
                  values_from = "mean") %>%
      select(perm_bal_acc = bal_accuracy) %>% 
      bind_cols(best_model %>% 
                  select(algorithm, feature_set, hp1, hp2, hp3, resample, bal_acc = bal_accuracy), .)   

unlist(models$.metrics) %>% 
  enframe() %>% 
  filter(name == ".estimate") %>% 
  mutate(value = as.numeric(value)) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 8, fill = "light grey", color = "black")

# collect_metrics(models, summarize = FALSE)$.estimate

# save results
write_csv(results_perm, here(path_models, "results_permutated_model.csv")) %>% 
  glimpse()
```


# Best active model compared to best passive

FIX: compare best model to null model

Bayesian correlated t-test (10 x 10 fold)
```{r}
# make splits on non-permuted data
set.seed(102030)
splits <- make_splits(d = data_all, cv_type = "group_kfold_10_x_10", group = group)

rec_null <- recipe(y ~ ., data = d) %>%
  step_string2factor(y, levels = c("no", "yes"))

# fit null model
null_fits <- null_model() %>%
    set_mode("classification") %>%
    fit_resamples(preprocessor = rec_null,
              resamples = splits,
              metrics = metric_set(bal_accuracy))

# pull out best active and passive
# best_active <- results_all %>% 
#     filter(feature_set == "feat_all") %>% 
#     slice_max(bal_accuracy)

# best model
best_passive <- results_all %>% 
  filter(feature_set == "feat_all_passive") %>% 
  slice_max(bal_accuracy)
  
# build recipes
# only best model
rec_active <- build_recipe(d = data_all, job = best_active)
rec_passive <- build_recipe(d = data_all, job = best_passive)

tic()
# change to best_model
if (best_passive$algorithm == "glmnet") {
  best_passive_fits <- logistic_reg(penalty = best_passive$hp2,
                                    mixture = best_passive$hp1) %>%
      set_engine("glmnet") %>%
      set_mode("classification") %>%
      fit_resamples(preprocessor = rec_passive,
                resamples = splits,
                metrics = metric_set(bal_accuracy))
  
} else if (best_passive$algorithm == "random_forest") {
  best_passive_fits <- rand_forest(mtry = best_passive$hp1,
                        min_n = best_passive$hp2,
                        trees = best_passive$hp3) %>%
      set_engine("ranger",
                  importance = "impurity",
                  respect.unordered.factors = "order",
                  oob.error = FALSE,
                  seed = 102030) %>%
      set_mode("classification") %>%
      fit_resamples(preprocessor = rec_passive,
                resamples = splits,
                metrics = metric_set(bal_accuracy))
  
} else if (best_passive$algorithm == "knn") {
    best_passive_fits <- nearest_neighbor(neighbors = best_passive$hp1) %>% 
      set_engine("kknn") %>% 
      set_mode("classification") %>% 
      fit_resamples(preprocessor = rec_passive,
                resamples = splits,
                metrics = metric_set(bal_accuracy))
}
toc()

tic()
if (best_active$algorithm == "glmnet") {
    best_active_fits <- logistic_reg(penalty = best_active$hp2,
                                    mixture = best_active$hp1) %>%
      set_engine("glmnet") %>%
      set_mode("classification") %>%
      tune_grid(preprocessor = rec_active,
                resamples = splits,
                metrics = metric_set(bal_accuracy))
  
} else if (best_active$algorithm == "random_forest") {
    best_active_fits <- rand_forest(mtry = best_active$hp1,
                        min_n = best_active$hp2,
                        trees = best_active$hp3) %>%
      set_engine("ranger",
                  importance = "impurity",
                  respect.unordered.factors = "order",
                  oob.error = FALSE,
                  seed = 102030) %>%
      set_mode("classification") %>%
      tune_grid(preprocessor = rec_active,
                resamples = splits,
                metrics = metric_set(bal_accuracy))
  
} else if (best_active$algorithm == "knn") {
    best_active_fits <- nearest_neighbor(neighbors = best_active$hp1) %>% 
      set_engine("kknn") %>% 
      set_mode("classification") %>% 
      tune_grid(preprocessor = rec_active,
                resamples = splits,
                metrics = metric_set(bal_accuracy))
}
toc()

# save fits
save_rds(best_passive_fits., here(path_models, "best_passive_10_x_10_fold_fits.rds"))
save_rds(best_active_fits., here(path_models, "best_active_10_x_10_fold_fits.rds"))
```

bayesian correlated t test function from IAML
Null fits and best fits - each have 100 bal_accuracy to pass into baysian correlated t
```{r}
# cv_fits_compact from Null
# source
bayesian_correlated_t_test <- function(cv_fits_full, cv_fits_compact, rope_min, rope_max, 
                                       k = 10, plot_min = NULL, plot_max = NULL, plot_n = 1000){
  if (rope_max < rope_min){
    stop("rope_max should be larger than rope_min")
  }
  
  cv_metrics_full <- collect_metrics(cv_fits_full, summarize = FALSE)$.estimate
  cv_metrics_compact <- collect_metrics(cv_fits_compact, summarize = FALSE)$.estimate
  diffs <- cv_metrics_full - cv_metrics_compact 
  delta <- mean(diffs)
  n <- length(diffs)
  df <- n - 1
  stdX <- sd(diffs)
  rho = 1 / k
  sp <- sd(diffs)*sqrt(1/n + rho/(1-rho))
  p.left <- pt((rope_min - delta)/sp, df)
  p.rope <- pt((rope_max - delta)/sp, df)-p.left
  
  results <- list('left'=p.left,'rope'=p.rope,'right'=1-p.left-p.rope)
  
  if (!is.null(plot_min) & !is.null(plot_max)) {
    plot_diffs <- seq(plot_min, plot_max, length.out = plot_n)
    ts <- (plot_diffs - delta) / sp
    pdf <- dt(ts, df)
    results$plot_diffs <- plot_diffs
    results$pdf <- pdf
  }

  return(results)
}
```

```{r}
rope_min <- -0.01
rope_max <- 0.01
plot_min = -.14 
plot_max = .14 # may need to get bigger
results_ttest <- bayesian_correlated_t_test(cv_full, cv_compact, 
                                      rope_min = rope_min, 
                                      rope_max = rope_max, 
                                      k = 10, 
                                      plot_min = plot_min, plot_max = plot_max)

ggplot(mapping = aes(x = results_ttest$plot_diffs, y = results_ttest$pdf)) +
  geom_line() +
  geom_vline(mapping = aes(xintercept  = rope_min), linetype = "dashed") +
  geom_vline(mapping = aes(xintercept  = rope_max), linetype = "dashed") +
  scale_x_continuous(breaks=seq(plot_min, plot_max, .02)) +
  labs(x = "Accuracy Difference (Full - Compact Model)",
       y = "Posterior Probability")

results_ttest$left
results_ttest$rope
results_ttest$right
```

