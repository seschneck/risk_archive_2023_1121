---
title: "Model Performance"
author: "Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
    code_folding: show
editor_options: 
  chunk_output_type: console
---

### Setup

Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

Source training controls 
```{r}
source('./meta/chtc/training/training_controls_model_selection.R')
```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
# library(conflicted) 
#  conflict_prefer("filter", "dplyr")
#  conflict_prefer("select", "dplyr")
#  conflict_prefer("spec", "yardstick")

library(here)  
```

Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
library(tidyverse)  
library(janitor) 
library(lubridate)
library(ggplot2)
library(kableExtra)
library(vip)
library(tictoc)

theme_set(theme_classic()) 
```

Set additional paths
```{r}
path_models <- "meta/ana_scripts/model_output"
path_results <- "P:/studydata/risk/chtc/meta/jobs/training/model_selection/output/results"
path_features <- "P:/studydata/risk/chtc/meta/jobs/training/model_selection/input"
path_figures <- "meta/ana_scripts/figures"
path_study_start <- "P:/studydata/risk/data_processed/meta"
```

Source function scripts from lab support
```{r  source_script, message=FALSE, warning=FALSE}
source(here("meta/fun_meta.R"))
source(here("../lab_support/chtc/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))
```

### Notes
This script does model comparisons on the best models for the meta project.

### Read in results

```{r}
# individual results
result_files <- list.files(path_results, full.names = TRUE)
results_all <- vroom::vroom(result_files, col_types = vroom::cols()) %>% 
  glimpse()

# aggregate results
results_aggregate <- vroom::vroom(here(path_models, "results_aggregate.csv"), col_types = vroom::cols()) %>% 
  glimpse()

# features
data_all <- read_rds(here(path_features, "data_trn.rds")) %>% 
  rename(y = {{y_col_name}}) %>% 
  glimpse()
```


### Model performance by algorithm

Balanced accuracy by algorithm
```{r}
results_aggregate %>% 
  group_by(algorithm) %>% 
  arrange(desc(bal_accuracy)) %>% 
  slice(1) %>% 
  select(algorithm, feature_set, feature_fun_type, bal_accuracy)
```


Subset best fits for each top algorithm model
```{r}
results_best <- results_aggregate %>%
  group_by(algorithm) %>% 
  arrange(desc(bal_accuracy)) %>% 
  slice(1) %>% 
  mutate(algorithm_best = "best") %>% 
  select(-c(accuracy:roc_auc)) %>% 
  right_join(results_all, 
             by = c("algorithm", "feature_set", "feature_fun_type", "resample", "hp1", "hp2", "hp3", "n_feats")) %>% 
  filter(algorithm_best == "best") %>% 
  select(-algorithm_best) %>% 
  glimpse()
```


Refit best glmnet to get metrics across folds   
```{r}
# refit glmnet to pull out fold perf
# Takes awhile to run so saving out results to be read in for knitting
best_glmnet <- results_aggregate %>%
  filter(algorithm == "glmnet") %>%
  slice_max(., bal_accuracy) %>%
  glimpse()
# 
# rec_glmnet <- build_recipe(data_all, best_glmnet)
# # remove nzv if training controls set to TRUE
# if (remove_nzv) {
#   rec_glmnet <- rec_glmnet %>% 
#     step_nzv(all_predictors())
# }
# 
# # replicate chtc splits
# set.seed(102030)
# splits_glmnet <- make_splits(data_all, cv_type, group = group)
# 
# fits_glmnet <- logistic_reg(penalty = best_glmnet$hp2,
#                             mixture = best_glmnet$hp1) %>%
#   set_engine("glmnet") %>%
#   set_mode("classification") %>%
#   fit_resamples(preprocessor = rec_glmnet,
#                 resamples = splits_glmnet,
#                 metrics = metric_set(bal_accuracy))

# compare summary bal_accuracy to chtc
# collect_metrics(fits_glmnet) %>% 
#   glimpse()

# save individual folds
# collect_metrics(fits_glmnet, summarize = FALSE) %>% 
#   pivot_wider(., names_from = `.metric`,
#                 values_from = `.estimate`) %>%   
#   bind_cols(best_glmnet %>% select(-c(accuracy:roc_auc)), .) %>% 
#   select(-c(`.estimator`, `.config`)) %>% 
#   write_csv(., here(path_models, "best_glmnet_fits.csv"))

# read back in glmnet fits
results_glmnet <- vroom::vroom(here(path_models, "best_glmnet_fits.csv"), col_types = vroom::cols()) %>% 
  glimpse()

# avg balance accuracy
mean(results_glmnet$bal_accuracy) # matches CHTC
```

make histograms
```{r}
ba_hist <- results_best %>% 
  filter(algorithm != "glmnet") %>% 
  bind_rows(results_glmnet) %>% 
  group_by(algorithm) %>% 
  ggplot(aes(x = bal_accuracy)) +
  geom_histogram(bins = 10, color = "black", fill = "light grey") +
  facet_wrap(~ algorithm) +
  scale_y_continuous(breaks = seq(1, 10, by = 1), labels = seq(1, 10, by = 1)) +
  xlab("balanced accuracy") +
  labs(title = "model performance across 10 folds") +
  geom_vline(aes(xintercept = mean_ba), results_best %>% 
  filter(algorithm != "glmnet") %>% 
  bind_rows(results_glmnet) %>% 
  group_by(algorithm) %>% 
  summarise(mean_ba = mean(bal_accuracy)), color = "red3")

ba_hist

# save figure
ggsave("ba_hist.png", plot = ba_hist,  path_figures, width = 10, height = 5, units = "in", device = "png",  dpi = 500)
```



### Best model 

Best model
```{r}
best_model <- results_aggregate %>% 
  slice_max(bal_accuracy) %>% 
  glimpse()
```


#### Compared to a Null balanced accuracy of .5   
Bayesian correlated t-test (10 x 10 fold)    
Results from 10 x 10 get saved out so dont need to rerun chunk below everytime
```{r}
# make splits 
# set.seed(102030)
# splits <- make_splits(d = data_all, cv_type = "group_kfold_10_x_10", group = group)
# 
# rec_best <- build_recipe(d = data_all, job = best_model)
# 
# 
# # fit and save out each fold to make data smaller size to work with
# jobs <- expand.grid(n_fold = 1:10,
#                     n_repeat = 1:10) %>% 
#   rowid_to_column(var = "job_num")
# 
# for (i in 1:max(jobs$job_num)) {
#   
#   job <- slice(jobs, i)
#   
#   # fit model
#   features <- make_features(job = job, folds = splits, rec = rec_best, 
#                             cv_type = "group_kfold_10_x_10")
#   feat_in <- features$feat_in
#   feat_out <- features$feat_out
#   
#   if(best_model$algorithm == "random_forest") {
#     fit_best <- rand_forest(mtry = best_model$hp1,
#                             min_n = best_model$hp2,
#                             trees = best_model$hp3) %>%
#       set_engine("ranger",
#                  respect.unordered.factors = "order",
#                  oob.error = FALSE,
#                  seed = 102030) %>%
#       set_mode("classification") %>%
#       fit(y ~ .,
#           data = feat_in)
#   } else if(best_model$algorithm == "knn") {
#     fit_best <- nearest_neighbor(neighbors = best_model$hp1) %>% 
#       set_engine("kknn") %>% 
#       set_mode("classification") %>% 
#       fit(y ~ .,
#           data = feat_in)
#     
#   } else if(best_model$algorithm == "glmnet") {
#     fit_best <- logistic_reg(penalty = best_model$hp2,
#                              mixture = best_model$hp1) %>%
#       set_engine("glmnet") %>%
#       set_mode("classification") %>% 
#       fit(y ~ .,
#           data = feat_in)
#   }
#  
#   
#   # save results
#   get_metrics(model = fit_best, feat_out = feat_out) %>% 
#     pivot_wider(., names_from = "metric",
#                 values_from = "estimate") %>%   
#     mutate(algorithm = best_model$algorithm,
#            rec = "rec_best") %>% 
#     relocate(algorithm, rec) %>% 
#     bind_cols(job, .) %>%
#     write_csv(., here(path_models, 
#                       str_c("best_model_fits/fit_", 
#                             job$job_num, ".csv")))
#   
#   # save predictions for out of sample subids
#   predict(fit_best, feat_out, type = "prob") %>% 
#     select(pred_yes = `.pred_yes`) %>% 
#     bind_cols(feat_out %>% select(subid, dttm_label, y), .) %>% 
#     write_csv(., here(path_models, 
#                       str_c("best_model_preds/preds_", 
#                             job$job_num, ".csv")))
#   
# }
```

Model fits
```{r}
results_best_files <- list.files(here(path_models, "best_model_fits"), 
                              full.names = TRUE)
results_best <- vroom::vroom(results_best_files, col_types = vroom::cols()) %>% 
  glimpse()

results_best %>% 
  tabyl(bal_accuracy)
```


Run bayesian correlate t-test - compare 100 bal_accuracies to NULL (bal_accuracy = .5)
```{r}
# pull out vector of estimates
best_fits <- results_best$bal_accuracy

rope_min <- .49
rope_max <- .51
plot_min = .4
plot_max = .8 # may need to get bigger

# fix pass in bal_accuracy estimates instead of model fits
results_ttest <- bayesian_correlated_t_test(best_fits, 
                                      rope_min = rope_min, 
                                      rope_max = rope_max, 
                                      k = 10, 
                                      plot_min = plot_min, plot_max = plot_max)

best_posterior <- ggplot(mapping = aes(x = results_ttest$plot_diffs, y = results_ttest$pdf)) +
  geom_line() +
  geom_vline(mapping = aes(xintercept  = rope_min), linetype = "dashed") +
  geom_vline(mapping = aes(xintercept  = rope_max), linetype = "dashed") +
  scale_x_continuous(breaks=seq(plot_min, plot_max, .02)) +
  labs(x = "Accuracy Difference (Best - Null Model)",
       y = "Posterior Probability")

best_posterior

# save figure
ggsave("best_posterior.png", plot = best_posterior,  path_figures, width = 9, height = 5, units = "in", device = "png",  dpi = 500)
```

get values
```{r}
results_ttest$left
results_ttest$rope
results_ttest$right
```


#### Individual Predictions

Pull out individual predictions
```{r}
preds_files <- list.files(here(path_models, "best_model_preds"), 
                              full.names = TRUE)
predictions <- vroom::vroom(preds_files, col_types = vroom::cols()) %>% 
  glimpse()
```


Check 10 predictions for each observation (10 x 10 grouped kfold)
```{r}
predictions %>% 
  count(subid, dttm_label) %>% 
  tabyl(n)
```

Average over observations
```{r}
predictions <- predictions %>% 
  group_by(subid, dttm_label, y) %>% 
  summarise(mean = mean(pred_yes), .groups = "drop")
```


plot predictions by participant   

Read in study start dates to get days on study for x axis
```{r}
study_start <- vroom::vroom(here(path_study_start, "study_dates.csv"), col_types = vroom::cols()) %>% 
  mutate(study_start = with_tz(study_start, tzone = "America/Chicago"),
         study_end = with_tz(study_end, tzone = "America/Chicago") + days(1)) %>% 
  select(subid, study_start, study_end)

for(i in 1:nrow(study_start))  {
  subid_dates <- tibble(subid = study_start$subid[i],
                        hour = seq(study_start$study_start[i], 
                                   study_start$study_end[i], "hours"))
  subid_dates <- subid_dates %>% 
    mutate(study_hour = seq(1:nrow(subid_dates)))
  
  study_dates <- if (i == 1) {
    subid_dates
  } else {
    study_dates %>% 
      bind_rows(subid_dates)
  }
}
```

Merge day on study with prediction dttm labels
```{r}
predictions <- predictions %>% 
  left_join(study_dates, by = c("subid", "dttm_label" = "hour")) %>% 
  glimpse()
```



```{r fig.height = 60}
pred_plot <- predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap(~ subid, ncol = 3) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3)

pred_plot

# save plot
ggsave("pred_plot.png", plot = pred_plot,  path_figures, width = 8, height = 60, units = "in", device = "png",  dpi = 500, limitsize = FALSE)
```
