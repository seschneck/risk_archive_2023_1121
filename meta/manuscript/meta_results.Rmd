---
output: pdf_document
---

```{r setup, include=FALSE}
library(here)
library(papaja)
library(knitr)
library(tidyverse)
library(kableExtra)
library(lubridate)
library(ggforce)

theme_set(theme_classic()) 

knitr::opts_chunk$set(echo = FALSE)

path_models <- here("meta/ana_scripts/model_output")
path_study_start <- "P:/studydata/risk/data_processed/meta"
path_results <- "P:/studydata/risk/chtc/meta/jobs/training/model_selection/output/results"

source(here("meta/fun_meta.R"))
source(here("../lab_support/chtc/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))
```


# Results 

```{r}
# individual results
results_all <- read_rds(here(path_models, "results_all.rds"))


# aggregate results
results_aggregate <- vroom::vroom(here(path_models, "results_aggregate.csv"), 
                                  col_types = vroom::cols()) 

# best model
best_model <- results_aggregate %>% 
  slice_max(bal_accuracy)
```

## Participant Characteristics



## Best Model Performance 

We optimized each statistical algorithm by tuning hyperparameter values and fitting models across several feature set combinations (i.e., active or passive, and type of feature engineering). Each model configuration was fit using a grouped 1x10 resampling method. Table 1 shows the best performing model (i.e., highest balanced accuracy) for each statistical algorithm. Figure 1 shows the model's performance in each held out fold. 

Our top performing model, with the highest balanced accuracy, was a random forest statistical algorithm using passive features. To reduce the effects of optimization bias on our model evaluation of predictive performance, we refit the top performing model 100 times (grouped 10x10 resampling). We then averaged across performance estimates to get an estimate with low variance. This method gave us a balanced accuracy estimate of .60. Table 2 shows a confusion matrix where we can see how well the model predicts with negative cases (i.e., no lapse) compared to positive cases (i.e., lapses). Table 3 characterizes our best model over several metrics appropriate for classification.   

Since we did not have an independent held out test set we were not able to completely remove optimization bias. So, we performed a model comparison to assess our model's performance compared to a null model (i.e., intercept only) with a balanced accuracy of .50. A Bayesian correlated t-test revealed a posterior probability that the balanced accuracy of our model was above the Region of Practical Equivalence (ROPE) is .999 (Figure 2). This suggests there is a meaningful difference between our model and a null model with no signal. 

In the appendix we show that our best performing model has variation in predictions for each individual participant. Figure A1 contains predictions that are predicted probabilities of a lapse. Each observation was held out 10 times and the figure shows the averaged probability across these 10 predictions. Actual lapses, are depicted in red.   


```{r}
# Table 1
results_aggregate %>% 
  group_by(algorithm) %>% 
  arrange(desc(bal_accuracy)) %>% 
  slice(1) %>% 
  mutate(feature_set = if_else(feature_set == "feat_all", "active", "passive"),
         algorithm = str_replace(algorithm, "_", " "),
         feature_fun_type = str_replace(feature_fun_type, "_", ", "),
         resample = str_remove(resample, "_1")) %>% 
  select(algorithm, `feature set` = feature_set, `feature type(s)` = feature_fun_type, resample, 
         `balanced accuracy` = bal_accuracy) %>% 
  arrange(desc(`balanced accuracy`)) %>% 
  kbl(booktabs = TRUE,
      digits = 2,
      caption = "Best Balanced Accuracy for Each Statistical Algorithm across 10 Folds during Model Selection") %>% 
  kable_styling()
```



```{r}
# Table 2
results_best_model <- vroom::vroom(here(path_models, "results_best_model.csv"),
                                   col_types = vroom::cols())

preds_best_model <- vroom::vroom(here(path_models, "preds_best_model.csv"),
                                 col_types = vroom::cols())

cm <- preds_best_model %>% 
  mutate(pred_class = if_else(pred_yes >= .5, "yes", "no"),
         pred_class = factor(pred_class, levels = c("no", "yes")),
         y = factor(y, levels = c("no", "yes"))) %>% 
  conf_mat(y, pred_class)

tibble(Prediction = c("no", "yes"),
       no = c(unlist(tidy(cm)[1, 2]), unlist(tidy(cm)[2, 2])),
       yes = c(unlist(tidy(cm)[3, 2]), unlist(tidy(cm)[4, 2]))) %>% 
  kbl(booktabs = TRUE,
      caption = "Confusion Matrix for Best Model (passive random forest)") %>% 
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Truth" = 2)) %>% 
  row_spec(0, align = "c")
```


```{r}
# Table 3
results_best_model %>% 
  summarise(`balanced accuracy` = mean(bal_accuracy),
            accuracy = mean(accuracy),
            sens = mean(sens),
            spec = mean(spec)) %>%
  pivot_longer(everything(), names_to = "metric", values_to = "estimate") %>%
  bind_rows(cm %>%
  summary(event_level = "second") %>%
  select(metric = .metric, estimate = .estimate) %>%
  filter(metric %in% c("ppv", "npv"))) %>%
  bind_rows(results_best_model %>% 
              summarise(`area under the ROC curve` = mean(roc_auc)) %>% 
              pivot_longer(everything(), names_to = "metric", values_to = "estimate")) %>% 
  kbl(booktabs = TRUE,
      digits = 2,
      caption = "Classification Performance Metrics for Best Model (random forest passive) across 100 Folds") %>% 
  kable_styling() %>% 
  pack_rows(start_row = 3, end_row = 6, indent = FALSE) %>% 
  pack_rows(start_row = 7, end_row = 7, indent = FALSE)
```



```{r fig.height = 3}
best_fits <- results_best_model$bal_accuracy
null_fits <- vroom::vroom(here(path_models, "null_model_fits.csv"), col_types = vroom::cols())$bal_accuracy

rope_min <- -.01
rope_max <- .01
plot_min = -.1
plot_max = .3 

results_ttest <- bayesian_correlated_t_test(best_fits, null_fits,
                           rope_min = rope_min, 
                           rope_max = rope_max, 
                           k = 10, 
                           plot_min = plot_min, plot_max = plot_max)

ggplot(mapping = aes(x = results_ttest$plot_diffs, y = results_ttest$pdf)) +
  geom_line() +
  geom_vline(mapping = aes(xintercept  = rope_min), linetype = "dashed") +
  geom_vline(mapping = aes(xintercept  = rope_max), linetype = "dashed") +
  scale_x_continuous(breaks=seq(plot_min, plot_max, .02)) +
  labs(x = "Accuracy Difference (Best - Null Model)",
       y = "Posterior Probability")
```


Figure 1. Model comparison of best model (passive random forest) and null model (intercept only). 

  


## Model Comparison between Active and Passive

Our best performing active model using all the features available was a glmnet algorithm with a balanced accuracy of XX. Table 4 characterizes our best active model using various performance metrics. Figure 3 shows the balanced accuracy estimates for passive and active models over 100 fits. 

A Bayesian correlated t-test revealed there were no differences between the best active and best passive models. The posterior probability that was between the ROPE was .19 (Figure 4). This suggests we can predict lapses just as well with only passive features compared to models with active features.  


```{r}
# Table 4
best_active <- results_aggregate %>%
  filter(feature_set == "feat_all") %>%
  slice_max(bal_accuracy)

results_best_active <- vroom::vroom(here(path_models, "results_best_active.csv"),
                                   col_types = vroom::cols()) %>% 
  select(-feature_set)

preds_best_active <- vroom::vroom(here(path_models, "preds_best_active.csv"),
                                 col_types = vroom::cols())

cm <- preds_best_model %>%
  mutate(pred_class = if_else(pred_yes >= .5, "yes", "no"),
         pred_class = factor(pred_class, levels = c("no", "yes")),
         y = factor(y, levels = c("no", "yes"))) %>%
  conf_mat(y, pred_class)

results_best_active %>%
  summarise(`balanced accuracy` = mean(bal_accuracy),
            accuracy = mean(accuracy),
            sens = mean(sens),
            spec = mean(spec)) %>%
  pivot_longer(everything(), names_to = "metric", values_to = "estimate") %>%
  bind_rows(cm %>%
  summary(event_level = "second") %>%
  select(metric = .metric, estimate = .estimate) %>%
  filter(metric %in% c("ppv", "npv"))) %>%
  bind_rows(results_best_active %>% 
              summarise(`area under the ROC curve` = mean(roc_auc)) %>% 
              pivot_longer(everything(), names_to = "metric", values_to = "estimate")) %>% 
kbl(booktabs = TRUE,
    digits = 2,
    caption = "Classification Performance Metrics for Best Active Model (glmnet) across 100 Folds") %>%
  kable_styling() %>% 
  pack_rows(start_row = 3, end_row = 6, indent = FALSE) %>% 
  pack_rows(start_row = 7, end_row = 7, indent = FALSE)
```

```{r fig.height = 3}
results_best_model %>%
  bind_rows(results_best_active) %>% 
  rename(feature_set = rec) %>% 
  mutate(feature_set = if_else(feature_set == "rec_best", "Passive (random forest)", "Active (glmnet)")) %>%
  group_by(feature_set) %>%
  ggplot(aes(x = bal_accuracy)) +
  geom_histogram(bins = 20, color = "black", fill = "light grey") +
  facet_wrap(~ feature_set) +
  xlab("balanced accuracy") +
  labs(title = "Model performance across 100 folds") +
  geom_vline(aes(xintercept = mean_ba), results_best_model %>%
  bind_rows(results_best_active) %>%
  rename(feature_set = rec) %>% 
  mutate(feature_set = if_else(feature_set == "rec_best", "Passive (random forest)", "Active (glmnet)")) %>%
  group_by(feature_set) %>%
  summarise(mean_ba = mean(bal_accuracy)), color = "red3")
```

Figure 2. Histogram of model fits across 100 folds. (FIX: 14 folds still running so only 86 splits in active)


```{r fig.height = 3}
best_passive_fits <- results_best_model$bal_accuracy
best_active_fits <- results_best_active$bal_accuracy

rope_min <- -.01
rope_max <- .01
plot_min = -.20
plot_max = .20

results_ttest <- bayesian_correlated_t_test(best_active_fits,
                                            best_passive_fits,
                                            rope_min = rope_min,
                                            rope_max = rope_max,
                                            k = 10,
                                            plot_min = plot_min,
                                            plot_max = plot_max)

ggplot(mapping = aes(x = results_ttest$plot_diffs,
                                                 y = results_ttest$pdf)) +
  geom_line() +
  geom_vline(mapping = aes(xintercept  = rope_min), linetype = "dashed") +
  geom_vline(mapping = aes(xintercept  = rope_max), linetype = "dashed") +
  scale_x_continuous(breaks=seq(plot_min, plot_max, .02)) +
  labs(x = "Accuracy Difference (Active - Passive Model)",
       y = "Posterior Probability")
```

Figure 3. Model comparison of the best passive (random forest) and best active (glmnet) models.


## Top Features

We conducted a permutation test of feature importance to see which features contributed most to our top performing model. Figure 4 shows which features when removed from the model had the greatest effect on performance. Figure 5 shows the top features for the best active model.


```{r fig.height = 5, fig.width = 8}
vi_best <- read_rds(here(path_models, str_c("vi_", best_model$algorithm, ".rds")))
algorithm <- if_else(best_model$algorithm == "random_forest", "random forest",
                     best_model$algorithm)
feat_set <- if_else(best_model$feature_set == "feat_all", "active", "passive")
vi_best %>% 
      mutate(Variable = str_remove(Variable, ".passive"),
             Variable = str_remove(Variable, ".l0"),
             Variable = str_remove(Variable, ".org"),
             Variable = str_remove(Variable, ".dttm_obs"),
             Variable = str_replace(Variable, "pratecount", "perc_rate"),
             Variable = str_replace(Variable, "rratecount", "raw_rate"),
             Variable = str_replace(Variable, "pratesum_duration", "perc_sum.duration"),
             Variable = str_replace(Variable, "pmean_duration", "perc_mean.duration"),
             Variable = str_replace(Variable, "rmean_duration", "raw_mean.duration"),
             Variable = str_replace(Variable, "ppropdatetime", "perc_prop"),
             Variable = str_replace(Variable, "rpropdatetime", "raw_prop"),
             Variable = str_replace(Variable, "rpropcount", "raw_prop"),
             Variable = str_replace(Variable, "ppropcount", "perc_prop"),
             Variable = str_replace(Variable, "p6", "6hrs"),
             Variable = str_replace(Variable, "p12", "12hrs"),
             Variable = str_replace(Variable, "p24", "24hrs"),
             Variable = str_replace(Variable, "p48", "48hrs"),
             Variable = str_replace(Variable, "p72", "72hrs"),
             Variable = str_replace(Variable, "p168", "168hrs"),
             Variable = fct_reorder(Variable, Importance)) %>% 
      filter(Importance != 0) %>% 
      ggplot(aes(x = Importance, y = Variable)) +
      geom_col(color = "black", fill = "light grey") +
      scale_x_continuous(expand = c(0, 0)) +
      labs(y = NULL, title = str_c("Top features for best model (", feat_set, " ", algorithm, ")"))
```

Figure 4. Feature importance scores for best performing model (Passive Random Forest). 



```{r fig.height = 3}
vi_active <- read_rds(here(path_models, str_c("vi_glmnet.rds")))
vi_active %>%
      mutate(Variable = str_remove(Variable, ".passive"),
             Variable = str_remove(Variable, ".l0"),
             Variable = fct_reorder(Variable, Importance)) %>%
      filter(Importance != 0) %>%
      ggplot(aes(x = Importance, y = Variable)) +
      geom_col(color = "black", fill = "light grey") +
      scale_x_continuous(expand = c(0, 0)) +
      labs(y = NULL, title = "Top features for best active model (glmnet)")
```

Figure 5. Feature importance scores for best performing active model (glmnet). 


# Appendix

```{r fig.fullwidth = TRUE, fig.height = 11}
# average over predictions
predictions <- preds_best_model %>% 
  group_by(subid, dttm_label, y) %>% 
  summarise(mean = mean(pred_yes), .groups = "drop")

# read in study start dates for x-axis
study_start <- vroom::vroom(here(path_study_start, "study_dates.csv"), col_types = vroom::cols()) %>% 
  mutate(study_start = with_tz(study_start, tzone = "America/Chicago"),
         study_end = with_tz(study_end, tzone = "America/Chicago") + days(1)) %>% 
  select(subid, study_start, study_end)

for(i in 1:nrow(study_start))  {
  subid_dates <- tibble(subid = study_start$subid[i],
                        hour = seq(study_start$study_start[i], 
                                   study_start$study_end[i], "hours"))
  subid_dates <- subid_dates %>% 
    mutate(study_hour = seq(1:nrow(subid_dates)))
  
  study_dates <- if (i == 1) {
    subid_dates
  } else {
    study_dates %>% 
      bind_rows(subid_dates)
  }
}

predictions <- predictions %>% 
  left_join(study_dates, by = c("subid", "dttm_label" = "hour"))

# plot
predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap_paginate(~ subid, ncol = 3, nrow = 10, page = 1) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3)
cat('\r\n\r\n')

predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap_paginate(~ subid, ncol = 3, nrow = 10, page = 2) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3) 
cat('\r\n\r\n')

predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap_paginate(~ subid, ncol = 3, nrow = 10, page = 3) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3) 
cat('\r\n\r\n')

predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap_paginate(~ subid, ncol = 3, nrow = 10, page = 4) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3) 
cat('\r\n\r\n')

predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap_paginate(~ subid, ncol = 3, nrow = 11, page = 5) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3)
```


Figure A1. Predicted probabilities of lapse for each participant. A grouped 10x10 resampling method was used to obtain these probabilities. Known lapses are in red. The red dashed line represents the threshold for classifying a probability as a lapse (i.e., everything above the line was predicted to be a lapse).  

