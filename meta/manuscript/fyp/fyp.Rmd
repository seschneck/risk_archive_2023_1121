---
title             : "Personal Sensing of Smartphone Communications to Support Recovery from Alcohol Use Disorder"
shorttitle        : "Personal Sensing for AUD"

author: 
  - name          : "Kendra Wyant"
    affiliation   : "1"
    corresponding : yes    
    address       : "1202 West Johnson St., Madison, WI 53706"
    email         : "kpaquette2@wisc.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing


affiliation:
  - id            : "1"
    institution   : "Department of Psychology, University of Wisconsin-Madison"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography: meta.bib
csl: journal-of-medical-internet-research.csl

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf

header-includes:
  - \raggedbottom
  - \usepackage{booktabs}
  - \definecolor{lightred}{rgb}{1.0, 0.6, 0.6}
  
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk/manuscripts/meta/fyp", 
      "/Volumes/private/studydata/risk/manuscripts/meta/fyp")
    )
  })
---

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}

library(conflicted) # detect and warn about function conflicts
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")

library(here)  # establish project directory consistently as working directory
```

Absolute paths
```{r, paths}

switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_study_start <- "P:/studydata/risk/data_processed/meta"
          path_results <- "P:/studydata/risk/chtc/meta/jobs/training/model_selection/output/results"},

        # IOS paths
        Darwin = {
          path_study_start <- "/Volumes/private/studydata/risk/data_processed/meta"
          path_results <- "/Volumes/private/studydata/risk/chtc/meta/jobs/training/model_selection/output/results"}
        )
```


Relative paths
```{r}
path_models <- "meta/ana_scripts/model_output"
```

Packages & Source
```{r setup, include = FALSE}
library("papaja")
library(tidyverse)
library(tidymodels)
library(vroom)
library(kableExtra)
library(lubridate)
library(ggplot2)
library(ggforce)

source(here("meta/fun_meta.R"))
source(here("../lab_support/chtc/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))
```


## Alcohol Use Disorder
Paragraph on harms/costs/frequency of AUD

Alcohol Use Disorder is a chronic relapsing disease [@rounsavilleLapseRelapseChasing2010; @scottPathwaysRelapsetreatmentrecoveryCycle2005]. People can relapse days, weeks, months, or even years after achieving abstinence [@witkiewitzLapsesFollowingAlcohol2008; @jinPredictorsRelapseLongterm1998; @anderssonRelapseInpatientSubstance2018; @millerHowEffectiveAlcoholism2001a]. Lapses, an initial setback or slip, are often an early warning sign of relapse, a full return to previous drinking behavior [@witkiewitzRelapsePreventionAlcohol2004]. Studies show that lapses predict future lapses, with more frequent lapses resulting in increased chances of relapse [@rounsavilleLapseRelapseChasing2010]. Likewise, longer durations of abstinence is associated with decreased chances of lapse, suggesting the stability of a patient’s recovery itself as an important predictor of abstinence [@rounsavilleLapseRelapseChasing2010]. In fact, the most important predictor of relapse is whether the individual has already had a lapse during treatment [@hogstrombrandtPredictionSingleEpisodes1999]. This high correlation between an initial lapse and relapse is the abstinence violation effect, which states that people who internalize feelings of loss of control, guilt, and hopelessness after violating a self-imposed rule (i.e., abstinence) have a greater risk of relapse compared to those who view the lapse as external and controllable [@marlattRelapsePreventionAlcohol; @hogstrombrandtPredictionSingleEpisodes1999]. Thus, identifying when an initial lapse will occur is an important goal in preventing lapses, repeated lapses, and relapse. 

Like most mental health disorders, Alcohol Use Disorder can be characterized by fluctuations in affective and behavioral states that covary with the severity of the underlying disorder and impact its treatment. The ability to detect these changes offers the opportunity to selectively deliver different interventions to patients that match their needs in that moment.  For example, during periods of stability, patients in recovery from Alcohol Use Disorder might benefit from interventions that help them modify their social network and daily activities to include more time with family and friends who support their recovery. In contrast, during times of peak stress and associated alcohol craving, they might need focused interventions that prevent an imminent lapse back to alcohol use. 

## Lapse Risk Factors
Lapses may seem to come out of nowhere, but they are often preceded by changes in cognitive (e.g., negative affect, craving), situational (e.g., lack of social support, risky situations), and behavioral (e.g., decreased social interactions, coping strategies) factors [@rounsavilleLapseRelapseChasing2010; @jinPredictorsRelapseLongterm1998; @anderssonRelapseInpatientSubstance2018]. 

These risk factors are known as proximal risk factors that are fluid and change over time (much like how mental health disorder symptoms can ebb and flow through periods of stability and periods of lapse) [@rounsavilleLapseRelapseChasing2010; @witkiewitzLapsesFollowingAlcohol2008]. Proximal risk factors contrast distal risk factors - stable states or traits that contribute to a mental health disorder, but do not change much over time. For example, one’s genetic makeup, the presence of co-occurring psychopathology, or Alcohol Use Disorder severity may contribute to whether one is predisposed to Alcohol Use Disorder, but they are less involved in the cyclical nature of lapses [@huffordRelapseNonlinearDynamic2003]. On the other hand, fluctuations in proximal factors often precipitate a lapse [@witkiewitzRelapsePreventionAlcohol2004; @chihPredictiveModelingAddiction2014; @larimerRelapsePrevention1999]. But due to the dynamic nature of these factors, traditional treatments (e.g., monthly therapy or counseling sessions) may not be best suited for monitoring for changes in lapse risk. 

Not all proximal risk factors are detectable within the same time window. Situational factors, such as passing a bar or being around people who are drinking may immediately precede a lapse. Other stressors, such as getting upsetting news from a family member or increased financial strain, may slowly increase lapse risk over hours, days or weeks. Thus, a system for continuously monitoring a person's lapse risk is critical in detecting when someone will lapse. 

## Personal Sensing
With new advances in technology, we can now accomplish this through personal sensing, a longitudinal in situ measurement approach for collecting data via smartphone sensors, logs, and social media apps [@mohrPersonalSensingUnderstanding2017]. That is we can harness personal sensing data to capture fluctuations in lapse risk in real time.

Active-passive continuum (to support logs as being low burden).

GPS and EMA studies (GPS may detect immediate risk factors and EMA is active).

## Cellular Communication Logs
One understudied personal sensing method in the lapse risk literature is cellular communication logs. 

Cellular communication logs are both passive and they have the potential to capture risk factors on varying timetables. We may be able to capture immediate risk based on who someone is calling or what time of day it is. Additionally, slower acting risk indicators may also be detected. Decreased interactions may signify isolation common with depressive symptoms, reaching out to people in one’s social network could signify a positive coping strategy, or changes in patterns between a single person in one’s social network could indicate conflict [@millerHowEffectiveAlcoholism2001a; @huffordRelapseNonlinearDynamic2003; @chihPredictiveModelingAddiction2014].

Not only does it have potential predictive power, but it also appears to be a feasible source of personal sensing data. In a smartphone-based sensing platform the primary expense on the individual is the smartphone. Smartphone usage is already widespread. Eighty-five percent of US adults have a smartphone and this number is consistent across all sociodemographic groups, including those in recovery programs for substance use [@pewresearchcenterMobileFactSheet2021; @massonHealthrelatedInternetUse2019]. 

Additionally, people generally find personal sensing of cellular communication logs to be acceptable (cite Burden). 

## Social Context
Many proximal lapse predictors are inherently social [@hunter-reelEmphasizingInterpersonalFactors2009]. For example, decreased interactions may signify isolation common with depressive symptoms, reaching out to people in one’s social network could signify a positive coping strategy, or changes in patterns between a single person in one’s social network could indicate conflict. Accordingly, these factors are intrinsically linked to social interactions [@hunter-reelEmphasizingInterpersonalFactors2009; @witkiewitzEmphasisInterpersonalFactors2005; @stantonRelapsePreventionNeeds2005].

On its own, the nature of people’s social interactions with others appears to be a salient factor relevant to lapse risk. Positive social interaction has been associated with positive alcohol treatment outcomes [@grohFriendsFamilyAlcohol2010; @kellyRoleAlcoholicsAnonymous2012; @zywiakDecomposingRelationshipsPretreatment2002; @longabaughNetworkSupportPrognostic2010]. It is thought that the social component of self-help groups like Alcoholics Anonymous (i.e., increasing pro-abstinence relationship ties) play a major contributing role in their efficacy [@kellyRoleAlcoholicsAnonymous2012; @grohSocialNetworkVariables2008; @woffServiceProvidersPerceptions1996; @humphreysEnhancedFriendshipNetworks1999]. The buffering hypothesis of social support suggests that social relationships may buffer individuals against the negative effects of stress by providing a source of resources to promote adaptive responses to stress [@cohenStressSocialSupport; @holt-lunstadSocialRelationshipsMortality2010]. With stress being an often-cited precipitant of alcohol lapse, it is likely these buffering effects extrapolate to lapse risk [@fronkStressAllostasisSubstance2020]. 

There is also much literature on the importance of social network influences on drinking behavior [@hunter-reelEmphasizingInterpersonalFactors2009; @alvarezSocialNetworkHeavy2021]. A social network made up of heavy drinkers or abstainers can ultimately influence one’s recovery towards maintaining abstinence or lapsing [@alvarezSocialNetworkHeavy2021; @gordonSocialNetworksRecovery1991; @mohrGettingGettingHigh2001]. Individuals in recovery who maintain interactions with people who do not support their recovery is associated with decreased likelihood of maintaining abstinence. Social interactions with negative peer influences (e.g., heavy drinkers) may have more of an effect on treatment outcome than positive peer influences (e.g., abstainers) [@alvarezSocialNetworkHeavy2021]. This relationship between social support and lapses is robust and extends to substances other than alcohol [@havassySocialSupportRelapse1991]. 

Since many social interactions occur or are planned over the phone, cellular communication logs offer insight into patterns of social interaction. We know that social relationships are dynamic [@hidalgoDynamicsMobilePhone2008; @sekaraFundamentalStructuresDynamic2016; @saramakiSecondsMonthsOverview2015; @kossinetsEmpiricalAnalysisEvolving2006]. So, with time-stamped logs there is potential to build a sensing system that tracks how these interactions fluctuate over time. For example, changes in their top contacts (i.e., who they are interacting with the most) or changes in patterns of activities (e.g., is an individual is consistently ignoring phone calls from a contact they normally interact often with?). 

However, these communications may become much more powerful when they are contextualized (e.g., pleasantness of interactions with the contact, whether the contact is supportive of their recovery, drinking history with contact).  For example, context in concert with communication logs can tell us that someone is ignoring phone calls from a former drinking buddy or that their top contacts have recently shifted to include only people supportive of their recovery. So, we can imagine that providing this social context to cellular communication logs could greatly increase the predictive signal. Take one more example - an outgoing phone call late at night might be related to lapse risk, but the direction will be very different if they are calling their sponsor vs. a former drinking buddy.

## Present Study
- Build a personal sensing system to predict lapse risk from contextualized cellular communication logs (extend our current understanding of distal risk factors by identifying additional proximal risk factors).
- Evaluate the incremental contribution to the sensitivity of this sensing system provided by measures of contextual information regarding the people with whom the participant is communicating (e.g., pleasantness of interactions with the contact, whether the contact is supportive of their recovery, drinking history with contact).
- Identify the optimal time window for lapse risk with contextualized cellular communication logs.
- Evaluate the importance of feature sets within the top performing model to inform potential in situ treatments. 


# Method
This project performs the initial analyses on a subset of data collected from 2017 – 2019 as part of a larger grant funded through the National Institute of Alcohol Abuse and Alcoholism (R01 AA024391). A full list of measures used in the parent project can be found at <https://osf.io/brs68/>.

## Research Transparency
<!-- This section is taken from burden paper - need to rewrite? -->
<!--JJC: Is this all true?  You did all of this?  If so, looks good to include here-->
We value the principles of research transparency that are essential to the robustness and reproducibility of science [@schonbrodtVoluntaryCommitmentResearch2015]. Consequently, we maximized transparency through several complementary methods. First, we reported how we determined our sample size, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]. Second, we completed a transparency checklist, which can be found in the supplement of this paper [@aczelConsensusbasedTransparencyChecklist2019]. Third, we made the data, analysis scripts and annotated results, self-report surveys, and other study materials associated with this study publicly available [<https://osf.io/brs68/>].

## Participants
We recruited a community sample of people in initial stages of recovery for Alcohol Use Disorder from the Madison area. Avenues of recruitment included referrals from clinics, self-help groups, Facebook, radio, and television. Those who were interested in participating in our three-month study were given a brief description (i.e., told that our research focused on learning how mobile health technology can be used to provide individual support to anyone recovering from alcohol addiction) a short phone screen to determine initial eligibility (i.e., At least 18 years old, ability to read and write in English, and an eligible smartphone with existing cellphone plan).

Two hundred sixteen participants passed the initial phone screen and came in for a more in-depth screening session. Of the 216 interested participants, 199 enrolled in the study (i.e., they consented and were eligible to participate). We excluded potential participants if they did not meet the criteria for moderate or severe Alcohol Use Disorder (as defined by the DSM-5), did not have a goal of long-term abstinence, had not abstained from alcohol for at least one week, already had over two months of abstinence, or had severe symptoms of psychosis or paranoia. Of the 199 participants enrolled in the study, 154 provided at least one month of communications logs and EMA surveys. This was an additional requirement for the current study because cellphone communications were collected at each one month follow-up. We dropped data from three participants due to concerns quality of their data. Our final analyses were on a sample of 151 participants. <!--JJC: Any benefit to using your flow chart from BURDEN paper here? Or is that not well tuned to the meta project.   Not worth adding work but if its easy to adapt for this, might be nice-->

## Procedure
Our study involved five in-person visits (screening, enrollment, and three follow-up visits). It also required completion of daily EMA surveys to document alcohol lapses, and access to non-deleted text message and call logs (i.e., cellular communication logs). All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

During the screening session we obtained informed consent, determined eligibility, and documented basic demographic information. Participants that consented and were deemed eligible came back for a second enrollment visit. During enrollment, participants were briefed on how to delete log entries they did not want to share with us and completed a practice EMA survey. Participants also reported contacts they frequently communicated with and answered a series of questions documenting contextual information about their interactions with each contact (type of relationship with contact, whether they drank with contact in past, drinking status of contact, whether contact would drink in their presence, whether the contact is in recovery, the level of supportiveness the contact provides, and the pleasantness of their interactions with the contact). Participants returned for three follow-up visits, each one month apart. 

At each follow-up visit, we downloaded participants' cellular communication logs. These logs included the phone number of the other party, whether the call or message was incoming or outgoing, the duration of the call, and the date and time of the call or message. Participants also provided context information for newly identified frequent contacts (i.e., at least two communications in the past month). At the third follow-up visit, participants were debriefed and thanked for their participation. 

## Feature Engineering
<!--have a section that describes how you did feature engineering.  This is separate from the data analysis plan or at least a sub section of that plan.  It can be short but talk about how you calculated raw and  percent change. How you used different windows of data preceding the lapse, and how you combined this with context.  You might also describe some of the pre-processing that you did to the phone numbers if you want to.  Not needed but will give them a window into how much data processing is needed for a project like this-->

## Data Analysis Plan

We conducted all analyses in R version 4.0.3 [@rcoreteamLanguageEnvironmentStatistical2021] using RStudio [@rstudioteamRStudioIntegratedDevelopment2020a] and the tidyverse and tidymodels ecosystem of packages [@wickhamWelcomeTidyverse2019; @kuhnTidymodelsCollectionPackages2020]. 

<!--Describe the stats you will provide to characterize the sample.  Demographics, etc?-->

Our first study aim was to train and evaluate the best performing machine learning model to predict alcohol lapse onset from contextualized cellular communication data. We built, trained, and evaluated models with several statistical learning algorithms including penalized parametric linear classification algorithms (LASSO, ridge regression, glmnet), non-parametric classification algorithms (k nearest neighbor), and ensemble methods (random forest). Candidate statistical learning algorithms were trained on a subset of the data (i.e., k-fold cross validation) using combinations of features derived from participants cellular communications and social context information<!--JJC: decide how to refer to the context and then use the same term throughout.  Here you use "social" context but you didnt use "social" earlier-->. 

Balanced accuracy was our performance metric for both model selection and evaluation.  However, we also report other performance metrics (sensitivity, specificity, positive predictive value, negative predictive value) to fully characterize model performance of our best performing model(s).  We also visually inspected lapse probability predictions by participant to further probe model performance.

We used grouped 10-fold resampling without repeats (i.e, 1 set of 10 grouped folds) to estimate balanced accuracy to select the top performing model (statistical algorithm and combination of features).  All folds were grouped by participant ID so that a single participant's data were not being used to predict future data by the same participant. In other words, all data from a participant was either included in the training or held-out validation set, but not both.  <!-- Maybe consider calling sets either training or validation since we didnt have a "test" set?-->

We estimated model performance for our top performing model using 10 repeats of grouped 10-fold resampling (i.e., grouped 10x10-fold). We opted to use resampling for model evaluation instead of a held-out test set because, with our limited sample size, averaging balanced accuracy over 100 held-out validation sets gives us a more stable (i.e., low variance) and less biased estimate of model performance than other available options using a single, explicitly held-out, test set. 

Finally, we used a Bayesian correlated t-test to compare our top model's balanced accuracy across the 100 validation sets to the expected performance of a null model with no predictive signal(i.e., balanced accuracy = .50). In this analysis, we defined a meaningful difference to be more than a difference of 1% in either direction (i.e., Region of Practical Equivalence [ROPE] set to .49 - .51).

We used various feature engineering methods to build groups of feature sets to maximize model performance. A key distinction between features was to discriminate between features derived from passive only measures (i.e., communication logs) and those derived from more active measures (i.e., context information). Our study's second aim was to compare models that use all available features (both passive signals from communications logs and actively measured context) with models restricted to only passive signals. To do this we employed a model comparison approach. Our top passive and active models were evaluated using grouped 10x10-fold resampling. We then used a Bayesian correlated t-test to compare balanced accuracy across the 100 validation sets from our top model trained on all data with our top model trained on only passive features. We defined a meaningful difference between models to be more than a 1% difference in either direction (i.e., ROPE set to -.01 - .01). Through this relative comparison, we quantified any performance benefit from adding the active component of context. 

Our third aim of the present study was to evaluate the importance of features within the top performing models. The most predictive features were identified based on an algorithm agnostic feature importance index. This approach involves permuting each individual feature to determine how much predictive signal it adds to the model. Since, this only tells us whether a feature is important and not the direction of its predictive value, we also examined coefficients from our linear glmnet model for a more interpretable explanation. <!--JJC: drop this last sentence if we dont report this - and I tend to think we wont report it-->



# Results

```{r}
# individual results
results_all <- read_rds(here(path_models, "results_all.rds"))


# aggregate results
results_aggregate <- vroom(here(path_models, "results_aggregate.csv"), 
                                  col_types = vroom::cols()) 

# best model
best_model <- results_aggregate %>% 
  slice_max(bal_accuracy)
```

## Participant Characteristics



## Best Model Performance 

We optimized each statistical algorithm by tuning hyperparameter values and fitting models across several feature set combinations (i.e., active or passive, and type of feature engineering). Each model configuration was fit using a grouped 1x10 resampling method. Table 1 shows the best performing model (i.e., highest balanced accuracy) for each statistical algorithm. Figure 1 shows the model's performance in each held out fold. 

Our top performing model, with the highest balanced accuracy, was a random forest statistical algorithm using passive features. To reduce the effects of optimization bias on our model evaluation of predictive performance, we refit the top performing model 100 times (grouped 10x10 resampling). We then averaged across performance estimates to get an estimate with low variance. This method gave us a balanced accuracy estimate of .60. Table 2 shows a confusion matrix where we can see how well the model predicts with negative cases (i.e., no lapse) compared to positive cases (i.e., lapses). Table 3 characterizes our best model over several metrics appropriate for classification.   

Since we did not have an independent held out test set we were not able to completely remove optimization bias. So, we performed a model comparison to assess our model's performance compared to a null model (i.e., intercept only) with a balanced accuracy of .50. A Bayesian correlated t-test revealed a posterior probability that the balanced accuracy of our model was above the Region of Practical Equivalence (ROPE) is .999 (Figure 2). This suggests there is a meaningful difference between our model and a null model with no signal. 

In the appendix we show that our best performing model has variation in predictions for each individual participant. Figure A1 contains predictions that are predicted probabilities of a lapse. Each observation was held out 10 times and the figure shows the averaged probability across these 10 predictions. Actual lapses, are depicted in red.   


```{r}
# Table 1
results_aggregate %>% 
  group_by(algorithm) %>% 
  arrange(desc(bal_accuracy)) %>% 
  slice(1) %>% 
  mutate(feature_set = if_else(feature_set == "feat_all", "active", "passive"),
         algorithm = str_replace(algorithm, "_", " "),
         feature_fun_type = str_replace(feature_fun_type, "_", ", "),
         resample = str_remove(resample, "_1")) %>% 
  select(algorithm, `feature set` = feature_set, `feature type(s)` = feature_fun_type, resample, 
         `balanced accuracy` = bal_accuracy) %>% 
  arrange(desc(`balanced accuracy`)) %>% 
  kbl(booktabs = TRUE,
      digits = 2,
      caption = "Best Balanced Accuracy for Each Statistical Algorithm across 10 Folds during Model Selection") %>% 
  kable_styling()
```



```{r}
# Table 2
results_best_model <- vroom(here(path_models, "results_best_model.csv"),
                                   col_types = vroom::cols())

preds_best_model <- vroom(here(path_models, "preds_best_model.csv"),
                                 col_types = vroom::cols())

cm <- preds_best_model %>% 
  mutate(pred_class = if_else(pred_yes >= .5, "yes", "no"),
         pred_class = factor(pred_class, levels = c("no", "yes")),
         y = factor(y, levels = c("no", "yes"))) %>% 
  conf_mat(y, pred_class)

tibble(Prediction = c("no", "yes"),
       no = c(unlist(tidy(cm)[1, 2]), unlist(tidy(cm)[2, 2])),
       yes = c(unlist(tidy(cm)[3, 2]), unlist(tidy(cm)[4, 2]))) %>% 
  kbl(booktabs = TRUE,
      caption = "Confusion Matrix for Best Model (passive random forest)") %>% 
  kable_styling() %>% 
  add_header_above(c(" " = 1, "Truth" = 2)) %>% 
  row_spec(0, align = "c")
```


```{r}
# Table 3
results_best_model %>% 
  summarise(`balanced accuracy` = mean(bal_accuracy),
            accuracy = mean(accuracy),
            sens = mean(sens),
            spec = mean(spec)) %>%
  pivot_longer(everything(), names_to = "metric", values_to = "estimate") %>%
  bind_rows(cm %>%
  summary(event_level = "second") %>%
  select(metric = .metric, estimate = .estimate) %>%
  filter(metric %in% c("ppv", "npv"))) %>%
  bind_rows(results_best_model %>% 
              summarise(`area under the ROC curve` = mean(roc_auc)) %>% 
              pivot_longer(everything(), names_to = "metric", values_to = "estimate")) %>% 
  kbl(booktabs = TRUE,
      digits = 2,
      caption = "Classification Performance Metrics for Best Model (random forest passive) across 100 Folds") %>% 
  kable_styling() %>% 
  pack_rows(start_row = 3, end_row = 6, indent = FALSE) %>% 
  pack_rows(start_row = 7, end_row = 7, indent = FALSE)
```



```{r fig.height = 3}
best_fits <- results_best_model$bal_accuracy
null_fits <- vroom(here(path_models, "null_model_fits.csv"), col_types = vroom::cols())$bal_accuracy

rope_min <- -.01
rope_max <- .01
plot_min = -.1
plot_max = .3 

results_ttest <- bayesian_correlated_t_test(best_fits, null_fits,
                           rope_min = rope_min, 
                           rope_max = rope_max, 
                           k = 10, 
                           plot_min = plot_min, plot_max = plot_max)

ggplot(mapping = aes(x = results_ttest$plot_diffs, y = results_ttest$pdf)) +
  geom_line() +
  geom_vline(mapping = aes(xintercept  = rope_min), linetype = "dashed") +
  geom_vline(mapping = aes(xintercept  = rope_max), linetype = "dashed") +
  scale_x_continuous(breaks=seq(plot_min, plot_max, .02)) +
  labs(x = "Accuracy Difference (Best - Null Model)",
       y = "Posterior Probability")
```


Figure 1. Model comparison of best model (passive random forest) and null model (intercept only). 

  


## Model Comparison between Active and Passive

Our best performing active model using all the features available was a glmnet algorithm with a balanced accuracy of .60. Table 4 characterizes our best active model using various performance metrics. Figure 3 shows the balanced accuracy estimates for passive and active models over 100 fits. 

A Bayesian correlated t-test revealed there were no differences between the best active and best passive models. The posterior probability that was between the ROPE was .34 (Figure 4). This suggests we can predict lapses just as well with only passive features compared to models with active features.  


```{r}
# Table 4
best_active <- results_aggregate %>%
  filter(feature_set == "feat_all") %>%
  slice_max(bal_accuracy)

results_best_active <- vroom(here(path_models, "results_best_active.csv"),
                                   col_types = vroom::cols()) 

preds_best_active <- vroom(here(path_models, "preds_best_active.csv"),
                                 col_types = vroom::cols())

cm <- preds_best_model %>%
  mutate(pred_class = if_else(pred_yes >= .5, "yes", "no"),
         pred_class = factor(pred_class, levels = c("no", "yes")),
         y = factor(y, levels = c("no", "yes"))) %>%
  conf_mat(y, pred_class)

results_best_active %>%
  summarise(`balanced accuracy` = mean(bal_accuracy),
            accuracy = mean(accuracy),
            sens = mean(sens),
            spec = mean(spec)) %>%
  pivot_longer(everything(), names_to = "metric", values_to = "estimate") %>%
  bind_rows(cm %>%
  summary(event_level = "second") %>%
  select(metric = .metric, estimate = .estimate) %>%
  filter(metric %in% c("ppv", "npv"))) %>%
  bind_rows(results_best_active %>% 
              summarise(`area under the ROC curve` = mean(roc_auc)) %>% 
              pivot_longer(everything(), names_to = "metric", values_to = "estimate")) %>% 
kbl(booktabs = TRUE,
    digits = 2,
    caption = "Classification Performance Metrics for Best Active Model (glmnet) across 100 Folds") %>%
  kable_styling() %>% 
  pack_rows(start_row = 3, end_row = 6, indent = FALSE) %>% 
  pack_rows(start_row = 7, end_row = 7, indent = FALSE)
```

```{r fig.height = 3}
results_best_model %>%
  bind_rows(results_best_active) %>% 
  rename(feature_set = rec) %>% 
  mutate(feature_set = if_else(feature_set == "rec_best", "Passive (random forest)", "Active (glmnet)")) %>%
  group_by(feature_set) %>%
  ggplot(aes(x = bal_accuracy)) +
  geom_histogram(bins = 20, color = "black", fill = "light grey") +
  facet_wrap(~ feature_set) +
  xlab("balanced accuracy") +
  labs(title = "Model performance across 100 folds") +
  geom_vline(aes(xintercept = mean_ba), results_best_model %>%
  bind_rows(results_best_active) %>%
  rename(feature_set = rec) %>% 
  mutate(feature_set = if_else(feature_set == "rec_best", "Passive (random forest)", "Active (glmnet)")) %>%
  group_by(feature_set) %>%
  summarise(mean_ba = mean(bal_accuracy)), color = "red3")
```

Figure 2. Histogram of model fits across 100 folds. (FIX: 14 folds still running so only 86 splits in active)


```{r fig.height = 3}
best_passive_fits <- results_best_model$bal_accuracy
best_active_fits <- results_best_active$bal_accuracy

rope_min <- -.01
rope_max <- .01
plot_min = -.20
plot_max = .20

results_ttest <- bayesian_correlated_t_test(best_active_fits,
                                            best_passive_fits,
                                            rope_min = rope_min,
                                            rope_max = rope_max,
                                            k = 10,
                                            plot_min = plot_min,
                                            plot_max = plot_max)

ggplot(mapping = aes(x = results_ttest$plot_diffs,
                                                 y = results_ttest$pdf)) +
  geom_line() +
  geom_vline(mapping = aes(xintercept  = rope_min), linetype = "dashed") +
  geom_vline(mapping = aes(xintercept  = rope_max), linetype = "dashed") +
  scale_x_continuous(breaks=seq(plot_min, plot_max, .02)) +
  labs(x = "Accuracy Difference (Active - Passive Model)",
       y = "Posterior Probability")
```

Figure 3. Model comparison of the best passive (random forest) and best active (glmnet) models.


## Top Features

We conducted a permutation test of feature importance to see which features contributed most to our top performing model. Figure 4 shows which features when removed from the model had the greatest effect on performance. 


```{r fig.height = 5, fig.width = 8}
vi_best <- read_rds(here(path_models, str_c("vi_", best_model$algorithm, ".rds")))
algorithm <- if_else(best_model$algorithm == "random_forest", "random forest",
                     best_model$algorithm)
feat_set <- if_else(best_model$feature_set == "feat_all", "active", "passive")
vi_best %>% 
      mutate(Variable = str_remove(Variable, ".passive"),
             Variable = str_remove(Variable, ".l0"),
             Variable = str_remove(Variable, ".org"),
             Variable = str_remove(Variable, ".dttm_obs"),
             Variable = str_replace(Variable, "pratecount", "perc_rate"),
             Variable = str_replace(Variable, "rratecount", "raw_rate"),
             Variable = str_replace(Variable, "pratesum_duration", "perc_sum.duration"),
             Variable = str_replace(Variable, "pmean_duration", "perc_mean.duration"),
             Variable = str_replace(Variable, "rmean_duration", "raw_mean.duration"),
             Variable = str_replace(Variable, "ppropdatetime", "perc_prop"),
             Variable = str_replace(Variable, "rpropdatetime", "raw_prop"),
             Variable = str_replace(Variable, "rpropcount", "raw_prop"),
             Variable = str_replace(Variable, "ppropcount", "perc_prop"),
             Variable = str_replace(Variable, "p6", "6hrs"),
             Variable = str_replace(Variable, "p12", "12hrs"),
             Variable = str_replace(Variable, "p24", "24hrs"),
             Variable = str_replace(Variable, "p48", "48hrs"),
             Variable = str_replace(Variable, "p72", "72hrs"),
             Variable = str_replace(Variable, "p168", "168hrs"),
             Variable = fct_reorder(Variable, Importance)) %>% 
      filter(Importance != 0) %>% 
      ggplot(aes(x = Importance, y = Variable)) +
      geom_col(color = "black", fill = "light grey") +
      scale_x_continuous(expand = c(0, 0)) +
      labs(y = NULL, title = str_c("Top features for best model (", feat_set, " ", algorithm, ")"))
```

Figure 4. Feature importance scores for best performing model (Passive Random Forest). 




# Appendix

```{r fig.fullwidth = TRUE, fig.height = 11}
# average over predictions
predictions <- preds_best_model %>% 
  group_by(subid, dttm_label, y) %>% 
  summarise(mean = mean(pred_yes), .groups = "drop")

# read in study start dates for x-axis
study_start <- vroom(here(path_study_start, "study_dates.csv"), col_types = vroom::cols()) %>% 
  mutate(study_start = with_tz(study_start, tzone = "America/Chicago"),
         study_end = with_tz(study_end, tzone = "America/Chicago") + days(1)) %>% 
  select(subid, study_start, study_end)

for(i in 1:nrow(study_start))  {
  subid_dates <- tibble(subid = study_start$subid[i],
                        hour = seq(study_start$study_start[i], 
                                   study_start$study_end[i], "hours"))
  subid_dates <- subid_dates %>% 
    mutate(study_hour = seq(1:nrow(subid_dates)))
  
  study_dates <- if (i == 1) {
    subid_dates
  } else {
    study_dates %>% 
      bind_rows(subid_dates)
  }
}

predictions <- predictions %>% 
  left_join(study_dates, by = c("subid", "dttm_label" = "hour"))

# plot
predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap_paginate(~ subid, ncol = 3, nrow = 10, page = 1) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3)
cat('\r\n\r\n')

predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap_paginate(~ subid, ncol = 3, nrow = 10, page = 2) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3) 
cat('\r\n\r\n')

predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap_paginate(~ subid, ncol = 3, nrow = 10, page = 3) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3) 
cat('\r\n\r\n')

predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap_paginate(~ subid, ncol = 3, nrow = 10, page = 4) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3) 
cat('\r\n\r\n')

predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = study_hour, y = mean, color = y)) +
  geom_point(size = .9) +
  facet_wrap_paginate(~ subid, ncol = 3, nrow = 11, page = 5) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1) +
  ylab("Predicted Probability of Lapse") +
  scale_x_continuous(breaks = seq(1, 91*24, 30*24), labels = c("Start", "FU 1", "FU 2", "End")) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "red3", size = .3)
```


Figure A1. Predicted probabilities of lapse for each participant. A grouped 10x10 resampling method was used to obtain these probabilities. Known lapses are in red. The red dashed line represents the threshold for classifying a probability as a lapse (i.e., everything above the line was predicted to be a lapse).  


# Discussion


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
