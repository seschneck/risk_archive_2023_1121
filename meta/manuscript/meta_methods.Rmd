# Method
This project performs the initial analyses on a subset of data collected from 2017 â€“ 2019 as part of a larger grant funded through the National Institute of Alcohol Abuse and Alcoholism (R01 AA024391). A full list of measures used in the parent project can be found at <https://osf.io/brs68/>.

## Research Transparency
<!-- This section is taken from burden paper - need to rewrite? -->
<!--JJC: Is this all true?  You did all of this?  If so, looks good to include here-->
We value the principles of research transparency that are essential to the robustness and reproducibility of science [@schonbrodtVoluntaryCommitmentResearch2015]. Consequently, we maximized transparency through several complementary methods. First, we reported how we determined our sample size, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]. Second, we completed a transparency checklist, which can be found in the supplement of this paper [@aczelConsensusbasedTransparencyChecklist2019]. Third, we made the data, analysis scripts and annotated results, self-report surveys, and other study materials associated with this study publicly available [<https://osf.io/brs68/>].

## Participants
We recruited a community sample of people in initial stages of recovery for Alcohol Use Disorder from the Madison area. Avenues of recruitment included referrals from clinics, self-help groups, Facebook, radio, and television. Those who were interested in participating in our three-month study were given a brief description (i.e., told that our research focused on learning how mobile health technology can be used to provide individual support to anyone recovering from alcohol addiction) a short phone screen to determine initial eligibility (i.e., At least 18 years old, ability to read and write in English, and an eligible smartphone with existing cellphone plan).

Two hundred sixteen participants passed the initial phone screen and came in for a more in-depth screening session. Of the 216 interested participants, 199 enrolled in the study (i.e., they consented and were eligible to participate). We excluded potential participants if they did not meet the criteria for moderate or severe Alcohol Use Disorder (as defined by the DSM-5), did not have a goal of long-term abstinence, had not abstained from alcohol for at least one week, already had over two months of abstinence, or had severe symptoms of psychosis or paranoia. Of the 199 participants enrolled in the study, 154 provided at least one month of communications logs and EMA surveys. This was an additional requirement for the current study because cellphone communications were collected at each one month follow-up. We dropped data from three participants due to concerns quality of their data. Our final analyses were on a sample of 151 participants. <!--JJC: Any benefit to using your flow chart from BURDEN paper here? Or is that not well tuned to the meta project.   Not worth adding work but if its easy to adapt for this, might be nice-->

## Procedure
Our study involved five in-person visits (screening, enrollment, and three follow-up visits). It also required completion of daily EMA surveys to document alcohol lapses, and access to non-deleted text message and call logs (i.e., cellular communication logs). All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

During the screening session we obtained informed consent, determined eligibility, and documented basic demographic information. Participants that consented and were deemed eligible came back for a second enrollment visit. During enrollment, participants were briefed on how to delete log entries they did not want to share with us and completed a practice EMA survey. Participants also reported contacts they frequently communicated with and answered a series of questions documenting contextual information about their interactions with each contact (type of relationship with contact, whether they drank with contact in past, drinking status of contact, whether contact would drink in their presence, whether the contact is in recovery, the level of supportiveness the contact provides, and the pleasantness of their interactions with the contact). Participants returned for three follow-up visits, each one month apart. 

At each follow-up visit, we downloaded participants' cellular communication logs. These logs included the phone number of the other party, whether the call or message was incoming or outgoing, the duration of the call, and the date and time of the call or message. Participants also provided context information for newly identified frequent contacts (i.e., at least two communications in the past month). At the third follow-up visit, participants were debriefed and thanked for their participation. 

## Feature Engineering
<!--have a section that describes how you did feature engineering.  This is separate from the data analysis plan or at least a sub section of that plan.  It can be short but talk about how you calculated raw and  percent change. How you used different windows of data preceding the lapse, and how you combined this with context.  You might also describe some of the pre-processing that you did to the phone numbers if you want to.  Not needed but will give them a window into how much data processing is needed for a project like this-->

## Data Analysis Plan

We conducted all analyses in R version 4.0.3 [@rcoreteamLanguageEnvironmentStatistical2021] using RStudio [@rstudioteamRStudioIntegratedDevelopment2020] and the tidyverse and tidymodels ecosystem of packages [@wickhamWelcomeTidyverse2019; @kuhnTidymodelsCollectionPackages2020]. 

<!--Describe the stats you will provide to characterize the sample.  Demographics, etc?-->

Our first study aim was to train and evaluate the best performing machine learning model to predict alcohol lapse onset from contextualized cellular communication data. We built, trained, and evaluated models with several statistical learning algorithms including penalized parametric linear classification algorithms (LASSO, ridge regression, glmnet), non-parametric classification algorithms (k nearest neighbor), and ensemble methods (random forest). Candidate statistical learning algorithms were trained on a subset of the data (i.e., k-fold cross validation) using combinations of features derived from participants cellular communications and social context information<!--JJC: decide how to refer to the context and then use the same term throughout.  Here you use "social" context but you didnt use "social" earlier-->. 

Balanced accuracy was our performance metric for both model selection and evaluation.  However, we also report other performance metrics (sensitivity, specificity, positive predictive value, negative predictive value) to fully characterize model performance of our best performing model(s).  We also visually inspected lapse probability predictions by participant to further probe model performance.

We used grouped 10-fold resampling without repeats (i.e, 1 set of 10 grouped folds) to estimate balanced accuracy to select the top performing model (statistical algorithm and combination of features).  All folds were grouped by participant ID so that a single participant's data were not being used to predict future data by the same participant. In other words, all data from a participant was either included in the training or held-out validation set, but not both.  <!-- Maybe consider calling sets either training or validation since we didnt have a "test" set?-->

We estimated model performance for our top performing model using 10 repeats of grouped 10-fold resampling (i.e., grouped 10x10-fold). We opted to use resampling for model evaluation instead of a held-out test set because, with our limited sample size, averaging balanced accuracy over 100 held-out validation sets gives us a more stable (i.e., low variance) and less biased estimate of model performance than other available options using a single, explicitly held-out, test set. 

Finally, we used a Bayesian correlated t-test to compare our top model's balanced accuracy across the 100 validation sets to the expected performance of a null model with no predictive signal(i.e., balanced accuracy = .50). In this analysis, we defined a meaningful difference to be more than a difference of 1% in either direction (i.e., Region of Practical Equivalence [ROPE] set to .49 - .51).

We used various feature engineering methods to build groups of feature sets to maximize model performance. A key distinction between features was to discriminate between features derived from passive only measures (i.e., communication logs) and those derived from more active measures (i.e., context information). Our study's second aim was to compare models that use all available features (both passive signals from communications logs and actively measured context) with models restricted to only passive signals. To do this we employed a model comparison approach. Our top passive and active models were evaluated using grouped 10x10-fold resampling. We then used a Bayesian correlated t-test to compare balanced accuracy across the 100 validation sets from our top model trained on all data with our top model trained on only passive features. We defined a meaningful difference between models to be more than a 1% difference in either direction (i.e., ROPE set to -.01 - .01). Through this relative comparison, we quantified any performance benefit from adding the active component of context. 

Our third aim of the present study was to evaluate the importance of features within the top performing models. The most predictive features were identified based on an algorithm agnostic feature importance index. This approach involves permuting each individual feature to determine how much predictive signal it adds to the model. Since, this only tells us whether a feature is important and not the direction of its predictive value, we also examined coefficients from our linear glmnet model for a more interpretable explanation. <!--JJC: drop this last sentence if we dont report this - and I tend to think we wont report it-->









