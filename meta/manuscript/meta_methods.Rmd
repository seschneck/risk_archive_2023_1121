# Method
This project performs the initial analyses on a subset of data collected from 2017 – 2019 as part of a larger grant funded through the National Institute of Alcohol Abuse and Alcoholism (R01 AA024391). A full list of measures used in the parent project can be found at <https://osf.io/brs68/>.

## Research Transparency
<!-- This section is taken from burden paper - need to rewrite? -->
We value the principles of research transparency that are essential to the robustness and reproducibility of science [@schonbrodtVoluntaryCommitmentResearch2015]. Consequently, we maximized transparency through several complementary methods. First, we reported how we determined our sample size, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]. Second, we completed a transparency checklist, which can be found in the supplement of this paper [@aczelConsensusbasedTransparencyChecklist2019]. Third, we made the data, analysis scripts and annotated results, self-report surveys, and other study materials associated with this report publicly available [<https://osf.io/brs68/>].

## Participants
We recruited a community sample of people in initial stages of recovery for Alcohol Use Disorder from the Madison area. Avenues of recruitment included referrals from clinics, self-help groups, Facebook, radio, and television. Those who were interested in participating in our three-month study were given a brief description (i.e., told that our research focused on learning how mobile health technology can be used to provide individual support to anyone recovering from alcohol addiction) a short phone screen to determine initial eligibility (i.e., At least 18 years old, ability to read and write in English, and an eligible smartphone with existing cellphone plan).

Two hundred sixteen participants passed the initial phone screen and came in for a more in-depth screening session. Of the 216 interested participants, 199 enrolled in the study (i.e., they consented and were eligible to participate). We excluded potential participants if they did not meet the criteria for moderate or severe Alcohol Use Disorder (as defined by the DSM-5), did not have a goal of long-term abstinence, had not abstained from alcohol for at least one week, already had over two months of abstinence, or had severe symptoms of psychosis or paranoia. Of the 199 participants enrolled in the study, 154 provided at least one month of data and make up the sample used in our analyses. 

## Procedure
Our study involved five in-person visits (screening, enrollment, and three follow-up visits). It is also required completion of daily EMA surveys to document alcohol lapses, and access to non-deleted text message and call logs (i.e., cellular communication logs). All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

During the screening session we obtained informed consent, determined eligibility, and documented basic demographic information. Participants that consented and were deemed eligible came back for a second enrollment visit. During enrollment, participants were briefed on how to delete log entries they did not want to share with us and completed a practice EMA survey. Participants also reported contacts they frequently communicated with and answered a series of questions documenting contextual information about their interactions with each contact (type of relationship with contact, whether they drank with contact in past, drinking status of contact, whether contact would drink in their presence, whether the contact is in recovery, the level of supportiveness the contact provides, and the pleasantness of their interactions with the contact). Participants returned for three follow-up visits, each one month apart. 

At each follow-up visit, we downloaded participants' cellular communication logs. These logs included the phone number of the other party, whether the call or message was incoming or outgoing, the duration of the call, and the date and time of the call or message. Participants also provided context information for newly identified frequent contacts (i.e., at least two communications in the past month). At the third follow-up visit, participants were debriefed and thanked for their participation. 

## Data Analysis Plan
We conducted all analyses in R version 4.0.3 [@rcoreteamLanguageEnvironmentStatistical2021] using RStudio [@rstudioteamRStudioIntegratedDevelopment2020] and the tidyverse ecosystem of packages [@wickhamWelcomeTidyverse2019]. 

<!-- Aim 1: Train and evaluate the best performing machine learning model to predict alcohol lapse from cellular communication data. I will build, train, and evaluate models with several statistical learning algorithms including penalized parametric linear classification algorithms (LASSO, ridge regression, glmnet), non-linear classification algorithms (neural networks), non-parametric classification algorithms (k nearest neighbor), and ensemble methods (random forest). These statistical algorithms will be combined with various combinations of features derived from participants cellular communications and the context for these communications. Bootstrap resampling will be used to select the top performing model. Expected model performance for new participants (i.e., participants not used to train models) will be evaluated on an independent held-out test sample using the model’s area under the receiver operating characteristic curve (AUC; i.e., plot of sensitivity vs. specificity across classification thresholds). -->

<!-- Aim 2: Employ a model comparison approach to compare models that use all available features (both passive signals from communications logs and actively measured context) with models that are restricted to only passive signals. I will identify the top performing model using all available features and the top performing model without context variables and perform a model comparison. Through this relative comparison, I will quantify any performance benefit from adding the active component of context. This will be useful for future cost-benefit analyses that weigh the incremental benefit in performance relative to burden. -->

<!-- Aim 3: Evaluate the importance of feature sets within the top performing model to inform potential in situ treatments. The most predictive features will be identified based on feature important indices. I will also combine model comparison and feature ablation methods to remove subsets of context features in order to test their predictive utility. These evaluations will provide insight into the protective and harmful aspects of social interaction on lapse risk. Such insights may guide future intervention efforts. -->


<!-- Machine learning methods are appropriate for addressing these current gaps in the literature. Specifically, they can be used for both prediction and explanation.51 -->

<!-- Personal sensing measures can produce hundreds to thousands of observations for a single person. Machine learning methods are well suited for such large datasets in that they require larger sample sizes to be able to test the models on different data than what it was trained with. -->

<!-- Additionally, a key tenant of precision medicine is its focus on risk factors specific to the individual instead of generalizing across an entire population. To capture these unique differences from one individual to another, hundreds of predictors are required. To avoid issues of overfitting (i.e., fitting the model to noise in the sample and preventing it from generalizing to new data) due to high numbers of predictors, statistical learning algorithms such as LASSO and ridge regression can be used to penalize the model coefficients (i.e., through regularization). Thus, further emphasizing the necessity of machine learning in precision medicine research. -->

<!-- Overall, the heavy reliance of precision medicine on distal, static factors has proven fruitful for determining who is predisposed to AUD and vulnerable overall for relapse. Unfortunately, this does not say anything about when someone will lapse. To implement interventions “just-in-time” and prior to a full relapse, proximal factors must be considered. With the advancements of personal sensing measures and machine learning methods in the field of addiction, precision medicine approaches can now capitalize on dynamic predictors of lapse risk. -->

<!-- Candidate algorithms for our lapse risk prediction model include penalized parametric linear (LASSO, ridge, glmnet) and non-linear (neural networks) classification algorithms, non-parametric classification algorithms (k nearest neighbor), and ensemble methods (random forest). -->

<!-- Feature engineering is the process of transforming raw data to create a representation of the data that increases a model’s predictive power.52 These transformations can include imputation of missing data, removing irrelevant predictors, handling issues of multicollinearity, normalizing a skewed distribution, accounting for interactions between predictors, and creating interpretable predictors based on prior knowledge.53 -->

<!-- One example of feature engineering can be seen with handling the time-stamped communication logs. Entering the raw communication log dates and times into the model may yield some predictive power, but when transformed to dummy-coded features, such as weekends, happy hour, holidays, one can clearly see a more direct relationship to lapse risk. These features can be aggregated to represent daily, weekly, or monthly totals and averages and then analyzed to recognize changes over time (e.g., more activity during happy hour this week). -->

<!-- My first-year project will utilize feature engineering methods through interactions of static and time-varying social variables (i.e., average duration of calls as a function of sex) and through counts, sums, and proportions of social variables at various temporal windows (e.g., number of weekly interactions with supportive contacts, total contacts in recovery, ratio of incoming to outgoing messages). -->

<!-- Candidate statistical learning algorithms will be trained on a subset of the data (training sample) using combinations of features derived from participants cellular communications and social context information. Bootstrap resampling will be used to select the top performing model. Expected model performance for new participants (i.e., participants not used to train models) will be evaluated on an independent held-out test sample using the model’s area under the receiver operating characteristic curve (AUC; i.e., plot of sensitivity vs. specificity across classification thresholds). -->

<!-- Additionally, we will conduct a model comparison between our top performing model using all available features, and the top performing model restricted to only the passive communication logs. The incremental benefit in performance will be inferentially evaluated by comparing AUC values. -->
