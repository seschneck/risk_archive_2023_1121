# Method
This project performs the initial analyses on a subset of data collected from 2017 â€“ 2019 as part of a larger grant funded through the National Institute of Alcohol Abuse and Alcoholism (R01 AA024391). A full list of measures used in the parent project can be found at <https://osf.io/brs68/>.

## Research Transparency
<!-- This section is taken from burden paper - need to rewrite? -->
We value the principles of research transparency that are essential to the robustness and reproducibility of science [@schonbrodtVoluntaryCommitmentResearch2015]. Consequently, we maximized transparency through several complementary methods. First, we reported how we determined our sample size, all data exclusions, all manipulations, and all available measures in the study [@simmons21WordSolution2012]. Second, we completed a transparency checklist, which can be found in the supplement of this paper [@aczelConsensusbasedTransparencyChecklist2019]. Third, we made the data, analysis scripts and annotated results, self-report surveys, and other study materials associated with this study publicly available [<https://osf.io/brs68/>].

## Participants
We recruited a community sample of people in initial stages of recovery for Alcohol Use Disorder from the Madison area. Avenues of recruitment included referrals from clinics, self-help groups, Facebook, radio, and television. Those who were interested in participating in our three-month study were given a brief description (i.e., told that our research focused on learning how mobile health technology can be used to provide individual support to anyone recovering from alcohol addiction) a short phone screen to determine initial eligibility (i.e., At least 18 years old, ability to read and write in English, and an eligible smartphone with existing cellphone plan).

Two hundred sixteen participants passed the initial phone screen and came in for a more in-depth screening session. Of the 216 interested participants, 199 enrolled in the study (i.e., they consented and were eligible to participate). We excluded potential participants if they did not meet the criteria for moderate or severe Alcohol Use Disorder (as defined by the DSM-5), did not have a goal of long-term abstinence, had not abstained from alcohol for at least one week, already had over two months of abstinence, or had severe symptoms of psychosis or paranoia. Of the 199 participants enrolled in the study, 154 provided at least one month of data and make up the sample used in our analyses. 

## Procedure
Our study involved five in-person visits (screening, enrollment, and three follow-up visits). It is also required completion of daily EMA surveys to document alcohol lapses, and access to non-deleted text message and call logs (i.e., cellular communication logs). All procedures were approved by the University of Wisconsin-Madison Institutional Review Board.

During the screening session we obtained informed consent, determined eligibility, and documented basic demographic information. Participants that consented and were deemed eligible came back for a second enrollment visit. During enrollment, participants were briefed on how to delete log entries they did not want to share with us and completed a practice EMA survey. Participants also reported contacts they frequently communicated with and answered a series of questions documenting contextual information about their interactions with each contact (type of relationship with contact, whether they drank with contact in past, drinking status of contact, whether contact would drink in their presence, whether the contact is in recovery, the level of supportiveness the contact provides, and the pleasantness of their interactions with the contact). Participants returned for three follow-up visits, each one month apart. 

At each follow-up visit, we downloaded participants' cellular communication logs. These logs included the phone number of the other party, whether the call or message was incoming or outgoing, the duration of the call, and the date and time of the call or message. Participants also provided context information for newly identified frequent contacts (i.e., at least two communications in the past month). At the third follow-up visit, participants were debriefed and thanked for their participation. 

## Data Analysis Plan
We conducted all analyses in R version 4.0.3 [@rcoreteamLanguageEnvironmentStatistical2021] using RStudio [@rstudioteamRStudioIntegratedDevelopment2020] and the tidyverse and tidymodels ecosystem of packages [@wickhamWelcomeTidyverse2019; @kuhnTidymodelsCollectionPackages2020]. 

Our first study aim was to train and evaluate the best performing machine learning model to predict alcohol lapse from contextualized cellular communication data. We built, trained, and evaluated models with several statistical learning algorithms including penalized parametric linear classification algorithms (LASSO, ridge regression, glmnet), non-parametric classification algorithms (k nearest neighbor), and ensemble methods (random forest). Candidate statistical learning algorithms were trained on a subset of the data (training sample) using combinations of features derived from participants cellular communications and social context information.

We used grouped 10x10-fold resampling to select the top performing model (statistical algorithm and combination of features). All folds were grouped by participant ID so that a single participant's data were not being used to predict future data by the same participant. We evaluated expected model performance by our top performing model on new data (i.e., participants not used to train models) with an independent held-out test sample. Our performance metrics for model selection and evaluation were balanced accuracy and the area under the receiver operating characteristic curve (AUC; i.e., measure of sensitivity vs. specificity across classification thresholds). 

We used various feature engineering methods to build groups of feature sets to maximize model performance. The first distinction between features was to discriminate between features derived from passive only measures (i.e., communication logs) and those derived from more active measures (i.e., context information). Our study's second aim was to compare models that use all available features (both passive signals from communications logs and actively measured context) with models restricted to only passive signals. To do this we employed a model comparison approach. Through this relative comparison, we quantified any performance benefit from adding the active component of context. The incremental benefit in performance was inferentially evaluated by comparing balanced accuracy and AUC values.

Additionally, we used domain knowledge to engineer features from the raw data using averages, proportions, sums, interactions, and more. For example, dates and times from the communication logs were dummy-coded to represent features such as weekends, happy hour, and business hours. We also engineered our raw data so that we could recognize patterns and changes over time (e.g., more activity during happy hour this week). 

Finally, we used various period durations from which to engineer our features (e.g., two weeks of data vs. one month of data) and different lapse onset times for prediction (e.g., predicting lapse one hour out vs. three days out).

Our third aim of the present study was to evaluate the importance of feature sets within the top performing model. The most predictive features were identified based on feature important indices. We also combined model comparison and feature ablation methods to remove subsets of context features in order to test their predictive utility. 










