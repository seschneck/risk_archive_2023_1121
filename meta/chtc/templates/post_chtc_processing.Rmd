---
title: "Post-CHTC Processing"
author: "Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
    code_folding: show
editor_options: 
  chunk_output_type: console
---

### Setup

Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

Absolute paths
```{r, paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = { path_lab_support <- "P:/toolboxes/lab_support" },
        # IOS paths
        Darwin = { path_lab_support <- "/Volumes/toolboxes/lab_support" }
        )
```

Relative paths
```{r}
path_out <- "out"
path_results <- "results"
path_err <- "err"
path_jobs <- "../input/jobs.csv"
```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) 
 conflict_prefer("filter", "dplyr")
 conflict_prefer("select", "dplyr")

library(here)  
```

Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
library(tidyverse)  
library(janitor) 
library(lubridate)
library(ggplot2)
library(kableExtra)

theme_set(theme_classic()) 
```

Source for script
```{r, source_script, message=FALSE, warning=FALSE}
source(here(path_lab_support, "fun_modeling.R"))
```

### Notes
This script aggregates all CHTC results, runs checks on jobs and hyperparameters, 
and summarizes model performance across all folds.   

Inputs:  

Returned CHTC files   

- meta_output.zip  
- meta_results.zip    
- meta_output.zip   

Jobs input file   

- jobs.csv   

Output:   

- results_aggregate.csv   

### Unzip and check jobs

unzip and rename chtc folders
```{r}
unzip(zipfile = "meta_output.zip")
invisible(file.rename("meta_output", "out"))
unzip(zipfile = "meta_results.zip")
invisible(file.rename("meta_results", "results"))
unzip(zipfile = "meta_error.zip")
invisible(file.rename("meta_error", "err"))
```


check all error files are blank (0 kb)
```{r}
err_files <- map_df(list.files(path_err, full.names = TRUE), file.info)
tabyl(err_files$size)
```

check all output files are blank (0 kb)
```{r}
out_files <- map_df(list.files(path_out, full.names = TRUE), file.info)
tabyl(out_files$size)
```



### Aggregate all result CSVs

read in all CSVs
```{r}
result_files <- list.files(path_results, full.names = TRUE)
results <- vroom::vroom(result_files, col_types = vroom::cols()) %>% 
  glimpse()
```


check for missing jobs
```{r}
jobs <- vroom::vroom(path_jobs, col_types = vroom::cols())

missing_job_nums <- enframe(seq(1:nrow(jobs)), name = NULL, value = "job_num") %>% 
  filter(!job_num %in% results$job_num)
```

`r nrow(results)` results from `r nrow(jobs)` jobs.      

```{r}
if (nrow(missing_job_nums) > 0) {
  print(str_c("missing jobs: ", missing_job_nums$job_num))
  } else print(str_c("No missing jobs"))
```



### Average metrics across folds 
100 models per configuration with a 10 x 10-fold design    
Note: In testing using only 1 x 10-fold (10 models per configuration)    

```{r}
results_aggregate <- results %>% 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample) %>% 
  summarize(across(c(accuracy, bal_accuracy, sens, spec, roc_auc),
                   mean),
            n_jobs = n(), .groups = "drop") 
```


`r nrow(results_aggregate)` total configurations.    
`r nrow(jobs)/nrow(results_aggregate)` models per configuration.   

```{r}
results_aggregate %>% 
  print()
```


### Plot hyperparameters

FIX: plotting alpha hyperparameters but not sure how to plot penalty hyperparameters. 
Plot for best model only? How to get fits?
```{r}
ggplot(mapping = aes(x = results_aggregate$hp1, y = results_aggregate$bal_accuracy)) +
  geom_line() +
  xlab("mixture (alpha)") +
  ylab("balanced accuracy")
```


### Pull best model performance

Highest balanced accuracy is `r round(max(results_aggregate$bal_accuracy), 2)`
```{r}
results_aggregate %>% 
  slice_max(bal_accuracy) %>% 
  glimpse()
```

highest ROC is `r round(max(results_aggregate$roc_auc), 2)`
```{r}
results_aggregate %>% 
  slice_max(roc_auc) %>% 
  glimpse()
```

Fits for model with highest balanced accuracy
```{r}
(best_model_bal_accuracy <- results %>% 
  select(-job_num) %>%
  inner_join(results_aggregate %>% 
               slice_max(bal_accuracy) %>% 
               select(-c(accuracy:n_jobs)),
             by = c("algorithm", "feature_set", "hp1", "hp2", "hp3", "resample")) %>% 
  arrange(n_fold, n_repeat))
```

Fits for model with highest ROC AUC (if different configuration)
```{r}
if (slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results %>% 
    select(-job_num) %>% 
    inner_join(results_aggregate %>% 
                 slice_max(roc_auc) %>% 
                 select(-c(accuracy:n_jobs)),
               by = c("algorithm", "feature_set", "hp1", "hp2", "hp3", "resample")) %>% 
    arrange(n_fold, n_repeat)
}
```


### Plot/visualize model performance 

FIX: How to plot observed vs predicted outcomes with just performance metrics? 
Refit best model?    

Histogram of aggregate metrics across configurations
```{r collapse = TRUE}
hist(results_aggregate$roc_auc)
hist(results_aggregate$bal_accuracy)
hist(results_aggregate$accuracy, breaks = 20)
hist(results_aggregate$sens, breaks = 20)
hist(results_aggregate$spec, breaks = 20)
```


### Write out aggregated model configurations

Save configuration summaries
```{r}
write_csv(results_aggregate, "results_aggregate.csv") %>% 
  glimpse()
```

