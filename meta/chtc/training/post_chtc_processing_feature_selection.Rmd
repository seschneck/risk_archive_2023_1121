---
title: "Post-CHTC Processing"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
    code_folding: show
editor_options: 
  chunk_output_type: console
---

### Setup

Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

Source training controls 
```{r}
source('./meta/chtc/training/training_controls_feature_selection.R')
```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) 
 conflict_prefer("filter", "dplyr")
 conflict_prefer("select", "dplyr")
 conflict_prefer("spec", "yardstick")

library(here)  
```

Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
library(tidyverse)  
library(janitor) 
library(lubridate)
library(ggplot2)
library(kableExtra)
library(vip)

theme_set(theme_classic()) 
```

Set additional paths
```{r}
path_job <- here(path_jobs, name_job)
path_output <- here(path_job, "output/output")
path_results <-  here(path_job, "output/results")
path_error <-  here(path_job, "output/error")
```

Source function scripts from lab support
```{r  source_script, message=FALSE, warning=FALSE}
source(here("../lab_support/chtc/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))
```

### Notes
This script aggregates all CHTC results for the job `r name_job`. It runs checks for 
missing jobs, plots hyperparameters, and summarizes model performance across all folds.    
  

Inputs:  

Returned CHTC files   

- output.zip  
- results.zip    
- output.zip   

Jobs input file   

- jobs.csv   

Output:   

- results_aggregate.csv   

### Unzip and check jobs

unzip chtc folders
```{r}
# unzip(zipfile = file.path(path_job, "output/output.zip"), exdir = file.path(path_job, "output"))
# unzip(zipfile = file.path(path_job, "output/results.zip"), exdir = file.path(path_job, "output"))
# unzip(zipfile = file.path(path_job, "output/error.zip"), exdir = file.path(path_job, "output"))
```

Read in jobs
```{r}
jobs <- vroom::vroom(here(path_job, "input/jobs.csv"), col_types = vroom::cols()) %>% 
    rownames_to_column("job_num") 
```



check all error files are blank (0 kb)    
Note: error files also contain warnings
```{r}
err_files <- map_df(list.files(path_error, full.names = TRUE), file.info)
tabyl(err_files$size)
```

Pull error messages and jobs if error files are not all blank
```{r}
if (nrow(subset(err_files, size > 0)) > 0) {
  err_paths <- err_files %>%
    filter(size > 0) %>%
    rownames_to_column("path") %>%
    pull(path)
  for (i in err_paths) {
    err_i <- read_file(i) %>%
    enframe(value = "message", name = NULL) %>%
    mutate(job_num = as.numeric(str_remove(str_split(str_split(i, "/")[[1]][11], "_")[[1]][2], ".err")),
    message = str_remove_all(message, "\\n")) %>%
    relocate(job_num)
    errs <- if (i == err_paths[1]) {
      err_i
    } else rbind(errs, err_i)
  }

  # print error messages
  print_kbl(errs, align = "l")
}
```


Job rows with error messages
```{r}
if (nrow(subset(err_files, size > 0)) > 0) {
  # print jobs
  jobs %>%
    mutate(job_num = as.numeric(job_num)) %>% 
    inner_join(errs, by = "job_num") %>%
    print_kbl(align = "l")
} else print("no errors or warnings")
```


check all output files are blank (0 kb)
```{r}
out_files <- map_df(list.files(path_output, full.names = TRUE), file.info)
tabyl(out_files$size)
```

### Aggregate all result CSVs

read in all result CSVs
```{r}
result_files <- list.files(path_results, full.names = TRUE)

# for (i in result_files) {
#   read_csv(i, col_types = readr::cols()) %>%
#     mutate(job_num = as.numeric(str_remove(str_remove(str_split(i, "/")[[1]][11], "results_"), ".csv"))) %>%
#     relocate(job_num) %>%
#     write_csv(., here(i))
# }

results <- vroom::vroom(result_files, col_types = vroom::cols()) %>% 
  glimpse()
```


check for missing jobs
```{r}
missing_job_nums <- enframe(seq(1:nrow(jobs)), name = NULL, value = "job_num") %>% 
  filter(!job_num %in% results$job_num)
```

`r nrow(results)` results from `r nrow(jobs)` jobs.      

```{r}
if (nrow(missing_job_nums) > 0) {
  print(str_c("missing ", nrow(missing_job_nums), " of ", nrow(jobs),
              " jobs: ", str_c(as.character(missing_job_nums$job_num), collapse=", ")))
  } else print("No missing jobs")
```

```{r}
jobs %>% 
  filter(job_num %in% missing_job_nums$job_num) %>% 
  select(-c(n_repeat, n_fold, hp2, hp3)) %>% 
  print(n = Inf)
```


### Average metrics across folds 

NOTE: glmnet algorithms will have 1 returned model for each penalty/lambda combination. 
This has already been averaged across folds on the whole dataset in `tune_model()`.   

Other models (Knn and random forest) may have 10 - 100 models per unique configuration.   

```{r}
results_aggregate <- results %>% 
  group_by(algorithm, feature_set, feature_fun_type, hp1, hp2, hp3, resample, n_feats) %>% 
  summarize(across(c(accuracy, bal_accuracy, sens, spec, roc_auc),
                   mean),
            n_jobs = n(), .groups = "drop") 
```


`r nrow(results_aggregate)` unique model configurations.    
   

```{r}
# results_aggregate %>% 
#   print_kbl(digits = 4)
```

### Pull Best performing model by feature set

Summary of feature sets
```{r}
results_aggregate %>% 
  count(feature_set, feature_fun_type, n_feats) %>% 
  select(-n)
```

Check discrepancies  

Discrepancies due to step_nz
```{r}
# d <- read_rds(here(path_job, "input/data_trn.rds")) %>% 
#   rename(y = {{y_col_name}})
# 
# diff_1959 <- results %>% 
#   filter(n_feats == 1959) %>% 
#   slice(1)
# 
# diff_1959
# 
# rec <- build_recipe(d = d, job = diff_1959)
# 
# feat_1959 <-  rec %>% 
#   # remove id variables from count
#   step_rm(has_role(match = "id variable")) %>% 
#   prep(training = d, strings_as_factors = FALSE) %>% 
#   bake(new_data = NULL) %>% 
#   glimpse()
# 
# diff_2125 <- results %>% 
#   filter(n_feats == 2125) %>% 
#   slice(1)
# 
# diff_2125
# 
# rec <- build_recipe(d = d, job = diff_2125)
# 
# feat_2125 <-  rec %>% 
#   # remove id variables from count
#   step_rm(has_role(match = "id variable")) %>% 
#   prep(training = d, strings_as_factors = FALSE) %>% 
#   bake(new_data = NULL) %>% 
#   glimpse()
# 
# subset(names(feat_2125), !names(feat_2125) %in% names(feat_1959))
```



Get metrics
```{r}
feature_sets <- unique(results_aggregate$feature_set) 

for (i in feature_sets) {
  
  results_i <- results_aggregate %>% 
    filter(feature_set == i)
  
  for (j in feature_fun_type) {
    
    results_i_j <- results_i %>% 
      filter(feature_fun_type == j)
    
    print(str_c("Best performing model for ", i, "_", j))
  
    if (slice_max(results_i, bal_accuracy)$algorithm == "glmnet") {
    results_i_j %>% 
      slice_max(bal_accuracy) %>% 
      mutate(hp2 = log(hp2)) %>% 
      glimpse()
  } else {
    results_i_j %>% 
      slice_max(bal_accuracy) %>% 
      glimpse()
}
  }
  
}
```


### Plot hyperparameters

```{r}
algorithms <- unique(results_aggregate$algorithm)
feature_fun_types <- unique(results_i$feature_fun_type)
for (k in algorithms) {

  results_k <- results_aggregate %>%
      filter(algorithm == k)

  for (i in feature_sets) {

    results_i <- results_k %>%
      filter(feature_set == i)
 
  for (j in feature_fun_types) {
    
    results_j <- results_i %>%
      filter(feature_fun_type == j)

     # glmnet
    if (nrow(subset(results_j, algorithm == "glmnet")) != 0){

      plot_title <- str_c("Plotting glmnet hyperparameters for ", i, j, " feature set")


      plot_j <- results_j %>%
        filter(algorithm == "glmnet") %>%
        mutate(hp1 = factor(hp1, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>%
        separate(resample, c("resample", "under_ratio"), "_") %>%
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>%
        ggplot(mapping = aes(x = log(hp2),
                         y = bal_accuracy,
                         group = hp1,
                         color = hp1)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "mixture (alpha)") +
          labs(title = plot_title, x = "penalty (lambda)", y = "balanced accuracy")

      print(plot_j)
    }


    # random forest
    if (nrow(subset(results_j, algorithm == "random_forest")) != 0) {

      plot_title <- str_c("Plotting RF hyperparameters for ", i, j, " feature set")

      plot_j <- results_j %>%
        filter(algorithm == "random_forest") %>%
        mutate(hp2 = factor(hp2, ordered = TRUE),
              resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>%
        separate(resample, c("resample", "under_ratio"), "_") %>%
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>%
        ggplot(mapping = aes(x = hp1,
                         y = bal_accuracy,
                         group = hp2,
                         color = hp2)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "min n") +
          labs(title = plot_title, x = "mtry", y = "balanced accuracy")

       print(plot_j)
    }

    # knn
    if (nrow(subset(results_j, algorithm == "knn")) != 0) {

      plot_title <- str_c("Plotting knn hyperparameters for ", i, j, " feature set")

      plot_j <- results_j %>%
        filter(algorithm == "knn") %>%
        mutate(resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>%
        separate(resample, c("resample", "under_ratio"), "_") %>%
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>%
        ggplot(mapping = aes(x = hp1,
                         y = bal_accuracy)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          labs(title = plot_title, x = "neighbors", y = "balanced accuracy")

        print(plot_j)
    }
  }
  }
}
```


### Overall best model performance

Highest balanced accuracy is `r round(max(results_aggregate$bal_accuracy), 2)`
```{r}
if (slice_max(results_aggregate, bal_accuracy)$algorithm == "glmnet") {
  results_aggregate %>% 
    slice_max(bal_accuracy) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_aggregate %>% 
    slice_max(bal_accuracy) %>% 
    glimpse()
  }
```

highest ROC AUC is `r round(max(results_aggregate$roc_auc), 2)`
```{r}
# pull best AUC model if different than best balanced accuracy model
if (slice_max(results_aggregate, bal_accuracy)$algorithm == "glmnet" & slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results_aggregate %>% 
    slice_max(roc_auc) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else if (slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results_aggregate %>% 
    slice_max(roc_auc) %>% 
    glimpse()
  }
```



### View top predictors

```{r}
best_model <- slice_max(results_aggregate, bal_accuracy)
```


Train best model on all data
```{r}
d <- read_rds(here(path_job, "input/data_trn.rds")) %>% 
  rename(y = {{y_col_name}})

rec <- build_recipe(d, best_model)
rec <- rec %>% 
  step_rm(has_role(match = "id variable"))

feat_all <-  rec %>% 
    prep(training = d, strings_as_factors = FALSE) %>% 
    bake(new_data = d)

if (best_model$algorithm == "glmnet") {
  model <- logistic_reg(penalty = best_model$hp2,
                        mixture = best_model$hp1) %>% 
    set_engine("glmnet") %>%
    set_mode("classification") %>%
    fit(y ~ .,
          data = feat_all)
} else if (best_model$algorithm == "random_forest") {
  model <- rand_forest(mtry = best_model$hp1,
                       min_n = best_model$hp2,
                       trees = best_model$hp3) %>%
      set_engine("ranger",
                 importance = "impurity",
                 respect.unordered.factors = "order",
                 oob.error = FALSE,
                 seed = 102030) %>%
      set_mode("classification") %>%
      fit(y ~ .,
          data = feat_all)
} else if (best_model$algorithm == "knn") {
  model <- nearest_neighbor(neighbors = best_model$hp1) %>% 
      set_engine("kknn") %>% 
      set_mode("classification") %>% 
      fit(y ~ .,
          data = feat_all)
}
```

View top features    
*FIX: Cannot get variable importance scores for Knn - discuss with JC about how to handle*
```{r}
if (best_model$algorithm == "random_forest") {
  vip(model)
} else if (best_model$algorithm == "glmnet") {
  model %>% 
    tidy() %>% 
    filter(estimate != 0) %>% 
    arrange(desc(abs(estimate))) %>% 
    print(n = Inf)
    
  vip::vi(model) %>% 
    mutate(Variable = fct_reorder(Variable, Importance)) %>%
    head(n = 20) %>% 
    ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
    geom_col() +
    scale_x_continuous(expand = c(0, 0)) +
    labs(y = NULL)
}
```


Look at passive diff_perc model
```{r}
# temporary change algorithm of best model for testing
best_model <- results_aggregate %>%
  filter(feature_set == "feat_all_passive" & feature_fun_type == "diff_perc") %>%
  arrange(desc(bal_accuracy)) %>%
  slice(1)

best_model
```

Fit new best model on all data
```{r}
rec <- build_recipe(d, best_model)
rec <- rec %>% 
  step_rm(has_role(match = "id variable"))

feat_all <-  rec %>% 
    prep(training = d, strings_as_factors = FALSE) %>% 
    bake(new_data = d)

if (best_model$algorithm == "glmnet") {
  model <- logistic_reg(penalty = best_model$hp2,
                        mixture = best_model$hp1) %>% 
    set_engine("glmnet") %>%
    set_mode("classification") %>%
    fit(y ~ .,
          data = feat_all)
} else if (best_model$algorithm == "random_forest") {
  model <- rand_forest(mtry = best_model$hp1,
                       min_n = best_model$hp2,
                       trees = best_model$hp3) %>%
      set_engine("ranger",
                 importance = "impurity",
                 respect.unordered.factors = "order",
                 oob.error = FALSE,
                 seed = 102030) %>%
      set_mode("classification") %>%
      fit(y ~ .,
          data = feat_all)
} else if (best_model$algorithm == "knn") {
  model <- nearest_neighbor(neighbors = best_model$hp1) %>% 
      set_engine("kknn") %>% 
      set_mode("classification") %>% 
      fit(y ~ .,
          data = feat_all)
}

if (best_model$algorithm == "random_forest") {
  vip(model)
} else if (best_model$algorithm == "glmnet") {
  model %>% 
    tidy() %>% 
    # filter(estimate != 0) %>% 
    arrange(desc(abs(estimate))) %>% 
    print(n = Inf)
    
  vip::vi(model) %>% 
    mutate(Variable = fct_reorder(Variable, Importance)) %>%
    head(n = 20) %>% 
    ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
    geom_col() +
    scale_x_continuous(expand = c(0, 0)) +
    labs(y = NULL)
}
```


Look at best passive model
```{r}
# temporary change algorithm of best model for testing
best_model <- results_aggregate %>%
  filter(feature_set == "feat_all_passive") %>%
  arrange(desc(bal_accuracy)) %>%
  slice(1)

best_model
```

Fit new best model on all data
```{r}
rec <- build_recipe(d, best_model)
rec <- rec %>% 
  step_rm(has_role(match = "id variable"))

feat_all <-  rec %>% 
    prep(training = d, strings_as_factors = FALSE) %>% 
    bake(new_data = d)

if (best_model$algorithm == "glmnet") {
  model <- logistic_reg(penalty = best_model$hp2,
                        mixture = best_model$hp1) %>% 
    set_engine("glmnet") %>%
    set_mode("classification") %>%
    fit(y ~ .,
          data = feat_all)
} else if (best_model$algorithm == "random_forest") {
  model <- rand_forest(mtry = best_model$hp1,
                       min_n = best_model$hp2,
                       trees = best_model$hp3) %>%
      set_engine("ranger",
                 importance = "impurity",
                 respect.unordered.factors = "order",
                 oob.error = FALSE,
                 seed = 102030) %>%
      set_mode("classification") %>%
      fit(y ~ .,
          data = feat_all)
} else if (best_model$algorithm == "knn") {
  model <- nearest_neighbor(neighbors = best_model$hp1) %>% 
      set_engine("kknn") %>% 
      set_mode("classification") %>% 
      fit(y ~ .,
          data = feat_all)
}

if (best_model$algorithm == "random_forest") {
  vip(model)
} else if (best_model$algorithm == "glmnet") {
  model %>% 
    tidy() %>% 
    # filter(estimate != 0) %>%
    arrange(desc(abs(estimate))) %>% 
    print(n = Inf)
    
  vip::vi(model) %>% 
    mutate(Variable = fct_reorder(Variable, Importance)) %>%
    head(n = 20) %>% 
    ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
    geom_col() +
    scale_x_continuous(expand = c(0, 0)) +
    labs(y = NULL)
}
```


