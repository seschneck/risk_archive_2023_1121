---
title: "Create meta lapse labels"
author: "John Curtin and Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk/knits/meta", 
      "/Volumes/private/studydata/risk/knits/meta")
    )
  })
---

### Code Status

These labels are now complete to be used for the meta project. This script makes labels based on the cleaned lapses. See conclusions for important decisions and notes related to the pre-processing of lapses done in shared/scripts_mak/mak_lapses.   



### Conclusions   

- More complete EDA on all EMA data and lapses is in cln_ema.Rmd and mak_lapses.Rmd.  


- We decided to retain lapses with no end time if their onset was valid. We will use a
24 hour rule when sampling non-lapses. That is we will not sample non-lapses in the 
24 hours following the onset of the lapse.  

- with all lapses we will not sample no_lapses +- 24 hours from lapse   

- invalid lapses were also not sampled from for no_lapses within 24 hours of lapse  

- Our exclusion criteria can be summarized as:

  - lapses that were reported in an interview   
  - lapses that have no start time and date     
  - lapses that have a negative duration   
  - lapses that have a duration longer than 24 hours     
  - future lapses (lapse start > ema end time)   


- John and Kendra decided to change Subid 74's start date to 2018-07-06 - the day after 
their first cellular communication (2018-07-05).    
  - Participant broke their phone before followup 1 so we were not able to download 
  cellular communications at followup 1 (only 2 SMS/voice log files in Raw data). 
  Unclear how this impacted other data sources yet. 
  - As a result their cellular communications don't start until day 33 on study.           
  - Subid 74 had no lapses while on study.  
  

- John and Kendra have decided to drop subid 104's data for the following reasons:   
  - They had lapses every day on study except one day.    
  - Only had 75 surveys where a lapse was not reported.   
  - Viewing their lapse times it appears they were drinking constantly (morning and 
  night).   
  - They consistently report being uncertain that their goal is to be abstinent 
  (uncertain on 125 of 137 lapses. They also report they are uncertain in this goal 
  at followup 1 and 2.    
  - They are ultimately discontinued since they were struggling to gain sobriety.   
  - Unfortunately this drops 109 valid lapses.    


- John and Kendra have decided to drop subid 269's data for the following reasons:       
  - They completed 10-15 surveys on many days on study (avg number of surveys per 
  day is 6.76).  
  - Their responses indicate careless responding - they were filling 2-4 surveys out 
  within an hour of each other and the responses to the questions were completely different.     
  - They have questionable no lapse labels - they reported no lapses while on study but 
  according to notes left two messages for study staff where they admitted to drinking 
  over the weekend.   
  

- John and Kendra have decided to drop subid 204's data for the following reasons:    
  - Subid 204 had extremely poor compliance. 33 out of 89 study days had an EMA completed. 
  They only did a total of 5 surveys between followup 2 and 3.    
  - We don't trust their lapse labels - They report several lapses during their interviews 
  but times appear questionable (same time every night). They only report 1 lapse with EMA.
  - From notes - "Participant did not do many surveys during their second month of participation. 
  At their Follow-up 2 visit they reported several lapses that were not documented in their 
  EMAs - estimated lapse days/times in subid's raw data log."  
  - JC note: "There are issues with 204. They are missing lapses reported by interview. But they  
  also stopped doing any ema by 5/17 even though their study end date was 6/13. Probably need to 
  drop them for lapse analyses for anything after 5/17.  Probably also need to add in their 
  reported lapses at follow-up 2. OR we drop them at the end of follow-up 1 or wherever their 
  ema gets sketchy"    


- John and Kendra have decided to decided to retain 128's data even though they have over 100 lapses for 
the following reasons:   
  - Compliance is good (averaged over 3 surveys per day, no days without an EMA).       
  - completed the study for the full 90 days.    
  - appeared to still want abstinence as they reported they were uncertain to ema_1_5 
  on only 3 surveys. They reported they were uncertain that their goal was to remain 
  abstinent at followup 1 and confirmed their goal was to remain abstinent at followup 2.    
  - Has more non-lapse surveys than lapse surveys.   
  

- Subid 118 has an invalid end time (None12pm) left in cleaned EMA as study-level decision. 
After Kendra and John reviewed this situation, it was concluded that it was likely they were 
still drinking at time of EMA (i.e., no end time). Changing end time to NA in this script.    
  
  
- Subid 238 has 419 total EMAs for 89 days on study. Some days they have as many as 10 
surveys a day. Not clear why. They report no lapses while on study. They also have no other notes or 
evidence to suggest their data is unreliable.    

- Interview lapses are used to exclude days from non-lapse sampling but should not be 
used as valid lapses. Times are approximations made retrospectively.     

- There were no EMA surveys sent to any participants on 3/2/19.   

- 23 EMAs with lapses took more than 10 minutes to complete (ranges from 11 - 582 minutes 
for finished surveys).     

- All unfinished surveys (finished = 0) have at least ema_1 answered which reports if 
there have been any lapses since the last survey.  
  - Note that there are several "self-correcting surveys" where a participant answers 
  "Yes" for ema_1 then restarts another survey within minutes of the incomplete survey 
  and puts "no" for ema_1.  
  

- All final timezones are in America/Chicago timezone.    


### Set Up Environment

Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

Absolute paths
```{r, absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_raw <- "P:/studydata/risk/data_raw"
          path_processed <- "P:/studydata/risk/data_processed/shared"
          path_meta <- "P:/studydata/risk/data_processed/meta"},

        # IOS paths
        Darwin = {
          path_raw <- "/Volumes/private/studydata/risk/data_raw"
          path_processed <- "/Volumes/private/studydata/risk/data_processed/shared"
          path_meta <- "/Volumes/private/studydata/risk/data_processed/meta"}
        )
```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) # detect and warn about function conflicts
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")

library(here)  # establish project directory consistently as working directory
```


Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
# for data wrangling
library(tidyverse)
library(vroom)
library(janitor)
library(lubridate)
library(tictoc)
```


Source for script
```{r, source_script, message=FALSE, warning=FALSE}
source(here("../lab_support/print_kbl.R"))
source(here("shared/fun_local.R"))
```

### Read in lapses
```{r}
lapses <- vroom::vroom(here(path_processed, "lapses.csv"), col_types = vroom::cols()) %>% 
  mutate(across(c(lapse_start, lapse_end, ema_end)), with_tz(., "America/Chicago")) %>% 
  glimpse()
```


### Study Dates/Times

We will not predict lapses outside of study participation dates. Need info on:

* Start and end dates for participation
* Who completed followup_1 so that they have context
* Time of the last EMA so we can know the definite window for no_lapse sampling.
We can assume no_lapse sampling starts with study_start but it end with the earlier
of study_end or ema_end (time of the last ema that we trust has valid data)     

Read in dates from study level folder
```{r}
dates <- vroom::vroom(here(path_meta, "study_dates.csv"), col_types = vroom::cols()) %>%
  mutate(across(study_start:ema_end), with_tz(., tz = "America/Chicago")) %>% 
  glimpse()
```


`r length(unique(subset(lapses, !exclude)$subid))` out of `r length(unique(dates$subid))` subids (`r round(length(unique(subset(lapses, !exclude)$subid))/length(unique(dates$subid)), 2)`) have lapses. 
```{r}
lapses %>%
  filter(!exclude) %>%
  tabyl(subid) %>% 
  arrange(desc(n))
```



### Get valid observations of lapse and no_lapse

Get valid lapse and no_lapse observations by specified window duration (in seconds)   

Make labels with 1 hour window duration
```{r cache = TRUE}  
tic()
labels_1_hour <- get_lapse_labels(lapses, dates, buffer_start = 0, window_dur = 3600) %>% 
  glimpse()
toc()
```

Make labels with 1 day window duration
```{r cache = TRUE}   
tic()
labels_1_day <- get_lapse_labels(lapses, dates, buffer_start = 0, window_dur = 86400) %>% 
  glimpse()
toc()
```

Make labels with 1 week window duration
```{r cache = TRUE}
tic()
labels_1_week <- get_lapse_labels(lapses, dates, buffer_start = 0, window_dur = 604800) %>% 
  glimpse()
toc()
```




### Check counts of lapse and no lapse labels

1 hour labels should match original labels created (labels_all.csv)

```{r}
labels_all <- vroom::vroom(here(path_meta, "labels_all.csv"), col_types = vroom::cols()) %>% 
  mutate(dttm_label = with_tz(dttm_label, tzone = "America/Chicago")) %>% 
  glimpse()
```

Check new labels dataframe is same as labels_all    
```{r}
labels_1_hour %>% 
  glimpse()
```

Check if same number of rows
```{r}
if (nrow(labels_1_hour) == nrow(labels_all)) {
  print(str_c("labels_all and labels_1 hour both have ", nrow(labels_1_hour), " rows"))
} else if (nrow(labels_1_hour) < nrow(labels_all)) {
  print(str_c("labels_all has ", nrow(labels_all) - nrow(labels_1_hour), 
              " more rows than labels_1_hour"))
} else if (nrow(labels_1_hour) > nrow(labels_all)) { 
  print(str_c("labels_1_hour has ", nrow(labels_all) - nrow(labels_1_hour), 
              " more rows than labels_all"))
} 
```

Check if any rows in `labels_1_hour` without a match in `labels_all`  
```{r}
(missing_rows <- anti_join(labels_1_hour, labels_all, by = c("subid", "dttm_label", "lapse", "no_lapse")))

print(str_c(nrow(missing_rows), " missing matches in labels_all"))
```

Discrepancies in no_lapse label    
All fall on daylight savings for 2018 (March 11) 
```{r}
missing_rows %>% 
  left_join(labels_all, by = c("subid", "dttm_label")) %>% 
  rename(lapse_1_hour = lapse.x,
         no_lapse_1_hour = no_lapse.x,
         lapse_all = lapse.y,
         no_lapse_all = no_lapse.y)

missing_rows$dttm_label
```

No lapse should be FALSE on 2018-03-11 01:00:00   
```{r}
labels_1_hour %>% 
  filter(subid == 39) %>% 
  head(n = 550)
```

Found reason for discrepancy - need to use duration() instead of seconds() in get_lapse_labels   
This has been corrected in the function - changing to FALSE until labels are rerun
```{r}
for (i in 1:nrow(missing_rows)) {
    mr_subid <- missing_rows$subid[[i]]
    mr_dttm_label <- missing_rows$dttm_label[[i]]
    
    row_index <- which(labels_1_hour$subid == mr_subid & 
                       labels_1_hour$dttm_label == mr_dttm_label)

    if (length(row_index) == 1) {
      labels_1_hour$no_lapse[row_index] <- FALSE
    }
}

# check all rows match now
anti_join(labels_1_hour, labels_all, by = c("subid", "dttm_label", "lapse", "no_lapse"))
```



# Time zone documentation

No labels on 2017-03-12 at 2:00 AM (where CST ends and CDT began in 2017)
```{r}
labels_1_hour %>% 
  pull(dttm_label)
```

Duplicate entries on 2019-11-03 at 1:00 AM (1 in CST and 1 in CDT - daylight savings ended in 2019)
```{r}
labels_1_hour %>% 
  tail(n = 1000) %>% 
  pull(dttm_label) 
```
  
This is an issue in labels_all too
```{r}
labels_all %>% 
  pull(dttm_label)

labels_all %>% 
  tail(n = 1000) %>% 
  pull(dttm_label) 
```



Check if any rows in `labels_all` without a match in `labels_1_hour`
```{r}
(missing_rows <- anti_join(labels_all, labels_1_hour, by = c("subid", "dttm_label", "lapse", "no_lapse")))

print(str_c(nrow(missing_rows), " missing matches in labels_1_hour"))
```

No discrepancies in labels
```{r}
missing_rows %>% 
  left_join(labels_all, by = c("subid", "dttm_label")) %>% 
  rename(lapse_all = lapse.x,
         no_lapse_all = no_lapse.x,
         lapse_1_hour = lapse.y,
         no_lapse_1_hour = no_lapse.y)
```

missing labels by subid
```{r}
labels_1_hour %>% 
  count(subid) %>% 
  left_join(labels_all %>% 
              count(subid),
            by = "subid") %>% 
  rename(n_1_hour = n.x,
         n_all = n.y) %>% 
  filter(n_1_hour != n_all)
```

17 missing labels are because labels_all included study_end date/time for some subids
```{r}
missing_rows %>% 
  left_join(labels_all, by = c("subid", "dttm_label")) %>% 
  select(subid, dttm_label) %>% 
  left_join(dates %>% 
              select(subid, study_end),
            by = "subid")
```



Check counts in `labels_1_hour`       

* A total of `r nrow(labels_1_hour)` labels made.    
* `r nrow(subset(labels_1_hour, lapse))` labels are lapses.   
* `r nrow(subset(labels_1_hour, no_lapse))` labels are labeled as no lapse.    
* `r sum(labels_1_hour$lapse & labels_1_hour$no_lapse)` are labeled lapse and no lapse (should be zero).   
* `r sum(!labels_1_hour$lapse & !labels_1_hour$no_lapse)` are not labeled lapse or no lapse (should be non-zero. Hard to know correct value).  
* `r nrow(subset(labels_1_hour, lapse))` + `r nrow(subset(labels_1_hour, no_lapse))` + `r sum(!labels_1_hour$lapse & !labels_1_hour$no_lapse)` = `r nrow(subset(labels_1_hour, lapse)) + nrow(subset(labels_1_hour, no_lapse)) +  sum(!labels_1_hour$lapse & !labels_1_hour$no_lapse)`    


Check counts in `labels_1_day`       

* A total of `r nrow(labels_1_day)` labels made.    
* `r nrow(subset(labels_1_day, lapse))` labels are lapses.   
* `r nrow(subset(labels_1_day, no_lapse))` labels are labeled as no lapse.    
* `r sum(labels_1_day$lapse & labels_1_day$no_lapse)` are labeled lapse and no lapse (should be zero).   
* `r sum(!labels_1_day$lapse & !labels_1_day$no_lapse)` are not labeled lapse or no lapse (should be non-zero. Hard to know correct value).  
* `r nrow(subset(labels_1_day, lapse))` + `r nrow(subset(labels_1_day, no_lapse))` + `r sum(!labels_1_day$lapse & !labels_1_day$no_lapse)` = `r nrow(subset(labels_1_day, lapse)) + nrow(subset(labels_1_day, no_lapse)) +  sum(!labels_1_day$lapse & !labels_1_day$no_lapse)`     


Check counts in `labels_1_week`       

* A total of `r nrow(labels_1_week)` labels made.    
* `r nrow(subset(labels_1_week, lapse))` labels are lapses.   
* `r nrow(subset(labels_1_week, no_lapse))` labels are labeled as no lapse.    
* `r sum(labels_1_week$lapse & labels_1_week$no_lapse)` are labeled lapse and no lapse (should be zero).   
* `r sum(!labels_1_week$lapse & !labels_1_week$no_lapse)` are not labeled lapse or no lapse (should be non-zero. Hard to know correct value).  
* `r nrow(subset(labels_1_week, lapse))` + `r nrow(subset(labels_1_week, no_lapse))` + `r sum(!labels_1_week$lapse & !labels_1_week$no_lapse)` = `r nrow(subset(labels_1_week, lapse)) + nrow(subset(labels_1_week, no_lapse)) +  sum(!labels_1_week$lapse & !labels_1_week$no_lapse)`     


### Get Final Labels 

Removes no_lapse labels that we do not want to use (e.g., they are 24 hours before or after a lapse)
```{r}
labels_1_hour <- sample_labels(labels_1_hour)

labels_1_day <- sample_labels(labels_1_day)

labels_1_week <- sample_labels(labels_1_week)
```



### EDA on Final Labels 

#### Proportion of lapses    

at the level of the hour
```{r}
labels_1_hour %>% 
  tabyl(label)
```

at the level of the day
```{r}
labels_1_day %>% 
  tabyl(label)
```

at the level of the week
```{r}
labels_1_week %>% 
  tabyl(label)
```


#### Labels per subid

`r length(unique(labels_1_hour$subid))` of `r length(unique(dates$subid))` subids have 1 hour labels.  

```{r}
labels_1_hour %>% 
  tabyl(subid, label) %>% 
  adorn_totals("col")
```


`r length(unique(labels_1_day$subid))` of `r length(unique(dates$subid))` subids have 1 day labels.  

```{r}
labels_1_day %>% 
  tabyl(subid, label) %>% 
  adorn_totals("col")
```

`r length(unique(labels_1_week$subid))` of `r length(unique(dates$subid))` subids have 1 week labels.  

```{r}
labels_1_week %>% 
  tabyl(subid, label) %>% 
  adorn_totals("col")
```


#### Check labels end one epoch before study_end date

recalculate study end time
```{r}
study_end_times <- dates %>% 
  rowwise() %>% 
  mutate(study_end = study_end + (hours(23)),
         ema_end = floor_date(ema_end, unit = "hours"),
         end_time = min(study_end, ema_end)) %>% 
  ungroup() %>% 
  select(subid, end_time)
```

calculate difference between study end date and last label dttm
```{r}
labels_1_hour %>% 
  left_join(study_end_times, by = "subid") %>% 
  group_by(subid) %>% 
  slice_tail(n = 1) %>% 
  mutate(end_label_diff = end_time - dttm_label) %>% 
  tabyl(end_label_diff)

labels_1_day %>% 
  left_join(study_end_times, by = "subid") %>% 
  group_by(subid) %>% 
  slice_tail(n = 1) %>% 
  mutate(end_label_diff = end_time - dttm_label) %>% 
  tabyl(end_label_diff)

labels_1_week %>% 
  left_join(study_end_times, by = "subid") %>% 
  group_by(subid) %>% 
  slice_tail(n = 1) %>% 
  mutate(end_label_diff = end_time - dttm_label) %>% 
  tabyl(end_label_diff)
```


#### Check all valid lapses retained

`r nrow(subset(labels_1_hour, label == "lapse"))` of `r nrow(subset(lapses, exclude == FALSE))` valid lapses retained in 1 hour labels.  

`r nrow(subset(labels_1_day, label == "lapse"))` of `r nrow(subset(lapses, exclude == FALSE))` valid lapses retained in 1 day labels.  

`r nrow(subset(labels_1_week, label == "lapse"))` of `r nrow(subset(lapses, exclude == FALSE))` valid lapses retained in 1 week labels.  
  
 
`r nrow(subset(left_join(lapses, dates, by = "subid"), !exclude & date(study_start) == date(lapse_start)))` valid lapses occurred on a participants' first day on study (i.e., day after intake).     
```{r}
lapses %>% 
  left_join(dates, by = "subid") %>% 
  filter(!exclude & date(study_start) == date(lapse_start)) %>% 
  print_kbl()
```



#### Check no excluded lapses were retained

1 hour window
```{r}
labels_1_hour %>% 
  filter(label == "lapse") %>% 
  left_join(lapses %>% select(subid, lapse_start, exclude), 
            by = c("subid", "dttm_label" = "lapse_start")) %>% 
  tabyl(exclude)
```

`r nrow(subset(left_join(labels_1_hour, lapses, by = c("subid", "dttm_label" = "lapse_start")), exclude & label == "lapse"))` excluded lapse is showing as included
```{r}
labels_1_hour %>% 
  left_join(lapses, by = c("subid", "dttm_label" = "lapse_start")) %>% 
  filter(exclude & label == "lapse") %>% 
  glimpse()
```


Excluded lapse included in labels was excluded for being a future lapse (times removed during pre-processing)
```{r}
ema <- vroom(here(path_processed, "ema_later.csv"), col_types = vroom::cols())%>% 
  rename_with(~ str_replace(.x, "emal_", "ema_")) %>% 
  mutate(start_date = with_tz(start_date, "America/Chicago"),
         end_date = with_tz(end_date, "America/Chicago")) %>% 
  select(subid, response_id, start_date, end_date, ema_1_1:ema_1_4)

ema %>% 
  filter(response_id == "R_eWLl6bIFFddp6cV") %>%
  left_join(lapses %>% select(response_id, lapse_start, exclude, lapse_cnt), 
            by = "response_id") %>% 
  mutate(diff = difftime(end_date, lapse_start, units = "hours")) %>% 
  glimpse()
```

The same lapse is later reported and included (no longer a future lapse) - 2 different response_ids    
Since this is a valid lapse it is okay that it is retained in our final labels
```{r}
lapses %>% 
  filter(lapse_start == subset(left_join(ema, lapses, by = "response_id"), 
                               response_id == "R_eWLl6bIFFddp6cV")$lapse_start)
```

EMA for valid lapse
```{r}
ema %>% 
  filter(response_id == "R_UGFcl9goLnpGrwl") %>% 
  left_join(lapses %>% select(response_id, lapse_start, exclude, lapse_cnt), 
            by = "response_id") %>% 
   mutate(diff = difftime(end_date, lapse_start, units = "hours")) %>% 
  glimpse()
```

1 day window
```{r}
labels_1_day %>% 
  filter(label == "lapse") %>% 
  left_join(lapses %>% select(subid, lapse_start, exclude), 
            by = c("subid", "dttm_label" = "lapse_start")) %>% 
  tabyl(exclude)
```

1 week window
```{r}
labels_1_week %>% 
  filter(label == "lapse") %>% 
  left_join(lapses %>% select(subid, lapse_start, exclude), 
            by = c("subid", "dttm_label" = "lapse_start")) %>% 
  tabyl(exclude)
```


#### Relationship between lapses and no lapses

`lapses` and `no_lapses` are negatively correlated
```{r}
labels_1_hour %>% 
  count(subid, label) %>% 
  pivot_wider(names_from = label, values_from = n) %>% 
  select(-subid) %>% 
  cor(use = "pairwise.complete.obs") %>% 
  round(2)

labels_1_day %>% 
  count(subid, label) %>% 
  pivot_wider(names_from = label, values_from = n) %>% 
  select(-subid) %>% 
  cor(use = "pairwise.complete.obs") %>% 
  round(2)

labels_1_week %>% 
  count(subid, label) %>% 
  pivot_wider(names_from = label, values_from = n) %>% 
  select(-subid) %>% 
  cor(use = "pairwise.complete.obs") %>% 
  round(2)
```



### Save

save valid_observations and labels
```{r}
labels_1_hour %>% 
  write_csv(here(path_meta, "labels_1_hour.csv"))

labels_1_day %>% 
  write_csv(here(path_meta, "labels_1_day.csv"))

labels_1_week %>% 
  write_csv(here(path_meta, "labels_1_week.csv"))
```


save as compressed rds files for CHTC
```{r}
labels_1_hour %>% 
  write_rds(here(path_meta, "labels_1_hour.rds"), compress = "xz")

labels_1_day %>% 
  write_rds(here(path_meta, "labels_1_day.rds"), compress = "xz")

labels_1_week %>% 
  write_rds(here(path_meta, "labels_1_week.rds"), compress = "xz")
```



