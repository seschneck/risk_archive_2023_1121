---
title: "Create EMA lapse labels"
author: "Gaylen Fronk, John Curtin, and Kendra Wyant"
date: "`r lubridate::today()`"
output: 
  html_document:
    toc: true 
    toc_depth: 4
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = dplyr::if_else(Sys.info()[["sysname"]] == "Windows",
      "P:/studydata/risk/knits/insight", 
      "/Volumes/private/studydata/risk/knits/insight")
    )
  })
---

### Code Status

These labels are now complete to be used for the EMA project. This script makes labels based on the cleaned lapses. See conclusions for important decisions and notes related to the pre-processing of lapses done in shared/scripts_mak/mak_lapses.  

GEF: Updating for Insight project. Primary update is to use EMA completion times as the anchor for lapse labels rather than "raw" time.


### Conclusions   

- More complete EDA on all EMA data and lapses is in cln_ema.Rmd and mak_lapses.Rmd.  


- We decided to retain lapses with no end time if their onset was valid. We will use a
24 hour rule when sampling non-lapses. That is we will not sample non-lapses in the 
24 hours following the onset of the lapse.  

- with all lapses we will not sample no_lapses +- 24 hours from lapse   

- invalid lapses were also not sampled from for no_lapses within 24 hours of lapse  

- Our exclusion criteria can be summarized as:

- lapses that were reported in an interview   
- lapses that have no start time and date     
- lapses that have a negative duration   
- lapses that have a duration longer than 24 hours     
- future lapses (lapse start > ema end time)   


- John and Kendra have decided to drop subid 104's data for the following reasons:   
- They had lapses every day on study except one day.    
- Only had 75 surveys where a lapse was not reported.   
- Viewing their lapse times it appears they were drinking constantly (morning and 
night).   
- They consistently report being uncertain that their goal is to be abstinent 
(uncertain on 125 of 137 lapses. They also report they are uncertain in this goal 
at followup 1 and 2.    
- They are ultimately discontinued since they were struggling to gain sobriety.   
- Unfortunately this drops 109 valid lapses.    


- John and Kendra have decided to drop subid 269's data for the following reasons:       
- They completed 10-15 surveys on many days on study (avg number of surveys per 
day is 6.76).  
- Their responses indicate careless responding - they were filling 2-4 surveys out 
within an hour of each other and the responses to the questions were completely different.     
- They have questionable no lapse labels - they reported no lapses while on study but 
according to notes left two messages for study staff where they admitted to drinking 
over the weekend.   


- John and Kendra have decided to drop subid 204's data for the following reasons:    
- Subid 204 had extremely poor compliance. 33 out of 89 study days had an EMA completed. 
They only did a total of 5 surveys between followup 2 and 3.    
- We don't trust their lapse labels - They report several lapses during their interviews 
but times appear questionable (same time every night). They only report 1 lapse with EMA.
- From notes - "Participant did not do many surveys during their second month of participation. 
At their Follow-up 2 visit they reported several lapses that were not documented in their 
EMAs - estimated lapse days/times in subid's raw data log."  
- JC note: "There are issues with 204. They are missing lapses reported by interview. But they  
also stopped doing any ema by 5/17 even though their study end date was 6/13. Probably need to 
drop them for lapse analyses for anything after 5/17.  Probably also need to add in their 
reported lapses at follow-up 2. OR we drop them at the end of follow-up 1 or wherever their 
ema gets sketchy"    


- John and Kendra have decided to decided to retain 128's data even though they have over 100 lapses for 
the following reasons:   
- Compliance is good (averaged over 3 surveys per day, no days without an EMA).       
- completed the study for the full 90 days.    
- appeared to still want abstinence as they reported they were uncertain to ema_1_5 
on only 3 surveys. They reported they were uncertain that their goal was to remain 
abstinent at followup 1 and confirmed their goal was to remain abstinent at followup 2.    
- Has more non-lapse surveys than lapse surveys.   


- Subid 118 has an invalid end time (None12pm) left in cleaned EMA as study-level decision. 
After Kendra and John reviewed this situation, it was concluded that it was likely they were 
still drinking at time of EMA (i.e., no end time). Changing end time to NA in this script.    


- Subid 238 has 419 total EMAs for 89 days on study. Some days they have as many as 10 
surveys a day. Not clear why. They report no lapses while on study. They also have no other notes or 
evidence to suggest their data is unreliable.    

- Interview lapses are used to exclude days from non-lapse sampling but should not be 
used as valid lapses. Times are approximations made retrospectively.     

- There were no EMA surveys sent to any participants on 3/2/19.   

- 23 EMAs with lapses took more than 10 minutes to complete (ranges from 11 - 582 minutes 
for finished surveys).     

- All unfinished surveys (finished = 0) have at least ema_1 answered which reports if 
there have been any lapses since the last survey.  
- Note that there are several "self-correcting surveys" where a participant answers 
"Yes" for ema_1 then restarts another survey within minutes of the incomplete survey 
and puts "no" for ema_1.  


- All final timezones are in America/Chicago timezone.    


### Set Up Environment

Chunk Defaults
```{r defaults, include=FALSE}
knitr::opts_chunk$set(attr.output='style="max-height: 500px;"')

options(tibble.width = Inf)
options(tibble.print_max = Inf)
```

Absolute paths
```{r, absolute paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_raw <- "P:/studydata/risk/data_raw"
          path_processed <- "P:/studydata/risk/data_processed/shared"
          path_insight <- "P:/studydata/risk/data_processed/insight"},
        
        # IOS paths
        Darwin = {
          path_raw <- "/Volumes/private/studydata/risk/data_raw"
          path_processed <- "/Volumes/private/studydata/risk/data_processed/shared"
          path_insight <- "/Volumes/private/studydata/risk/data_processed/insight"}
)
```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) # detect and warn about function conflicts
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")

library(here)  # establish project directory consistently as working directory
```


Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
# for data wrangling
library(tidyverse)
library(vroom)
library(janitor)
library(lubridate)
library(furrr)
```


Source for script
```{r, source_script, message=FALSE, warning=FALSE}
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/print_kbl.R?raw=true")
source(here("shared/fun_local.R"))
```

### Read in lapses
```{r}
lapses <- vroom(here(path_processed, "lapses.csv"), 
                col_types = vroom::cols()) %>% 
  mutate(across(c(lapse_start, lapse_end, ema_end)), 
         with_tz(., "America/Chicago")) %>% 
  glimpse()
```


### Study Dates/Times

We will not predict lapses outside of study participation dates. Need info on:

* Start and end dates for participation
* Who completed followup_1 so that they have context
* Time of the last EMA so we can know the definite window for no_lapse sampling.
We can assume no_lapse sampling starts with study_start but it end with the earlier
of study_end or ema_end (time of the last ema that we trust has valid data)     

Read in dates from study level folder
```{r}
dates <- vroom(here(path_insight, "study_dates.csv"), col_types = vroom::cols()) %>%
  mutate(across(study_start:ema_end), with_tz(., tz = "America/Chicago")) %>% 
  glimpse()
```


`r length(unique(subset(lapses, !exclude)$subid))` out of `r length(unique(dates$subid))` subids (`r round(length(unique(subset(lapses, !exclude)$subid))/length(unique(dates$subid)), 2)`) have lapses. 
```{r}
lapses %>%
  filter(!exclude) %>%
  tabyl(subid) %>% 
  arrange(desc(n))
```

GEF NOTES:
open up EMA, get all morning reports
iterate over end time of morning reports
for each end time, then create an end window (starts 1s after completion time, goes 7 days after that)
then look at that window, check if any of the 1029 valid lapse onsets fall into that window --> "LAPSE"
then check if any of the 47 invalid "lapses" overlap with the window --> if so, set to NA AMONG REMAINING WINDOWS ONLY
if not set to positive or to NA, then set to "NO LAPSE"
if(any("in window")) [vector of 1s after to week after] then ^yes
look at how the negative windows are currently handled

first set every one of week windows to "no lapse" (default)
then change away from no lapse to NA if includes lapse start or end of invalid lapses
then set any windows that contain lapse start or end of valid lapses to "lapse" (checking among full data, including windows marked as NA)

might not use/need get_lapse_labels() function bc problem here is simpler

setting code up (see DWT webbook):
change parallel processing to be new correct version
try to keep # of attached packages as small as possible (library) - can use just one or a handful of functions via include/exclude parameter in library
otherwise can use ::
chapter on conflicts - set up script so that it warns you if you have any conflicts right away when loading packages - not library(conflicted), that's an old and no longer preferred way to resolve conflicts
conflicts are also avoided by minimizing packages loaded!
maybe no longer need here package, base R seems to have caught up

### Read in morning EMA

Read in morning EMA data from shared processed data
```{r}
emam <- vroom(here(path_processed, "ema_morning.csv"), 
              col_types = vroom::cols()) %>% 
  mutate(across(c(start_date, end_date)), 
         with_tz(., tz = "America/Chicago")) %>% 
  glimpse()
```


### Get valid observations of lapse and no_lapse

Get valid lapse and no_lapse observations by specified window duration (in seconds)   

Set up parallel processing
```{r}
# use furrr to future map over subids (the_subid) when making labels
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)

subids <- unique(dates$subid)
```

Make window_start and window_end dttm columns
```{r}
labels <- emam %>% 
  select(subid, response_id, start_date, end_date) %>% 
  mutate(window_start = end_date + seconds(1)) %>% 
  mutate(window_end = window_start + seconds(604800)) %>% 
  mutate(label = "no_lapse") %>% 
  glimpse()

```

Make labeling function (based off get_lapse_labels() in fun_local.R but adjusted for insight project where we are windowing differently)
```{r}
label_insight_windows <- function(the_subid, lapses, dates, labels) {
  # Purpose: Returns a tibble of lapse labels that includes subid, dttm_label, and
  #   boolean lapse and no_lapse columns. 
  # Inputs:
  #   dates = tibble that should contain subid, study_start, study_end, ema_end 
  #     dttm vars should be in America/Chicago
  
  # Step 1: Handle inavlid lapses (set labels$label to NA)
  exclusions <- lapses %>% 
    filter(subid == the_subid) %>% 
    filter(exclude)
  
  if(nrow(exclusions) != 0) {
    
    inv_lapse_subid <- invalid_lapses$subid[[i]]
    
    for (i in 1:nrow(exclusions)) {
      
      if(is.na(exclusions$lapse_start[[i]])) {
        lapse_start <- as_datetime(exclusions$lapse_start_date[[i]],
                                   format = "%m-%d-%Y", tz = "America/Chicago")
        lapse_end <- as_datetime(exclusions$lapse_end_date[[i]],
                                 format = "%m-%d-%Y", tz = "America/Chicago")
        
        # Exclude full date
        lapse_hours_exclude <- seq(lapse_start, # beginning of day
                                   lapse_end + hours(24), # end of day
                                   by = "hours")
        
        for (lapse_hour_exclude in lapse_hours_exclude) {
          row_index <- which(labels$subid == inv_lapse_subid &
                               labels$window_start <= lapse_hour_exclude &
                               labels$window_end >= lapse_hour_exclude)
          
          labels$label[row_index] <- NA
        }
      } else {
        
        if(is.na(exclusions$lapse_end[[i]])) {
          # be cautious & set lapse duration as longest valid lapse duration (24h)
          # then create lapse_end (lapse_start + 24h)
          
          lapse_start <- exclusions$lapse_start[[i]]
          
          lapse_hours_exclude <- seq(lapse_start, # beginning of day
                                     lapse_start + hours(24), # end of day
                                     by = "hours")
          
          for (lapse_hour_exclude in lapse_hours_exclude) {
            row_index <- which(labels$subid == inv_lapse_subid &
                                 labels$window_start <= lapse_hour_exclude &
                                 labels$window_end >= lapse_hour_exclude)
            
            labels$label[row_index] <- NA
          }
          
        }
        
        # STOPPED HERE
        
        
        
        if(any(invalid_lapses$duration < 0)) {
          # switch start and end days/times, then exclude full period
        }
        
        
        
        inv_lapse_start <- invalid_lapses$lapse_start[[i]]
        
        row_index <- which(labels$subid == inv_lapse_subid &
                             labels$window_start <= inv_lapse_start &
                             labels$window_end >= inv_lapse_start)
      }
      
      
    }
    
    # Step 1: handle valid lapses
    valid_lapses <- lapses %>%
      filter(subid == the_subid) %>% 
      filter(!exclude)
    
    if(nrow(valid_lapses) != 0) {
      for (i in 1:nrow(valid_lapses)) {
        lapse_subid <- valid_lapses$subid[[i]]
        lapse_start <- valid_lapses$lapse_start[[i]]
        
        row_index <- which(labels$subid == lapse_subid & 
                             labels$dttm_label <= lapse_start &
                             lapse_start < labels$dttm_label + duration(window_dur, units = "seconds"))
        
        # row index will contain more than one for days
        # if (length(row_index) == 1) {
        labels$lapse[row_index] <- TRUE
        # }
        
        # some extra checks
        # this should not happen
        # if (length(row_index) > 1) {
        #   stop("get_lapse_labels() multiple match; SubID: ", lapse_subid, " Lapse hour: ", lapse_start, " i: ", i)
        # }
        
        # this can happen if we have a lapse report outside of the study_hours
        # if (length(row_index) == 0) {
        #   no_match <- no_match + 1
        # }
      }
    }
    
    # final check for lapses
    # if (! (nrow(valid_lapses) == (sum(labels$lapse) + no_match))) {
    #   stop("Not all lapses accounted")
    # }
    
    # Step 2: Handle no_lapse exclusions for valid lapse periods +- 24 hours.
    
    # First set an end time for valid lapses that have a missing end time (there are a couple)
    # We will be cautious and set longest valid lapse duration (24 hours)
    valid_lapses <- valid_lapses %>%
      mutate(lapse_end = if_else(is.na(lapse_end),
                                 lapse_start + hours(24),
                                 lapse_end))
    
    if(nrow(valid_lapses) != 0) {
      for (i in 1:nrow(valid_lapses)) {
        
        # exclude lapse +- 24 hours on each side
        lapse_hours_exclude <- seq(valid_lapses$lapse_start[[i]] - hours(24),
                                   valid_lapses$lapse_end[[i]] + hours(24),
                                   by = "hours")
        
        for (lapse_hour_exclude in lapse_hours_exclude) {
          row_index <- which(labels$subid == valid_lapses$subid[[i]] & 
                               labels$dttm_label <= lapse_hour_exclude &
                               lapse_hour_exclude < labels$dttm_label + duration(window_dur, units = "seconds")) 
          
          # Not valid check for day level - think about built in check
          # if (length(row_index) == 1) {
          labels$no_lapse[row_index] <- FALSE
          # }
        }
      }
    }
    
    # Step 3: Handle no_lapse exclusions for excluded periods with date but no times
    exclusions <- lapses %>%
      filter(subid == the_subid) %>% 
      filter(exclude & is.na(lapse_start) & is.na(lapse_end))
    
    if(nrow(exclusions) != 0) {
      for (i in 1:nrow(exclusions)) {
        lapse_start <- as_datetime(exclusions$lapse_start_date[[i]],
                                   format = "%m-%d-%Y", tz = "America/Chicago")
        lapse_end <- as_datetime(exclusions$lapse_end_date[[i]],
                                 format = "%m-%d-%Y", tz = "America/Chicago")
        
        # Exclude full date +- 24 hours
        lapse_hours_exclude <- seq(lapse_start - hours(24),
                                   lapse_end + hours(48), # end of day + 24 hours
                                   by = "hours")
        
        for (lapse_hour_exclude in lapse_hours_exclude) {
          row_index <- which(labels$subid == exclusions$subid[[i]] & 
                               labels$dttm_label <= lapse_hour_exclude &
                               lapse_hour_exclude < labels$dttm_label + duration(window_dur, units = "seconds")) 
          
          # if (length(row_index) == 1) {
          labels$no_lapse[row_index] <- FALSE
          # }
        }
      }
    }
    
    # Step 4: Handle no_lapse exclusions for very long duration lapses (> 24 hours)
    # Exclude full period +- 24 hours
    exclusions <- lapses %>%
      filter(subid == the_subid) %>% 
      filter(exclude & duration > 24)
    
    if(nrow(exclusions) != 0) {
      for (i in 1:nrow(exclusions)) {
        
        # exclude lapse +- 24 hours on each side
        lapse_hours_exclude <- seq(exclusions$lapse_start[[i]] - hours(24),
                                   exclusions$lapse_end[[i]] + hours(24),
                                   by = "hours")
        
        for (lapse_hour_exclude in lapse_hours_exclude) {
          row_index <- which(labels$subid == exclusions$subid[[i]] & 
                               labels$dttm_label <= lapse_hour_exclude &
                               lapse_hour_exclude < labels$dttm_label + duration(window_dur, units = "seconds")) 
          
          # if (length(row_index) == 1) {
          labels$no_lapse[row_index] <- FALSE
          # }
        }
      }
    }
    
    # Step 5: Handle no_lapse exclusions for negative duration lapses
    # Flip start and end time and then exclude full period +- 24 hours
    exclusions <- lapses %>%
      filter(subid == the_subid) %>% 
      filter(exclude & duration <= 24)
    
    if(nrow(exclusions) != 0) {
      for (i in 1:nrow(exclusions)) {
        
        # exclude lapse +- 24 hours on each side
        lapse_hours_exclude <- seq(exclusions$lapse_end[[i]] - hours(24),
                                   exclusions$lapse_start[[i]] + hours(24),
                                   by = "hours")
        
        for (lapse_hour_exclude in lapse_hours_exclude) {
          row_index <- which(labels$subid == exclusions$subid[[i]] & 
                               labels$dttm_label <= lapse_hour_exclude &
                               lapse_hour_exclude < labels$dttm_label + duration(window_dur, units = "seconds")) 
          
          # if (length(row_index) == 1) {
          labels$no_lapse[row_index] <- FALSE
          # }
        }
      }
    }
    
    return(labels)
    
  } 
  ```
  
  
  Add labels to 1 week windows
  ```{r}
  labels_1_week <- subids %>% 
    future_map_dfr(~ ,
                   .options = furrr_options(seed = NULL)) %>% 
    glimpse()
  ```
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  Make labels with 1 hour window duration
  start at midnight on second day
  ```{r}  
  labels_1_hour <- subids %>% 
    future_map_dfr(~ get_lapse_labels(.x, lapses, dates, 
                                      buffer_start = 60 * 60 * 24, window_dur = 3600),
                   .options = furrr_options(seed = NULL)) %>% 
    glimpse()
  ```
  
  Make labels with 1 day window duration
  ```{r}   
  labels_1_day <- subids %>% 
    future_map_dfr(~ get_lapse_labels(.x, lapses, dates, 
                                      buffer_start = 60 * 60 * 24, window_dur = 86400),
                   .options = furrr_options(seed = NULL)) %>% 
    glimpse()
  ```
  
  Make labels with 1 week window duration
  ```{r}
  labels_1_week <- subids %>% 
    future_map_dfr(~ get_lapse_labels(.x, lapses, dates, 
                                      buffer_start = 60 * 60 * 24, window_dur = 604800),
                   .options = furrr_options(seed = NULL)) %>% 
    glimpse()
  ```
  
  
  Return to sequential processing
  ```{r}
  plan(sequential)
  ```
  
  ### Check counts of lapse and no lapse labels
  
  
  Check counts in `labels_1_hour`       
  
  * A total of `r nrow(labels_1_hour)` labels made.    
  * `r nrow(subset(labels_1_hour, lapse))` labels are lapses.   
  * `r nrow(subset(labels_1_hour, no_lapse))` labels are labeled as no lapse.    
  * `r sum(labels_1_hour$lapse & labels_1_hour$no_lapse)` are labeled lapse and no lapse (should be zero).   
  * `r sum(!labels_1_hour$lapse & !labels_1_hour$no_lapse)` are not labeled lapse or no lapse (should be non-zero. Hard to know correct value).  
  * `r nrow(subset(labels_1_hour, lapse))` + `r nrow(subset(labels_1_hour, no_lapse))` + `r sum(!labels_1_hour$lapse & !labels_1_hour$no_lapse)` = `r nrow(subset(labels_1_hour, lapse)) + nrow(subset(labels_1_hour, no_lapse)) +  sum(!labels_1_hour$lapse & !labels_1_hour$no_lapse)`    
  
  
  Check counts in `labels_1_day`       
  
  * A total of `r nrow(labels_1_day)` labels made.    
  * `r nrow(subset(labels_1_day, lapse))` labels are lapses.   
  * `r nrow(subset(labels_1_day, no_lapse))` labels are labeled as no lapse.    
  * `r sum(labels_1_day$lapse & labels_1_day$no_lapse)` are labeled lapse and no lapse (should be zero).   
  * `r sum(!labels_1_day$lapse & !labels_1_day$no_lapse)` are not labeled lapse or no lapse (should be non-zero. Hard to know correct value).  
  * `r nrow(subset(labels_1_day, lapse))` + `r nrow(subset(labels_1_day, no_lapse))` + `r sum(!labels_1_day$lapse & !labels_1_day$no_lapse)` = `r nrow(subset(labels_1_day, lapse)) + nrow(subset(labels_1_day, no_lapse)) +  sum(!labels_1_day$lapse & !labels_1_day$no_lapse)`     
  
  
  Check counts in `labels_1_week`       
  
  * A total of `r nrow(labels_1_week)` labels made.    
  * `r nrow(subset(labels_1_week, lapse))` labels are lapses.   
  * `r nrow(subset(labels_1_week, no_lapse))` labels are labeled as no lapse.    
  * `r sum(labels_1_week$lapse & labels_1_week$no_lapse)` are labeled lapse and no lapse (should be zero).   
  * `r sum(!labels_1_week$lapse & !labels_1_week$no_lapse)` are not labeled lapse or no lapse (should be non-zero. Hard to know correct value).  
  * `r nrow(subset(labels_1_week, lapse))` + `r nrow(subset(labels_1_week, no_lapse))` + `r sum(!labels_1_week$lapse & !labels_1_week$no_lapse)` = `r nrow(subset(labels_1_week, lapse)) + nrow(subset(labels_1_week, no_lapse)) +  sum(!labels_1_week$lapse & !labels_1_week$no_lapse)`     
  
  
  ### Check labels end one epoch before study_end date
  
  recalculate study end time
  ```{r}
  study_end_times <- dates %>% 
    rowwise() %>% 
    mutate(study_end = study_end + (hours(23)),
           ema_end = floor_date(ema_end, unit = "hours"),
           end_time = min(study_end, ema_end)) %>% 
    ungroup() %>% 
    select(subid, end_time)
  ```
  
  calculate difference between study end date and last label dttm
  ```{r}
  labels_1_hour %>% 
    left_join(study_end_times, by = "subid") %>% 
    group_by(subid) %>% 
    slice_tail(n = 1) %>% 
    mutate(end_label_diff = end_time - dttm_label) %>% 
    tabyl(end_label_diff)
  
  labels_1_day %>% 
    left_join(study_end_times, by = "subid") %>% 
    group_by(subid) %>% 
    slice_tail(n = 1) %>% 
    mutate(end_label_diff = end_time - dttm_label) %>% 
    tabyl(end_label_diff)
  
  labels_1_week %>% 
    left_join(study_end_times, by = "subid") %>% 
    group_by(subid) %>% 
    slice_tail(n = 1) %>% 
    mutate(end_label_diff = end_time - dttm_label) %>% 
    tabyl(end_label_diff)
  ```
  
  
  
  ### Get Final Labels 
  
  Removes no_lapse labels that we do not want to use (e.g., they are 24 hours before or after a lapse)
  ```{r}
  labels_1_hour <- bind_labels(labels_1_hour) %>% 
    rename(lapse = label) %>% 
    mutate(lapse = if_else(lapse == "lapse", "yes", "no")) %>% 
    arrange(subid, dttm_label)
  
  labels_1_day <- bind_labels(labels_1_day) %>% 
    rename(lapse = label) %>% 
    mutate(lapse = if_else(lapse == "lapse", "yes", "no")) %>% 
    arrange(subid, dttm_label)
  
  labels_1_week <- bind_labels(labels_1_week) %>% 
    rename(lapse = label) %>% 
    mutate(lapse = if_else(lapse == "lapse", "yes", "no")) %>% 
    arrange(subid, dttm_label)
  ```
  
  
  
  ### EDA on Final Labels 
  
  #### Proportion of lapses    
  
  at the level of the hour
  ```{r}
  labels_1_hour %>% 
    tabyl(lapse)
  ```
  
  at the level of the day
  ```{r}
  labels_1_day %>% 
    tabyl(lapse)
  ```
  
  at the level of the week
  ```{r}
  labels_1_week %>% 
    tabyl(lapse)
  ```
  
  
  #### Labels per subid
  
  ```{r}
  labels_1_hour %>% 
    tabyl(subid, lapse) %>% 
    arrange(yes) %>% 
    adorn_totals("col")
  ```
  
  ```{r}
  labels_1_day %>% 
    tabyl(subid, lapse) %>% 
    arrange(yes) %>% 
    adorn_totals("col")
  ```
  
  ```{r}
  labels_1_week %>% 
    tabyl(subid, lapse) %>% 
    arrange(yes) %>% 
    adorn_totals("col")
  ```
  
  
  
  ### Save
  
  save valid_observations and labels, raw only because file is pretty small
  ```{r}
  labels_1_hour %>% 
    write_csv(here(path_ema, "labels_1hour.csv")) 
  
  labels_1_day %>% 
    write_csv(here(path_ema, "labels_1day.csv")) 
  
  labels_1_week %>% 
    write_csv(here(path_ema, "labels_1week.csv")) 
  ```
  