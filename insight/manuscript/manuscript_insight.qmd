---
title: "Untitled"
format: pdf
csl: https://raw.githubusercontent.com/jjcurtin/lab_support/main/rmd_templates/csl/elsevier-vancouver.csl
geometry: margin=.5in
fontsize: 11pt
bibliography: paper_insight.bib
editor_options: 
  chunk_output_type: console
---



<!--General notes
Additional YAML formatting will need to be added once it's decided where this will be submitted. Tag Susan to handle that.

-->

```{r knitr_settings, include = FALSE}
# settings
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = FALSE, 
                      message = FALSE)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(fig.pos = "ht", out.extra = "")
```

```{r setup, include = FALSE}
library(knitr)
# library(yardstick) # for roc_curve
library(kableExtra)
library(janitor)
# library(corx)
library(patchwork)
library(ggtext)
library(consort)
library(tidyverse)
library(tidymodels)
library(tidyposterior)
library(cowplot)

theme_set(theme_classic()) 
```


```{r paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- "P:/studydata/risk/chtc/insight"
          path_processed <- "P:/studydata/risk/data_processed/insight"
          path_models <- "P:/studydata/risk/models/insight"
          path_shared <- "P:/studydata/risk/data_processed/shared"},

        # IOS paths
        Darwin = {
          path_input <- "/Volumes/private/studydata/risk/chtc/insight"
          path_processed <- "/Volumes/private/studydata/risk/data_processed/insight"
          path_models <- "/Volumes/private/studydata/risk/models/insight"
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"},
        
        # Linux paths
        Linux = {
          path_input <- "~/mnt/private/studydata/risk/chtc/insight"
          path_processed <- "~/mnt/private/studydata/risk/data_processed/insight"
          path_models <- "~/mnt/private/studydata/risk/models/insight"
          path_shared <- "~/mnt/private/studydata/risk/data_processed/shared"}
        )
```

```{r load_data}
```

# Abstract 
<!-- 225 word maximum, excluding title-->

Title for Abstract (CPA Submission): <!-- 100 character maximum -->
Dynamic assessment of self-efficacy predicts future alcohol use <!-- 63 characters -->

<!-- background -->

<!-- 
In the United States, alcohol use disorder (AUD) affects nearly 30 million adults annually and 30% of adults over the course of their lifetime. Of those who receive treatment, 2/3 relapse within 6 months. Static measures of self-efficacy have proven to be predictive of next lapse risk, but are less understood in the context of continuous self-monitoring. 

GEF: I love the key concepts you are portraying here! What I'm taking away is: 1) AUD is prevalent and costly, 2) individuals rarely receive (effective) treatment, 3) self-efficacy/self-monitoring is/can be predictive of future lapse, and 4) this construct hasn't been assessed dynamically. I think all of these should be in the abstract, so great job! For space purposes, however, and because the audience here is a narrow addiction-focused group who already know about the problems with AUD tx, we can minimize content related to points 1 and 2. My edits focused on simplifying those messages (e.g., removing numbers/statistics) and helping to link ideas 1/2 to ideas 3/4 (i.e., why are self-efficacy and continuous monitoring relevant and important?). 
-->

Treatment for alcohol use disorder (AUD) often includes self-monitoring, whereby individuals reflect on self-efficacy (i.e., likelihood of future alcohol use) to monitor risk. Self-efficacy is typically assessed statically; however, self-efficacy likely changes as risk changes throughout recovery.

<!-- purpose -->

<!-- 
The current study measured individuals' insight into their own lapse risk via daily EMAs situated within a digital therapeutic. 

GEF: This sounds great. I made some small tweaks to bring in the self-efficacy language & to highlight our key questions
-->

We investigated whether: 1) dynamically measured self-efficacy predicts dynamic lapse risk; 2) dynamic self-efficacy predicts better than a single, static self-efficacy measurement; and 3) including additional risk-relevant features improves prediction.

<!-- methods -->

<!--
We developed three separate models that provide week-level probabilities of a future lapse back to alcohol use using a baseline alcohol abstinence self-efficacy (AASE)-only score, a once-daily measure of self-efficacy, and a 'full-feature' model including 4 times daily EMA responses. Model features were based on scores collected through ecological momentary assessment (EMA). 

GEF: I like how you adapted the EMA paper methods to fit our project. Some small edits below. I also moved the participant sentence here to provide that context earlier on. You're right that in the paper, reporting on demographics of the sample would be in the Results, but I think providing who our sample is and the general procedure of the study fits best earlier here where we have less space
-->

Participants (*N*=151; 51% male; *M*~age~=41; 87% White, 97% Non-Hispanic) in early recovery (1–8 weeks abstinent) from AUD provided 4x daily ecological momentary assessments (EMAs) within a digital therapeutic for 1-3 months. We developed three models to predict probability of alcohol use in the next week. Models differed by included features: 1) self-efficacy ("How likely are you to drink alcohol in the next week?") from 1X daily EMAs; 2) Alcohol Abstinence Self-Efficacy (AASE) score measured at intake; or 3) self-efficacy plus additional relapse-relevant EMA items (e.g., craving, stress). We used grouped, nested cross-validation to select models and evaluate performance. 

<!-- results -->

<!-- 
Participants (*N*=151; 51% male; mean age = 41; 87% White, 97% Non-Hispanic) in early recovery (1–8 weeks of abstinence) from alcohol use disorder provided 4x daily EMA for up to three months. [Include results here that show the all feature model predicted the best, with with an ___, the once-daily measure of self-efficacy performed well at____, and finally, the baseline AASE-only had some predictive value for dichotomous outcomes but was insufficient for prediction and clinical utility]. 

GEF: You included all the right information here. I changed the structure/presentation slightly to match up with our questions, and I added the values. 
-->

Dynamic self-efficacy predicted next-week lapse risk (area under the receiver operating curve [auROC]=0.80) more accurately than static self-efficacy (auROC=0.59). Adding EMA items further improved performance (auROC=0.88).

<!-- discussion/conclusion/impact -->

<!-- 
This suggests dynamic measures of self-efficacy are accurate in predicting future lapse risk over time and may be further improved via inclusion of other data collected via EMA when available.

GEF: Just some slight tweaks to add context around that impact!
-->
Individuals can accurately predict their future lapse risk when cued via daily EMAs. Adding EMA items improves performance and identifies factors driving risk, which could guide treatment recommendations within a digital therapeutic. 

# Introduction

## Background

AUD/SUDs are problematic and prevalent and require lifelong care/monitoring.

Self-monitoring is thought to be a valuable and necessary tool during recovery from AUD and other SUDs.It promises a cost-effective, accessible, and easy-to-use tool when integrated in digital therapeutics
-   Anecdotally - encouraged by clinicians, prompted by many self-help tools/apps, etc. Considered to be a productive component of CBT in the treatment of SUDs
-   Empirically? Findings are mixed and data is limited. In one review of 41 studies investigating self-monitoring on substance use outcomes, only 29% found a helpful effect (like reducing substance use) (Gass et al., 2021). 63% had no effect and 8% found it to be detrimental. This ambiguity may be at least partially explained by a lack of research. For instance, in another review of 191 web-based interventions designed to decrease alcohol consumption, only ten met quality criteria for inclusion and only 1 was an RCT (Bewick et al., 2008).No empirical evidence was provided that apps could serve as an intervention. 
-   Consider ARCP framing - there is a strongly held belief about monitoring perhaps without **sufficient evidence** and/or without **sufficient nuance** to how monitoring should be implemented

Self-monitoring, insight, and/or confidence in remaining abstinent/pursuing treatment goals has traditionally been measured statically.
-   Measures like the AASE are often used to assess an individual's "long-term" likelihood of remaining abstinent. 
-   Efficacy can be hard to capture consistently because it is context-specific. Since its development in 1994, the AASE, built on Bandura's construct of self-efficacy, has been used with frequency. The 20-item scale demonstrates high-internal consistency as well as strong indices of reliability and validity (DiClemente et al., 1994). The shortened 12-item scale shares in these attributes (McKiernan et al., 2011)
-   Scores from measures like the AASe may be used at the end of treatment/end of study/etc. to estimate an individual's future likelihood of staying abstinent
-   It is **unclear if or how clinicians use or might use static measures like the AASE** (e.g., in determining treatment conclusion or retention, in determining level of care, etc.)
-   negative affect, social positive, physical and other concerns, and withdrawal and urges

It is **unclear how well static measures like the AASE predict future lapse/relapse outcomes**.
-    A baseline measure of self-efficacy is predictive of first lapse similar to how daily-measurements of self-efficacy are predictive of subsequent lapses, demonstrating the relative stability of efficacy (with daily fluctuation in AASE also predictive of following relapses)

Risk of lapse/relapse is best understood **dynamically**.
-   Relapse prevention model
-   Fluctuating risk over time due to fluctuating proximal and distal risk factors that interact fluidly with one another
-   Thus, **monitoring dynamic risk of lapse likely requires dense, dynamic monitoring to be effective**.

**EMA** offers a method to assess individuals' insight into risk of lapse densely and longitudinally.
-   This has/has not? (look at review) been done before (check Y/N, duration of monitoring, question being asked, etc. - Ariela to add from review)
-   When done, it has/has not? (look at review - Ariela to add from review) been done with **true prediction**.

Continuous monitoring with EMA can be embedded within a digital therapeutic context that can allow for monitoring of other risk factors and offer access to supports and tools to be used throughout recovery.

## Current study

We measured individuals' insight into their own lapse risk via daily EMA surveys situated within a digital therapeutic.
-   Thus, insight was assessed in a way that was: **cued** (i.e., prompted to reflect), **contextualized** (i.e., asked at the end of the survey assessing 9 other relevant risk factors, with three additional daily surveys asking similar question - a LOT of reflection!), and **continuous** (i.e., daily assessments).
-   Question was asked about individuals' risk of lapsing in the next week, so we used corresponding 1-week outcome windows that began at the time of the daily survey completion.

The purpose of this project is to determine:

1. The accuracy of individuals' insight into their lapse risk (in the next week)

*Relevant model configurations*
1 week windows ~ insight only (raw)

2. Whether dynamic assessment of insight and lapse risk has more predictive value than static assessment of insight and/or lapse risk

*Relevant model configurations*
1 week windows ~ insight only (raw)
1 week windows ~ AASE total score only --> comparable to how an individual might currently use their own post-treatment insight to assess ongoing lapse risk
Dichotomous lapse (whether an individual lapsed or not during study period) ~ AASE total score only --> comparable to how a clinician might use post-treatment insight to determine whether an individual should leave or continue treatment

3. Whether models that take advantage of full EMA data & feature engineering can predict lapse risk more accurately than insight alone

*Relevant model configurations*
1 week windows ~ insight only (raw)
1 week windows ~ all EMA items (4x daily surveys, engineered features)

# Method
<!-- copy HEAVILY from EMA paper -->

## Transparency and Openness
<!-- 
make sure 21-word solution is accurate
make sure to update OSF links
-->
We adhere to research transparency principles that are crucial for robust and replicable science. We reported how we determined the sample size, all data exclusions, all manipulations, and all study measures. We provide a transparency report in the supplement. Finally, we made the data, analysis scripts, annotated results, questionnaires, and other study materials publicly available ([OSF link here](OSF link here)). 

Our study design and analyses were not pre-registered. However, we restricted many researcher degrees of freedom via cross-validation. Cross-validation inherently includes replication; models are fit on held-in sets, decisions are made in held-out validation sets, and final performance is evaluated on held-out test sets.

## Participants
We recruited 151 participants in early recovery (1-8 weeks of abstinence) from AUD in Madison, Wisconsin, US. This sample size was determined based on traditional power analysis methods for logistic regression [@hsiehSampleSizeTables1989] because comparable approaches for machine learning models have not yet been validated.  Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required participants:

1.  were age 18 or older,
2.  could write and read in English,
3.  had at least moderate AUD (\>= 4 self-reported DSM-5 symptoms),
4.  were abstinent from alcohol for at least 1 week but no longer than 2 months, and
5.  were willing to use a single smartphone (personal or study provided) while enrolled in the study.

We also excluded participants exhibiting severe symptoms of psychosis or paranoia. 

## Procedure
Participants completed five study visits over approximately three months. After an initial phone screen, participants attended an in-person screening visit for eligibility determination, informed consent, and collection of self-report measures. Eligible and consented participants returned approximately one week later for an intake visit. Three additional follow-up visits occurred about every 30 days that participants remained on study. Participants were expected to complete four daily EMAs while on study. Other personal sensing data streams (geolocation, cellular communications, sleep quality, and audio check-ins) were collected as part of the parent grant's aims (R01 AA024391). 

## Measures

### EMA
Participants completed four brief (7-10 questions) EMAs daily following text message reminders. All EMAs included seven items that asked about any past alcohol use; current affective state (valence and arousal); craving; and past stressful events, risky situations, and pleasant events. The first EMA each day included three additional questions about the likelihood of future risky situations, stressful events, and drinking alcohol in the upcoming week (i.e., future efficacy). 

The first and last EMAs of the day were scheduled within one hour of participants' typical wake and sleep times. The other two EMAs were scheduled randomly within the first and second halves of the participants' typical day, with at least one hour between EMAs. 

### AASE

### Individual Differences
<!-- just to characterize data, not included in analyses -->
We collected self-report information about demographics (age, sex, race, ethnicity, education, employment, income, and marital status) and clinical characteristics (AUD milestones, number of quit attempts, lifetime AUD treatment history, lifetime receipt of AUD medication, DSM-5 AUD symptom count, and current drug use [@whoassistworkinggroupAlcoholSmokingSubstance2002]). 

## Data Analytic Strategy
Data preprocessing, modeling, and Bayesian analyses were done in R using the tidymodels ecosystem [@kuhnTidymodelsCollectionPackages2020]. All models were trained and evaluated using high-throughput computing resources provided by the University of Wisconsin Center for High Throughput Computing [@chtc].

### Lapse Labels
We predicted future lapses in three window widths that varied in their temporal precision: one week, one day, and one hour. Prediction windows were updated hourly. All classification models provide hour-by-hour predictions of future lapse probability for all three window widths.

We labeled each prediction window as *lapse* or *no lapse* using the EMA item "Have you drank any alcohol that you have not yet reported?". If participants answered yes to this question, they were prompted to enter the hour and date of the start and end of the drinking episode. Their responses were validated by study staff during monthly follow-up visits. 

For more detail on the creation of our prediction windows, see Lapse Labels in the Supplemental Methods section of our Supplement.

### Feature Engineering
Features were calculated using only data collected prior to the start of each prediction window. This ensured our models were making true *future predictions* versus identifying concurrent associations.  

Features were derived from three sources: baseline demographic characteristics (i.e., age, sex, race, marital status, education); day of the week and the time of day (daytime vs. evening/night) of the start of the prediction window; and previous EMA responses. We scored raw min, max, median, and count features from EMA items within varying lead up times (6, 12, 24, 48, 72, and 168 hours prior to start of prediction window).  We scored change EMA response features by subtracting the mean response for each feature over all data prior to the start of the prediction window from the associated raw feature.  

For more detail on feature engineering steps see Feature Engineering in the Supplemental Methods section of our Supplement. We also made a sample feature engineering script (i.e., tidymodels recipe) available on our study's OSF page.

### Model Training and Evaluation

#### Statistical Algorithm and Hyperparameters.
We trained and evaluated three separate classification models: one each for week, day, and hour prediction windows.  We initially considered four well-established statistical algorithms (XGBoost, Random Forest, K-Nearest Neighbors, and Elastic Net) that vary across characteristics expected to affect model performance (e.g., flexibility, complexity, and ability to handle higher-order interactions natively) [@kuhnAppliedPredictiveModeling2018]. However, preliminary exploratory analyses suggested that XGBoost consistently outperformed the other three algorithms.  Furthermore, the Shapley Additive Explanations (SHAP) method, which we planned to use for explanatory analyses of feature importance, is optimized for XGBoost.  For these reasons, we focused our primary model training and evaluation on the XGBoost algorithm only.  
  
Candidate XGBoost model configurations differed across sensible values for the hyperparameters mtry, tree depth, and learning rate using grid search.  All configurations used 500 trees with early stopping to prevent over-fitting.  All other hyperparameters were set to defaults established by the tidymodels packages.  Candidate model configurations also differed on outcome resampling method (i.e., up-sampling and down-sampling of the outcome using majority/no lapse to minority/lapse ratios ranging from 1:1 to 5:1).  We calibrated predicted probabilities using the beta distribution to support optimal decision-making under variable outcome distributions [@kullSigmoidsHowObtain2017].

#### Performance Metric.
Our primary performance metric for model selection and evaluation was area under the Receiver Operating Characteristic Curve (auROC) [@kuhnAppliedPredictiveModeling2018]. auROC indexes the probability that the model will predict a higher score for a randomly selected positive case (i.e., lapse) relative to a randomly selected negative case (i.e., no lapse). This metric was selected because it 1) combines sensitivity and specificity, which are both important characteristics to consider for clinical implementation; 2) is an aggregate metric across all decision thresholds, which is important because optimal decision thresholds may differ across settings and goals; and 3) is unaffected by class imbalance, which is important for comparing models with differing window widths and levels of class imbalance.

#### Cross-validation.
We used participant-grouped, nested cross-validation for model training, selection, and evaluation with auROC. Grouped cross-validation assigns all data from a participant as either held-in or held-out to avoid bias introduced when predicting a participant's data from their own data.  

Nested cross-validation uses two nested loops for dividing and holding out folds: an outer loop, where held-out folds serve as *test sets* for model evaluation; and inner loops, where held-out folds serve as *validation sets* for model selection. Importantly, these sets are independent, maintaining separation between data used to train the models, select best models, and evaluate those best models. Therefore, nested cross-validation removes optimization bias from the evaluation of model performance in the test sets and can yield lower variance performance estimates than single test set approaches [@jonathanUseCrossvalidationAssess2000].  

We used 1 repeat of 10-fold cross-validation for the inner loops and 3 repeats of 10-fold cross-validation for the outer loop.  Best model configurations were selected based on the median auROC across the 10 *validation sets*.  Final performance evaluation of those best model configurations was based on the median auROC across the 30 *test sets*.  For completeness, we report median auROC for our best model configurations for each model (week, day, and hour) separately from both the validation and test sets. In addition, we report other key performance metrics for the best model configurations including sensitivity, specificity, balanced accuracy, positive predictive value (PPV), and negative predictive value (NPV) from the test sets [@kuhnAppliedPredictiveModeling2018].

### Bayesian Estimation of auROC and Model Comparisons 
We used a Bayesian hierarchical generalized linear model to estimate the posterior probability distributions and 95% Bayesian confidence intervals (CIs) for auROC for the three best models (i.e., week, day, and hour). To determine the probability that these models' performance differed systematically from each other, we regressed the auROCs (logit transformed) from the 30 test sets for each model as a function of window width. Following recommendations from the tidymodels team [@kuhnTidyposteriorBayesianAnalysis2022; @kuhnBayesianAnalysisResampling], we set two random intercepts: one for the repeat, and another for the fold within repeat (folds are nested within repeats for auROCs collected with 3x10-fold cross-validation). We report the 95% (equal-tailed) Bayesian CIs from the posterior probability distributions for our models' auROCs.  We also report 95% (equal-tailed) Bayesian CIs for the differences in performance among the three models. For more detail on these analyses see Bayesian Analyses in Supplemental Methods section of the Supplement.  

### Shapley Additive Explanations for Feature Importance
We computed Shapley Values [@lundbergUnifiedApproachInterpreting2017] to provide a consistent and objective explanation of the importance of categories of features (based on EMA items) across our three models. Shapley values are model-agnostic and possess several useful properties including: Additivity (Shapley values for each feature can be computed independently and summed); Efficiency (the sum of Shapley values across features must add up to the difference between predicted and observed outcomes for each observation); Symmetry (Shapley values for two features should be equal if the two features contribute equally to all possible coalitions); and Dummy (a feature that does not change the predicted value in any coalition will have a Shapley value of 0).  We calculated Shapley values from the 30 test sets using the SHAPforxgboost package that provides Shapley values in log-odds units for binary classification models.  We averaged the three Shapley values for each observation for each feature across the three repeats to increase their stability. To calculate the local (i.e., for each observation) impact of categories of features (e.g., all features associated with the EMA craving item), we added Shapley values across all features in a category, separately for each observation.  To calculate global importance for categories of features, we averaged the absolute value of the Shapley values of all features in the category across all observations.  

# Results

## Demographic and Clinical Characteristics
<!-- identify if any participants excluded because of data exclusions -->


<!-- Demographics table-->

## Model Performance

### auROC

### Model Comparisons

### Feature Importance

# Discussion

<!-- all just notes -->

Cued, contextualized, continuous insight can accurately predict dynamic risk of alcohol lapse.

Why is this good?

-   Shows the value of insight/self-monitoring that we have long suspected/believed, now with empirical evidence and with theoretical nuance regarding the need for dynamic assessment of predictor and outcome

-   Within a dtx environment, accurate insight may lead to increased: trust in the algorithm (risk feedback aligns with insight), engagement with the app (d/t trust, but also perhaps reengagement following a period of lower use), self-efficacy, etc.

-   There may be opportunities for decreased burden (following additional research) if insight can be used in place of a full survey (still with cued daily assessment & ask to reflect on contextual factors), but we don't know how that will do on its own

AASE has some predictive value for dichotomous (and even rolling week window) outcomes (supports value of self-monitoring) but is insufficient for prediction and clinical utility.

Dynamic insight measurement can predict better than static AASE measurement.

Prediction algorithms grow more accurate when we add in other EMA features from 4X daily surveys - we can do our best prediction when we use more information. 

Cued insight only permits an individual to know:
* with relatively high accuracy if they are going to lapse
Full ema context permits an individual to know:
* with even higher accuracy if they are going to lapse
* whether that lapse risk represents a change from previous risk levels
* whether their self-efficacy/insight is changing from previous levels
* the key contributing factors that are driving that changing risk
* the key factors that may be targeted to most effectively reduce high and/or increasing lapse risk

\clearpage

# References
