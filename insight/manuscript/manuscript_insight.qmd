---
title: "Untitled"
format: pdf
csl: https://raw.githubusercontent.com/jjcurtin/lab_support/main/rmd_templates/csl/elsevier-vancouver.csl
geometry: margin=.5in
fontsize: 11pt
bibliography: paper_insight.bib
editor_options: 
  chunk_output_type: console
---



<!--General notes
Additional YAML formatting will need to be added once it's decided where this will be submitted. Tag Susan to handle that.

-->

```{r knitr_settings, include = FALSE}
# settings
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = FALSE, 
                      message = FALSE)
options(knitr.kable.NA = '')
knitr::opts_chunk$set(fig.pos = "ht", out.extra = "")
```

```{r setup, include = FALSE}
library(knitr)
# library(yardstick) # for roc_curve
library(kableExtra)
library(janitor)
# library(corx)
library(patchwork)
library(ggtext)
library(consort)
library(tidyverse)
library(tidymodels)
library(tidyposterior)
library(cowplot)

theme_set(theme_classic()) 
```


```{r paths}
switch (Sys.info()[['sysname']],
        # PC paths
        Windows = {
          path_input <- "P:/studydata/risk/chtc/insight"
          path_processed <- "P:/studydata/risk/data_processed/insight"
          path_models <- "P:/studydata/risk/models/insight"
          path_shared <- "P:/studydata/risk/data_processed/shared"},

        # IOS paths
        Darwin = {
          path_input <- "/Volumes/private/studydata/risk/chtc/insight"
          path_processed <- "/Volumes/private/studydata/risk/data_processed/insight"
          path_models <- "/Volumes/private/studydata/risk/models/insight"
          path_shared <- "/Volumes/private/studydata/risk/data_processed/shared"},
        
        # Linux paths
        Linux = {
          path_input <- "~/mnt/private/studydata/risk/chtc/insight"
          path_processed <- "~/mnt/private/studydata/risk/data_processed/insight"
          path_models <- "~/mnt/private/studydata/risk/models/insight"
          path_shared <- "~/mnt/private/studydata/risk/data_processed/shared"}
        )
```

```{r load_data}
```

# Abstract 
<!-- 225 word maximum, excluding title-->

Title for Abstract (CPA Submission): <!-- 100 character maximum -->
Dynamic assessment of self-efficacy predicts future alcohol use <!-- 63 characters -->

<!-- background -->

<!-- 
In the United States, alcohol use disorder (AUD) affects nearly 30 million adults annually and 30% of adults over the course of their lifetime. Of those who receive treatment, 2/3 relapse within 6 months. Static measures of self-efficacy have proven to be predictive of next lapse risk, but are less understood in the context of continuous self-monitoring. 

GEF: I love the key concepts you are portraying here! What I'm taking away is: 1) AUD is prevalent and costly, 2) individuals rarely receive (effective) treatment, 3) self-efficacy/self-monitoring is/can be predictive of future lapse, and 4) this construct hasn't been assessed dynamically. I think all of these should be in the abstract, so great job! For space purposes, however, and because the audience here is a narrow addiction-focused group who already know about the problems with AUD tx, we can minimize content related to points 1 and 2. My edits focused on simplifying those messages (e.g., removing numbers/statistics) and helping to link ideas 1/2 to ideas 3/4 (i.e., why are self-efficacy and continuous monitoring relevant and important?). 
-->

Treatment for alcohol use disorder (AUD) often includes self-monitoring, whereby individuals reflect on self-efficacy (i.e., likelihood of future alcohol use) to monitor risk. Self-efficacy is typically assessed statically; however, self-efficacy likely changes as risk changes throughout recovery.

<!-- purpose -->

<!-- 
The current study measured individuals' insight into their own lapse risk via daily EMAs situated within a digital therapeutic. 

GEF: This sounds great. I made some small tweaks to bring in the self-efficacy language & to highlight our key questions
-->

We investigated whether: 1) dynamically measured self-efficacy predicts dynamic lapse risk; 2) dynamic self-efficacy predicts better than a single, static self-efficacy measurement; and 3) including additional risk-relevant features improves prediction.

<!-- methods -->

<!--
We developed three separate models that provide week-level probabilities of a future lapse back to alcohol use using a baseline alcohol abstinence self-efficacy (AASE)-only score, a once-daily measure of self-efficacy, and a 'full-feature' model including 4 times daily EMA responses. Model features were based on scores collected through ecological momentary assessment (EMA). 

GEF: I like how you adapted the EMA paper methods to fit our project. Some small edits below. I also moved the participant sentence here to provide that context earlier on. You're right that in the paper, reporting on demographics of the sample would be in the Results, but I think providing who our sample is and the general procedure of the study fits best earlier here where we have less space
-->

Participants (*N*=151; 51% male; *M*~age~=41; 87% White, 97% Non-Hispanic) in early recovery (1–8 weeks abstinent) from AUD provided 4x daily ecological momentary assessments (EMAs) within a digital therapeutic for 1-3 months. We developed three models to predict probability of alcohol use in the next week. Models differed by included features: 1) self-efficacy ("How likely are you to drink alcohol in the next week?") from 1X daily EMAs; 2) Alcohol Abstinence Self-Efficacy (AASE) score measured at intake; or 3) self-efficacy plus additional relapse-relevant EMA items (e.g., craving, stress). We used grouped, nested cross-validation to select models and evaluate performance. 

<!-- results -->

<!-- 
Participants (*N*=151; 51% male; mean age = 41; 87% White, 97% Non-Hispanic) in early recovery (1–8 weeks of abstinence) from alcohol use disorder provided 4x daily EMA for up to three months. [Include results here that show the all feature model predicted the best, with with an ___, the once-daily measure of self-efficacy performed well at____, and finally, the baseline AASE-only had some predictive value for dichotomous outcomes but was insufficient for prediction and clinical utility]. 

GEF: You included all the right information here. I changed the structure/presentation slightly to match up with our questions, and I added the values. 
-->

Dynamic self-efficacy predicted next-week lapse risk (area under the receiver operating curve [auROC]=0.80) more accurately than static self-efficacy (auROC=0.59). Adding EMA items further improved performance (auROC=0.88).

<!-- discussion/conclusion/impact -->

<!-- 
This suggests dynamic measures of self-efficacy are accurate in predicting future lapse risk over time and may be further improved via inclusion of other data collected via EMA when available.

GEF: Just some slight tweaks to add context around that impact!
-->
Individuals can accurately predict their future lapse risk when cued via daily EMAs. Adding EMA items improves performance and identifies factors driving risk, which could guide treatment recommendations within a digital therapeutic. 

# Introduction

## Background

AUD/SUDs are problematic and prevalent and require lifelong care/monitoring.

Self-monitoring is thought to be a valuable and necessary tool during recovery from AUD and other SUDs.It promises a cost-effective, accessible, and easy-to-use tool when integrated in digital therapeutics
-   Anecdotally - encouraged by clinicians, prompted by many self-help tools/apps, etc. Considered to be a productive component of CBT in the treatment of SUDs
-   Empirically? Findings are mixed and data is limited. In one review of 41 studies investigating self-monitoring on substance use outcomes, only 29% found a helpful effect (like reducing substance use) (Gass et al., 2021). 63% had no effect and 8% found it to be detrimental. This ambiguity may be at least partially explained by a lack of research. For instance, in another review of 191 web-based interventions designed to decrease alcohol consumption, only ten met quality criteria for inclusion and only 1 was an RCT (Bewick et al., 2008).No empirical evidence was provided that apps could serve as an intervention. 
-   Consider ARCP framing - there is a strongly held belief about monitoring perhaps without **sufficient evidence** and/or without **sufficient nuance** to how monitoring should be implemented

Self-monitoring, insight, and/or confidence in remaining abstinent/pursuing treatment goals has traditionally been measured statically.
-   Measures like the AASE are often used to assess an individual's "long-term" likelihood of remaining abstinent. 
-   Efficacy can be hard to capture consistently because it is context-specific. Since its development in 1994, the AASE, built on Bandura's construct of self-efficacy, has been used with frequency. The 20-item scale demonstrates high-internal consistency as well as strong indices of reliability and validity (DiClemente et al., 1994). The shortened 12-item scale shares in these attributes (McKiernan et al., 2011)
-   Scores from measures like the AASe may be used at the end of treatment/end of study/etc. to estimate an individual's future likelihood of staying abstinent
-   It is **unclear if or how clinicians use or might use static measures like the AASE** (e.g., in determining treatment conclusion or retention, in determining level of care, etc.)
-   negative affect, social positive, physical and other concerns, and withdrawal and urges

It is **unclear how well static measures like the AASE predict future lapse/relapse outcomes**.
-    A baseline measure of self-efficacy is predictive of first lapse similar to how daily-measurements of self-efficacy are predictive of subsequent lapses, demonstrating the relative stability of efficacy (with daily fluctuation in AASE also predictive of following relapses)

Risk of lapse/relapse is best understood **dynamically**.
-   Relapse prevention model
-   Fluctuating risk over time due to fluctuating proximal and distal risk factors that interact fluidly with one another
-   Thus, **monitoring dynamic risk of lapse likely requires dense, dynamic monitoring to be effective**.

**EMA** offers a method to assess individuals' insight into risk of lapse densely and longitudinally.
-   This has/has not? (look at review) been done before (check Y/N, duration of monitoring, question being asked, etc. - Ariela to add from review)
-   When done, it has/has not? (look at review - Ariela to add from review) been done with **true prediction**.

Continuous monitoring with EMA can be embedded within a digital therapeutic context that can allow for monitoring of other risk factors and offer access to supports and tools to be used throughout recovery.

## Current study

We measured individuals' insight into their own lapse risk via daily EMA surveys situated within a digital therapeutic.
-   Thus, insight was assessed in a way that was: **cued** (i.e., prompted to reflect), **contextualized** (i.e., asked at the end of the survey assessing 9 other relevant risk factors, with three additional daily surveys asking similar question - a LOT of reflection!), and **continuous** (i.e., daily assessments).
-   Question was asked about individuals' risk of lapsing in the next week, so we used corresponding 1-week outcome windows that began at the time of the daily survey completion.

The purpose of this project is to determine:

1. The accuracy of individuals' insight into their lapse risk (in the next week)

*Relevant model configurations*
1 week windows ~ insight only (raw)

2. Whether dynamic assessment of insight and lapse risk has more predictive value than static assessment of insight and/or lapse risk

*Relevant model configurations*
1 week windows ~ insight only (raw)
1 week windows ~ AASE total score only --> comparable to how an individual might currently use their own post-treatment insight to assess ongoing lapse risk
Dichotomous lapse (whether an individual lapsed or not during study period) ~ AASE total score only --> comparable to how a clinician might use post-treatment insight to determine whether an individual should leave or continue treatment

3. Whether models that take advantage of full EMA data & feature engineering can predict lapse risk more accurately than insight alone

*Relevant model configurations*
1 week windows ~ insight only (raw)
1 week windows ~ all EMA items (4x daily surveys, engineered features)

# Method
<!-- copy HEAVILY from EMA paper -->

## Transparency and Openness
<!-- 
make sure 21-word solution is accurate
make sure to update OSF links
-->
We adhere to research transparency principles that are crucial for robust and replicable science. We reported how we determined the sample size, all data exclusions, all manipulations, and all study measures. We provide a transparency report in the supplement. Finally, we made the data, analysis scripts, annotated results, questionnaires, and other study materials publicly available ([OSF link here](OSF link here)). 

Our study design and analyses were not pre-registered. However, we restricted many researcher degrees of freedom via cross-validation. Cross-validation inherently includes replication; models are fit on held-in sets, decisions are made in held-out validation sets, and final performance is evaluated on held-out test sets.

## Participants
We recruited 151 participants in early recovery (1-8 weeks of abstinence) from AUD in Madison, Wisconsin, US. This sample size was determined based on traditional power analysis methods for logistic regression [@hsiehSampleSizeTables1989] because comparable approaches for machine learning models have not yet been validated.  Participants were recruited through print and targeted digital advertisements and partnerships with treatment centers. We required participants:

1.  were age 18 or older,
2.  could write and read in English,
3.  had at least moderate AUD (\>= 4 self-reported DSM-5 symptoms),
4.  were abstinent from alcohol for at least 1 week but no longer than 2 months, and
5.  were willing to use a single smartphone (personal or study provided) while enrolled in the study.

We also excluded participants exhibiting severe symptoms of psychosis or paranoia. 

## Procedure
Participants completed five study visits over approximately three months. After an initial phone screen, participants attended an in-person screening visit for eligibility determination, informed consent, and collection of self-report measures. Eligible and consented participants returned approximately one week later for an intake visit. Three additional follow-up visits occurred about every 30 days that participants remained on study. Participants were expected to complete four daily EMAs while on study. Other personal sensing data streams (geolocation, cellular communications, sleep quality, and audio check-ins) were collected as part of the parent grant's aims (R01 AA024391). 

## Measures

### EMA

### AASE

### Individual Differences
<!-- just to characterize data, not included in analyses -->

## Data Analytic Strategy

### Lapse Labels

### Feature Engineering

### Model Training and Evaluation

# Results

## Demographic and Clinical Characteristics
<!-- identify if any participants excluded because of data exclusions -->


<!-- Demographics table-->

## Model Performance

### auROC

### Model Comparisons

### Feature Importance

# Discussion

<!-- all just notes -->

Cued, contextualized, continuous insight can accurately predict dynamic risk of alcohol lapse.

Why is this good?

-   Shows the value of insight/self-monitoring that we have long suspected/believed, now with empirical evidence and with theoretical nuance regarding the need for dynamic assessment of predictor and outcome

-   Within a dtx environment, accurate insight may lead to increased: trust in the algorithm (risk feedback aligns with insight), engagement with the app (d/t trust, but also perhaps reengagement following a period of lower use), self-efficacy, etc.

-   There may be opportunities for decreased burden (following additional research) if insight can be used in place of a full survey (still with cued daily assessment & ask to reflect on contextual factors), but we don't know how that will do on its own

AASE has some predictive value for dichotomous (and even rolling week window) outcomes (supports value of self-monitoring) but is insufficient for prediction and clinical utility.

Dynamic insight measurement can predict better than static AASE measurement.

Prediction algorithms grow more accurate when we add in other EMA features from 4X daily surveys - we can do our best prediction when we use more information. 

Cued insight only permits an individual to know:
* with relatively high accuracy if they are going to lapse
Full ema context permits an individual to know:
* with even higher accuracy if they are going to lapse
* whether that lapse risk represents a change from previous risk levels
* whether their self-efficacy/insight is changing from previous levels
* the key contributing factors that are driving that changing risk
* the key factors that may be targeted to most effectively reduce high and/or increasing lapse risk

\clearpage

# References
